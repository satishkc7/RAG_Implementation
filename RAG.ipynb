{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_folder_path = '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP'\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [f for f in os.listdir(pdf_folder_path) if f.endswith('.pdf')]\n",
    "\n",
    "# Load each PDF file\n",
    "docs = []\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(os.path.join(pdf_folder_path, pdf_file))\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 0}, page_content='LORA: L OW-RANK ADAPTATION OF LARGE LAN-\\nGUAGE MODELS\\nEdward Hu‚àóYelong Shen‚àóPhillip Wallis Zeyuan Allen-Zhu\\nYuanzhi Li Shean Wang Lu Wang Weizhu Chen\\nMicrosoft Corporation\\n{edwardhu, yeshe, phwallis, zeyuana,\\nyuanzhil, swang, luw, wzchen }@microsoft.com\\nyuanzhil@andrew.cmu.edu\\n(Version 2)\\nABSTRACT\\nAn important paradigm of natural language processing consists of large-scale pre-\\ntraining on general domain data and adaptation to particular tasks or domains. As\\nwe pre-train larger models, full Ô¨Åne-tuning, which retrains all model parameters,\\nbecomes less feasible. Using GPT-3 175B as an example ‚Äì deploying indepen-\\ndent instances of Ô¨Åne-tuned models, each with 175B parameters, is prohibitively\\nexpensive. We propose Low-RankAdaptation, or LoRA, which freezes the pre-\\ntrained model weights and injects trainable rank decomposition matrices into each\\nlayer of the Transformer architecture, greatly reducing the number of trainable pa-\\nrameters for downstream tasks. Compared to GPT-3 175B Ô¨Åne-tuned with Adam,\\nLoRA can reduce the number of trainable parameters by 10,000 times and the\\nGPU memory requirement by 3 times. LoRA performs on-par or better than Ô¨Åne-\\ntuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite hav-\\ning fewer trainable parameters, a higher training throughput, and, unlike adapters,\\nno additional inference latency . We also provide an empirical investigation into\\nrank-deÔ¨Åciency in language model adaptation, which sheds light on the efÔ¨Åcacy of\\nLoRA. We release a package that facilitates the integration of LoRA with PyTorch\\nmodels and provide our implementations and model checkpoints for RoBERTa,\\nDeBERTa, and GPT-2 at https://github.com/microsoft/LoRA .\\n1 I NTRODUCTION\\nPretrained \\nWeights\\nùëä‚àà‚Ñùùëë√óùëë\\nxh\\nùêµ=0\\nùê¥=ùí©(0,ùúé2)\\nùëëùëüPretrained \\nWeights\\nùëä‚àà‚Ñùùëë√óùëë\\nxf(x)\\nùëë\\nFigure 1: Our reparametriza-\\ntion. We only train AandB.Many applications in natural language processing rely on adapt-\\ningonelarge-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via Ô¨Åne-tuning ,\\nwhich updates all the parameters of the pre-trained model. The ma-\\njor downside of Ô¨Åne-tuning is that the new model contains as many\\nparameters as in the original model. As larger models are trained\\nevery few months, this changes from a mere ‚Äúinconvenience‚Äù for\\nGPT-2 (Radford et al., b) or RoBERTa large (Liu et al., 2019) to a\\ncritical deployment challenge for GPT-3 (Brown et al., 2020) with\\n175 billion trainable parameters.1\\nMany sought to mitigate this by adapting only some parameters or\\nlearning external modules for new tasks. This way, we only need\\nto store and load a small number of task-speciÔ¨Åc parameters in ad-\\ndition to the pre-trained model for each task, greatly boosting the\\noperational efÔ¨Åciency when deployed. However, existing techniques\\n‚àóEqual contribution.\\n0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\\n1While GPT-3 175B achieves non-trivial performance with few-shot learning, Ô¨Åne-tuning boosts its perfor-\\nmance signiÔ¨Åcantly as shown in Appendix A.\\n1arXiv:2106.09685v2  [cs.CL]  16 Oct 2021'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 1}, page_content='often introduce inference latency (Houlsby et al., 2019; RebufÔ¨Å et al., 2017) by extending model\\ndepth or reduce the model‚Äôs usable sequence length (Li & Liang, 2021; Lester et al., 2021; Ham-\\nbardzumyan et al., 2020; Liu et al., 2021) (Section 3). More importantly, these method often fail to\\nmatch the Ô¨Åne-tuning baselines, posing a trade-off between efÔ¨Åciency and model quality.\\nWe take inspiration from Li et al. (2018a); Aghajanyan et al. (2020) which show that the learned\\nover-parametrized models in fact reside on a low intrinsic dimension. We hypothesize that the\\nchange in weights during model adaptation also has a low ‚Äúintrinsic rank‚Äù, leading to our proposed\\nLow-RankAdaptation (LoRA) approach. LoRA allows us to train some dense layers in a neural\\nnetwork indirectly by optimizing rank decomposition matrices of the dense layers‚Äô change during\\nadaptation instead, while keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\\n175B as an example, we show that a very low rank (i.e., rin Figure 1 can be one or two) sufÔ¨Åces even\\nwhen the full rank (i.e., d) is as high as 12,288, making LoRA both storage- and compute-efÔ¨Åcient.\\nLoRA possesses several key advantages.\\n‚Ä¢ A pre-trained model can be shared and used to build many small LoRA modules for dif-\\nferent tasks. We can freeze the shared model and efÔ¨Åciently switch tasks by replacing the\\nmatricesAandBin Figure 1, reducing the storage requirement and task-switching over-\\nhead signiÔ¨Åcantly.\\n‚Ä¢ LoRA makes training more efÔ¨Åcient and lowers the hardware barrier to entry by up to 3\\ntimes when using adaptive optimizers since we do not need to calculate the gradients or\\nmaintain the optimizer states for most parameters. Instead, we only optimize the injected,\\nmuch smaller low-rank matrices.\\n‚Ä¢ Our simple linear design allows us to merge the trainable matrices with the frozen weights\\nwhen deployed, introducing no inference latency compared to a fully Ô¨Åne-tuned model, by\\nconstruction.\\n‚Ä¢ LoRA is orthogonal to many prior methods and can be combined with many of them, such\\nas preÔ¨Åx-tuning. We provide an example in Appendix E.\\nTerminologies and Conventions We make frequent references to the Transformer architecture\\nand use the conventional terminologies for its dimensions. We call the input and output di-\\nmension size of a Transformer layer dmodel . We useWq,Wk,Wv, andWoto refer to the\\nquery/key/value/output projection matrices in the self-attention module. WorW0refers to a pre-\\ntrained weight matrix and ‚àÜWits accumulated gradient update during adaptation. We use rto\\ndenote the rank of a LoRA module. We follow the conventions set out by (Vaswani et al., 2017;\\nBrown et al., 2020) and use Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) for model\\noptimization and use a Transformer MLP feedforward dimension dffn= 4√ódmodel .\\n2 P ROBLEM STATEMENT\\nWhile our proposal is agnostic to training objective, we focus on language modeling as our motivat-\\ning use case. Below is a brief description of the language modeling problem and, in particular, the\\nmaximization of conditional probabilities given a task-speciÔ¨Åc prompt.\\nSuppose we are given a pre-trained autoregressive language model PŒ¶(y|x)parametrized by Œ¶.\\nFor instance, PŒ¶(y|x)can be a generic multi-task learner such as GPT (Radford et al., b; Brown\\net al., 2020) based on the Transformer architecture (Vaswani et al., 2017). Consider adapting this\\npre-trained model to downstream conditional text generation tasks, such as summarization, machine\\nreading comprehension (MRC), and natural language to SQL (NL2SQL). Each downstream task is\\nrepresented by a training dataset of context-target pairs: Z={(xi,yi)}i=1,..,N, where both xiand\\nyiare sequences of tokens. For example, in NL2SQL, xiis a natural language query and yiits\\ncorresponding SQL command; for summarization, xiis the content of an article and yiits summary.\\n2'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 2}, page_content='During full Ô¨Åne-tuning, the model is initialized to pre-trained weights Œ¶0and updated to Œ¶0+ ‚àÜŒ¶\\nby repeatedly following the gradient to maximize the conditional language modeling objective:\\nmax\\nŒ¶‚àë\\n(x,y)‚ààZ|y|‚àë\\nt=1log(PŒ¶(yt|x,y<t)) (1)\\nOne of the main drawbacks for full Ô¨Åne-tuning is that for each downstream task, we learn a different\\nset of parameters ‚àÜŒ¶whose dimension|‚àÜŒ¶|equals|Œ¶0|. Thus, if the pre-trained model is large\\n(such as GPT-3 with |Œ¶0|‚âà175Billion), storing and deploying many independent instances of\\nÔ¨Åne-tuned models can be challenging, if at all feasible.\\nIn this paper, we adopt a more parameter-efÔ¨Åcient approach, where the task-speciÔ¨Åc parameter\\nincrement ‚àÜŒ¶ = ‚àÜŒ¶(Œò) is further encoded by a much smaller-sized set of parameters Œòwith\\n|Œò|‚â™| Œ¶0|. The task of Ô¨Ånding ‚àÜŒ¶thus becomes optimizing over Œò:\\nmax\\nŒò‚àë\\n(x,y)‚ààZ|y|‚àë\\nt=1log(\\npŒ¶0+‚àÜŒ¶(Œò) (yt|x,y<t))\\n(2)\\nIn the subsequent sections, we propose to use a low-rank representation to encode ‚àÜŒ¶that is both\\ncompute- and memory-efÔ¨Åcient. When the pre-trained model is GPT-3 175B, the number of train-\\nable parameters|Œò|can be as small as 0.01% of|Œ¶0|.\\n3 A REN‚ÄôTEXISTING SOLUTIONS GOOD ENOUGH ?\\nThe problem we set out to tackle is by no means new. Since the inception of transfer learning, dozens\\nof works have sought to make model adaptation more parameter- and compute-efÔ¨Åcient. See Sec-\\ntion 6 for a survey of some of the well-known works. Using language modeling as an example, there\\nare two prominent strategies when it comes to efÔ¨Åcient adaptations: adding adapter layers (Houlsby\\net al., 2019; RebufÔ¨Å et al., 2017; Pfeiffer et al., 2021; R ¬®uckl¬¥e et al., 2020) or optimizing some forms\\nof the input layer activations (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020;\\nLiu et al., 2021). However, both strategies have their limitations, especially in a large-scale and\\nlatency-sensitive production scenario.\\nAdapter Layers Introduce Inference Latency There are many variants of adapters. We focus\\non the original design by Houlsby et al. (2019) which has two adapter layers per Transformer block\\nand a more recent one by Lin et al. (2020) which has only one per block but with an additional\\nLayerNorm (Ba et al., 2016). While one can reduce the overall latency by pruning layers or exploit-\\ning multi-task settings (R ¬®uckl¬¥e et al., 2020; Pfeiffer et al., 2021), there is no direct ways to bypass\\nthe extra compute in adapter layers. This seems like a non-issue since adapter layers are designed\\nto have few parameters (sometimes <1% of the original model) by having a small bottleneck di-\\nmension, which limits the FLOPs they can add. However, large neural networks rely on hardware\\nparallelism to keep the latency low, and adapter layers have to be processed sequentially. This makes\\na difference in the online inference setting where the batch size is typically as small as one. In a\\ngeneric scenario without model parallelism, such as running inference on GPT-2 (Radford et al., b)\\nmedium on a single GPU, we see a noticeable increase in latency when using adapters, even with a\\nvery small bottleneck dimension (Table 1).\\nThis problem gets worse when we need to shard the model as done in Shoeybi et al. (2020); Lep-\\nikhin et al. (2020), because the additional depth requires more synchronous GPU operations such as\\nAllReduce andBroadcast , unless we store the adapter parameters redundantly many times.\\nDirectly Optimizing the Prompt is Hard The other direction, as exempliÔ¨Åed by preÔ¨Åx tuning (Li\\n& Liang, 2021), faces a different challenge. We observe that preÔ¨Åx tuning is difÔ¨Åcult to optimize\\nand that its performance changes non-monotonically in trainable parameters, conÔ¨Årming similar\\nobservations in the original paper. More fundamentally, reserving a part of the sequence length for\\nadaptation necessarily reduces the sequence length available to process a downstream task, which\\nwe suspect makes tuning the prompt less performant compared to other methods. We defer the study\\non task performance to Section 5.\\n3'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 3}, page_content='Batch Size 32 16 1\\nSequence Length 512 256 128\\n|Œò| 0.5M 11M 11M\\nFine-Tune/LoRA 1449.4¬±0.8 338.0 ¬±0.6 19.8 ¬±2.7\\nAdapterL1482.0¬±1.0 (+2.2%) 354.8 ¬±0.5 (+5.0%) 23.9 ¬±2.1 (+20.7%)\\nAdapterH1492.2¬±1.0 (+3.0%) 366.3 ¬±0.5 (+8.4%) 25.8 ¬±2.2 (+30.3%)\\nTable 1: Infernece latency of a single forward pass in GPT-2 medium measured in milliseconds, av-\\neraged over 100 trials. We use an NVIDIA Quadro RTX8000. ‚Äú |Œò|‚Äù denotes the number of trainable\\nparameters in adapter layers. AdapterLand AdapterHare two variants of adapter tuning, which we\\ndescribe in Section 5.1. The inference latency introduced by adapter layers can be signiÔ¨Åcant in an\\nonline, short-sequence-length scenario. See the full study in Appendix B.\\n4 O URMETHOD\\nWe describe the simple design of LoRA and its practical beneÔ¨Åts. The principles outlined here apply\\nto any dense layers in deep learning models, though we only focus on certain weights in Transformer\\nlanguage models in our experiments as the motivating use case.\\n4.1 L OW-RANK -PARAMETRIZED UPDATE MATRICES\\nA neural network contains many dense layers which perform matrix multiplication. The weight\\nmatrices in these layers typically have full-rank. When adapting to a speciÔ¨Åc task, Aghajanyan et al.\\n(2020) shows that the pre-trained language models have a low ‚Äúinstrisic dimension‚Äù and can still\\nlearn efÔ¨Åciently despite a random projection to a smaller subspace. Inspired by this, we hypothe-\\nsize the updates to the weights also have a low ‚Äúintrinsic rank‚Äù during adaptation. For a pre-trained\\nweight matrix W0‚ààRd√ók, we constrain its update by representing the latter with a low-rank de-\\ncomposition W0+ ‚àÜW=W0+BA, whereB‚ààRd√ór,A‚ààRr√ók, and the rank r‚â™min(d,k).\\nDuring training, W0is frozen and does not receive gradient updates, while AandBcontain trainable\\nparameters. Note both W0and‚àÜW=BAare multiplied with the same input, and their respective\\noutput vectors are summed coordinate-wise. For h=W0x, our modiÔ¨Åed forward pass yields:\\nh=W0x+ ‚àÜWx=W0x+BAx (3)\\nWe illustrate our reparametrization in Figure 1. We use a random Gaussian initialization for Aand\\nzero forB, so‚àÜW=BAis zero at the beginning of training. We then scale ‚àÜWx byŒ±\\nr, whereŒ±\\nis a constant in r. When optimizing with Adam, tuning Œ±is roughly the same as tuning the learning\\nrate if we scale the initialization appropriately. As a result, we simply set Œ±to the Ô¨Årstrwe try\\nand do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary\\nr(Yang & Hu, 2021).\\nA Generalization of Full Fine-tuning. A more general form of Ô¨Åne-tuning allows the training of\\na subset of the pre-trained parameters. LoRA takes a step further and does not require the accumu-\\nlated gradient update to weight matrices to have full-rank during adaptation. This means that when\\napplying LoRA to all weight matrices and training all biases2, we roughly recover the expressive-\\nness of full Ô¨Åne-tuning by setting the LoRA rank rto the rank of the pre-trained weight matrices. In\\nother words, as we increase the number of trainable parameters3, training LoRA roughly converges\\nto training the original model, while adapter-based methods converges to an MLP and preÔ¨Åx-based\\nmethods to a model that cannot take long input sequences.\\nNo Additional Inference Latency. When deployed in production, we can explicitly compute and\\nstoreW=W0+BA and perform inference as usual. Note that both W0andBA are inRd√ók.\\nWhen we need to switch to another downstream task, we can recover W0by subtracting BAand\\nthen adding a different B‚Ä≤A‚Ä≤, a quick operation with very little memory overhead. Critically, this\\n2They represent a negligible number of parameters compared to weights.\\n3An inevitability when adapting to hard tasks.\\n4'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 4}, page_content='guarantees that we do not introduce any additional latency during inference compared to a Ô¨Åne-tuned\\nmodel by construction.\\n4.2 A PPLYING LORA TOTRANSFORMER\\nIn principle, we can apply LoRA to any subset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architecture, there are four weight matrices in\\nthe self-attention module ( Wq,Wk,Wv,Wo) and two in the MLP module. We treat Wq(orWk,Wv)\\nas a single matrix of dimension dmodel√ódmodel , even though the output dimension is usually sliced\\ninto attention heads. We limit our study to only adapting the attention weights for downstream\\ntasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity\\nand parameter-efÔ¨Åciency.We further study the effect on adapting different types of attention weight\\nmatrices in a Transformer in Section 7.1. We leave the empirical investigation of adapting the MLP\\nlayers, LayerNorm layers, and biases to a future work.\\nPractical BeneÔ¨Åts and Limitations. The most signiÔ¨Åcant beneÔ¨Åt comes from the reduction in\\nmemory and storage usage. For a large Transformer trained with Adam, we reduce that VRAM\\nusage by up to 2/3ifr‚â™dmodel as we do not need to store the optimizer states for the frozen\\nparameters. On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to\\n350GB. With r= 4and only the query and value projection matrices being adapted, the checkpoint\\nsize is reduced by roughly 10,000 √ó(from 350GB to 35MB)4. This allows us to train with signiÔ¨Å-\\ncantly fewer GPUs and avoid I/O bottlenecks. Another beneÔ¨Åt is that we can switch between tasks\\nwhile deployed at a much lower cost by only swapping the LoRA weights as opposed to all the\\nparameters. This allows for the creation of many customized models that can be swapped in and out\\non the Ô¨Çy on machines that store the pre-trained weights in VRAM. We also observe a 25% speedup\\nduring training on GPT-3 175B compared to full Ô¨Åne-tuning5as we do not need to calculate the\\ngradient for the vast majority of the parameters.\\nLoRA also has its limitations. For example, it is not straightforward to batch inputs to different tasks\\nwith different AandBin a single forward pass, if one chooses to absorb AandBintoWto eliminate\\nadditional inference latency. Though it is possible to not merge the weights and dynamically choose\\nthe LoRA modules to use for samples in a batch for scenarios where latency is not critical.\\n5 E MPIRICAL EXPERIMENTS\\nWe evaluate the downstream task performance of LoRA on RoBERTa (Liu et al., 2019), De-\\nBERTa (He et al., 2021), and GPT-2 (Radford et al., b), before scaling up to GPT-3 175B (Brown\\net al., 2020). Our experiments cover a wide range of tasks, from natural language understanding\\n(NLU) to generation (NLG). SpeciÔ¨Åcally, we evaluate on the GLUE (Wang et al., 2019) benchmark\\nfor RoBERTa and DeBERTa. We follow the setup of Li & Liang (2021) on GPT-2 for a direct com-\\nparison and add WikiSQL (Zhong et al., 2017) (NL to SQL queries) and SAMSum (Gliwa et al.,\\n2019) (conversation summarization) for large-scale experiments on GPT-3. See Appendix C for\\nmore details on the datasets we use. We use NVIDIA Tesla V100 for all experiments.\\n5.1 B ASELINES\\nTo compare with other baselines broadly, we replicate the setups used by prior work and reuse their\\nreported numbers whenever possible. This, however, means that some baselines might only appear\\nin certain experiments.\\nFine-Tuning (FT) is a common approach for adaptation. During Ô¨Åne-tuning, the model is initialized\\nto the pre-trained weights and biases, and all model parameters undergo gradient updates.A simple\\nvariant is to update only some layers while freezing others. We include one such baseline reported\\nin prior work (Li & Liang, 2021) on GPT-2, which adapts just the last two layers ( FTTop2).\\n4We still need the 350GB model during deployment; however, storing 100 adapted models only requires\\n350GB + 35MB * 100 ‚âà354GB as opposed to 100 * 350GB ‚âà35TB.\\n5For GPT-3 175B, the training throughput for full Ô¨Åne-tuning is 32.5 tokens/s per V100 GPU; with the same\\nnumber of weight shards for model parallelism, the throughput is 43.1 tokens/s per V100 GPU for LoRA.\\n5'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 5}, page_content='Model & Method # Trainable\\nParameters MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg.\\nRoB base(FT)* 125.0M 87.6 94.8 90.2 63.6 92.8 91.9 78.7 91.2 86.4\\nRoB base(BitFit)* 0.1M 84.7 93.7 92.7 62.0 91.8 84.0 81.5 90.8 85.2\\nRoB base(AdptD)* 0.3M 87.1¬±.094.2¬±.188.5¬±1.160.8¬±.493.1¬±.190.2¬±.071.5¬±2.789.7¬±.384.4\\nRoB base(AdptD)* 0.9M 87.3¬±.194.7¬±.388.4¬±.162.6¬±.993.0¬±.290.6¬±.075.9¬±2.290.3¬±.185.4\\nRoB base(LoRA) 0.3M 87.5¬±.395.1¬±.289.7¬±.763.4¬±1.293.3¬±.390.8¬±.186.6¬±.791.5¬±.287.2\\nRoB large(FT)* 355.0M 90.2 96.4 90.9 68.0 94.7 92.2 86.6 92.4 88.9\\nRoB large(LoRA) 0.8M 90.6¬±.296.2¬±.590.9¬±1.268.2¬±1.994.9¬±.391.6¬±.187.4¬±2.592.6¬±.289.0\\nRoB large(AdptP)‚Ä† 3.0M 90.2¬±.396.1¬±.390.2¬±.768.3¬±1.094.8¬±.291.9¬±.183.8¬±2.992.1¬±.788.4\\nRoB large(AdptP)‚Ä† 0.8M 90.5¬±.396.6¬±.289.7¬±1.267.8¬±2.594.8¬±.391.7¬±.280.1¬±2.991.9¬±.487.9\\nRoB large(AdptH)‚Ä† 6.0M 89.9¬±.596.2¬±.388.7¬±2.966.5¬±4.494.7¬±.292.1¬±.183.4¬±1.191.0¬±1.787.8\\nRoB large(AdptH)‚Ä† 0.8M 90.3¬±.396.3¬±.587.7¬±1.766.3¬±2.094.7¬±.291.5¬±.172.9¬±2.991.5¬±.586.4\\nRoB large(LoRA)‚Ä† 0.8M 90.6¬±.296.2¬±.590.2¬±1.068.2¬±1.994.8¬±.391.6¬±.285.2¬±1.192.3¬±.588.6\\nDeB XXL(FT)* 1500.0M 91.8 97.2 92.0 72.0 96.0 92.7 93.9 92.9 91.1\\nDeB XXL(LoRA) 4.7M 91.9¬±.296.9¬±.292.6¬±.672.4¬±1.196.0¬±.192.9¬±.194.9¬±.493.0¬±.291.3\\nTable 2: RoBERTa base, RoBERTa large, and DeBERTa XXLwith different adaptation methods on the\\nGLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthew‚Äôs\\ncorrelation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. Higher is better\\nfor all metrics. * indicates numbers published in prior works. ‚Ä†indicates runs conÔ¨Ågured in a setup\\nsimilar to Houlsby et al. (2019) for a fair comparison.\\nBias-only or BitFit is a baseline where we only train the bias vectors while freezing everything else.\\nContemporarily, this baseline has also been studied by BitFit (Zaken et al., 2021).\\nPreÔ¨Åx-embedding tuning (PreEmbed) inserts special tokens among the input tokens. These spe-\\ncial tokens have trainable word embeddings and are generally not in the model‚Äôs vocabulary. Where\\nto place such tokens can have an impact on performance. We focus on ‚ÄúpreÔ¨Åxing‚Äù, which prepends\\nsuch tokens to the prompt, and ‚ÄúinÔ¨Åxing‚Äù, which appends to the prompt; both are discussed in Li &\\nLiang (2021). We use lp(resp.li) denote the number of preÔ¨Åx (resp. inÔ¨Åx) tokens. The number of\\ntrainable parameters is |Œò|=dmodel√ó(lp+li).\\nPreÔ¨Åx-layer tuning (PreLayer) is an extension to preÔ¨Åx-embedding tuning. Instead of just learning\\nthe word embeddings (or equivalently, the activations after the embedding layer) for some special\\ntokens, we learn the activations after every Transformer layer. The activations computed from pre-\\nvious layers are simply replaced by trainable ones. The resulting number of trainable parameters is\\n|Œò|=L√ódmodel√ó(lp+li), whereLis the number of Transformer layers.\\nAdapter tuning as proposed in Houlsby et al. (2019) inserts adapter layers between the self-\\nattention module (and the MLP module) and the subsequent residual connection. There are two\\nfully connected layers with biases in an adapter layer with a nonlinearity in between. We call this\\noriginal design AdapterH. Recently, Lin et al. (2020) proposed a more efÔ¨Åcient design with the\\nadapter layer applied only after the MLP module and after a LayerNorm. We call it AdapterL. This\\nis very similar to another deign proposed in Pfeiffer et al. (2021), which we call AdapterP. We also\\ninclude another baseline call AdapterDrop (R ¬®uckl¬¥e et al., 2020) which drops some adapter layers for\\ngreater efÔ¨Åciency ( AdapterD). We cite numbers from prior works whenever possible to maximize\\nthe number of baselines we compare with; they are in rows with an asterisk (*) in the Ô¨Årst column.\\nIn all cases, we have |Œò|=ÀÜLAdpt√ó(2√ódmodel√ór+r+dmodel )+2√óÀÜLLN√ódmodel where ÀÜLAdpt\\nis the number of adapter layers and ÀÜLLNthe number of trainable LayerNorms (e.g., in AdapterL).\\nLoRA adds trainable pairs of rank decomposition matrices in parallel to existing weight matrices.\\nAs mentioned in Section 4.2, we only apply LoRA to WqandWvin most experiments for simplicity.\\nThe number of trainable parameters is determined by the rank rand the shape of the original weights:\\n|Œò|= 2√óÀÜLLoRA√ódmodel√ór, where ÀÜLLoRA is the number of weight matrices we apply LoRA to.\\n6'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 6}, page_content='Model & Method # Trainable E2E NLG Challenge\\nParameters BLEU NIST MET ROUGE-L CIDEr\\nGPT-2 M (FT)* 354.92M 68.2 8.62 46.2 71.0 2.47\\nGPT-2 M (AdapterL)* 0.37M 66.3 8.41 45.0 69.8 2.40\\nGPT-2 M (AdapterL)* 11.09M 68.9 8.71 46.1 71.3 2.47\\nGPT-2 M (AdapterH) 11.09M 67.3¬±.68.50¬±.07 46.0¬±.2 70.7¬±.2 2.44¬±.01\\nGPT-2 M (FTTop2)* 25.19M 68.1 8.59 46.0 70.8 2.41\\nGPT-2 M (PreLayer)* 0.35M 69.7 8.81 46.1 71.4 2.49\\nGPT-2 M (LoRA) 0.35M 70.4¬±.18.85¬±.02 46.8¬±.2 71.8¬±.1 2.53¬±.02\\nGPT-2 L (FT)* 774.03M 68.5 8.78 46.0 69.9 2.45\\nGPT-2 L (AdapterL) 0.88M 69.1¬±.18.68¬±.03 46.3¬±.0 71.4¬±.2 2.49¬±.0\\nGPT-2 L (AdapterL) 23.00M 68.9¬±.38.70¬±.04 46.1¬±.1 71.3¬±.2 2.45¬±.02\\nGPT-2 L (PreLayer)* 0.77M 70.3 8.85 46.2 71.7 2.47\\nGPT-2 L (LoRA) 0.77M 70.4¬±.18.89¬±.02 46.8¬±.2 72.0¬±.2 2.47¬±.02\\nTable 3: GPT-2 medium (M) and large (L) with different adaptation methods on the E2E NLG\\nChallenge. For all metrics, higher is better. LoRA outperforms several baselines with comparable\\nor fewer trainable parameters. ConÔ¨Ådence intervals are shown for experiments we ran. * indicates\\nnumbers published in prior works.\\n5.2 R OBERT A BASE /LARGE\\nRoBERTa (Liu et al., 2019) optimized the pre-training recipe originally proposed in BERT (Devlin\\net al., 2019a) and boosted the latter‚Äôs task performance without introducing many more trainable\\nparameters. While RoBERTa has been overtaken by much larger models on NLP leaderboards\\nsuch as the GLUE benchmark (Wang et al., 2019) in recent years, it remains a competitive and\\npopular pre-trained model for its size among practitioners. We take the pre-trained RoBERTa base\\n(125M) and RoBERTa large (355M) from the HuggingFace Transformers library (Wolf et al., 2020)\\nand evaluate the performance of different efÔ¨Åcient adaptation approaches on tasks from the GLUE\\nbenchmark. We also replicate Houlsby et al. (2019) and Pfeiffer et al. (2021) according to their\\nsetup. To ensure a fair comparison, we make two crucial changes to how we evaluate LoRA when\\ncomparing with adapters. First, we use the same batch size for all tasks and use a sequence length\\nof 128 to match the adapter baselines. Second, we initialize the model to the pre-trained model for\\nMRPC, RTE, and STS-B, not a model already adapted to MNLI like the Ô¨Åne-tuning baseline. Runs\\nfollowing this more restricted setup from Houlsby et al. (2019) are labeled with ‚Ä†. The result is\\npresented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used.\\n5.3 D EBERT AXXL\\nDeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger\\nscale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and Su-\\nperGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully\\nÔ¨Åne-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section).\\nSee Section D.2 for details on the hyperparameters used.\\n5.4 GPT-2 MEDIUM /LARGE\\nHaving shown that LoRA can be a competitive alternative to full Ô¨Åne-tuning on NLU, we hope to\\nanswer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al.,\\nb). We keep our setup as close as possible to Li & Liang (2021) for a direct comparison. Due\\nto space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section.\\nSee Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We\\ninclude a list of the hyperparameters used in Section D.3.\\n7'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 7}, page_content='Model&Method# Trainable WikiSQL MNLI-m SAMSum\\nParameters Acc. (%) Acc. (%) R1/R2/RL\\nGPT-3 (FT) 175,255.8M 73.8 89.5 52.0/28.0/44.5\\nGPT-3 (BitFit) 14.2M 71.3 91.0 51.3/27.4/43.5\\nGPT-3 (PreEmbed) 3.2M 63.1 88.6 48.3/24.2/40.5\\nGPT-3 (PreLayer) 20.2M 70.1 89.5 50.8/27.3/43.5\\nGPT-3 (AdapterH) 7.1M 71.9 89.8 53.0/28.9/44.8\\nGPT-3 (AdapterH) 40.1M 73.2 91.5 53.2/29.0/45.1\\nGPT-3 (LoRA) 4.7M 73.4 91.7 53.8/29.8/45.9\\nGPT-3 (LoRA) 37.7M 74.0 91.6 53.4/29.2/45.1\\nTable 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form\\nvalidation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on\\nSAMSum. LoRA performs better than prior approaches, including full Ô¨Åne-tuning. The results\\non WikiSQL have a Ô¨Çuctuation around ¬±0.5%, MNLI-m around ¬±0.1%, and SAMSum around\\n¬±0.2/¬±0.2/¬±0.1for the three metrics.\\n5.5 S CALING UP TO GPT-3 175B\\nAs a Ô¨Ånal stress test for LoRA, we scale up to GPT-3 with 175 billion parameters. Due to the high\\ntraining cost, we only report the typical standard deviation for a given task over random seeds, as\\nopposed to providing one for every entry. See Section D.4 for details on the hyperparameters used.\\nAs shown in Table 4, LoRA matches or exceeds the Ô¨Åne-tuning baseline on all three datasets. Note\\nthat not all methods beneÔ¨Åt monotonically from having more trainable parameters, as shown in Fig-\\nure 2. We observe a signiÔ¨Åcant performance drop when we use more than 256 special tokens for\\npreÔ¨Åx-embedding tuning or more than 32 special tokens for preÔ¨Åx-layer tuning. This corroborates\\nsimilar observations in Li & Liang (2021). While a thorough investigation into this phenomenon\\nis out-of-scope for this work, we suspect that having more special tokens causes the input distri-\\nbution to shift further away from the pre-training data distribution. Separately, we investigate the\\nperformance of different adaptation approaches in the low-data regime in Section F.3.\\n6 7 8 9 10 11\\nlog10 # Trainable Parameters0.550.600.650.700.75Validation Accuracy\\nWikiSQL\\nMethod\\nFine-Tune\\nPrefixEmbed\\nPrefixLayer\\nAdapter(H)\\nLoRA\\n6 7 8 9 10 11\\nlog10 # Trainable Parameters0.840.860.880.900.92\\nMultiNLI-matched\\nFigure 2: GPT-3 175B validation accuracy vs. number of trainable parameters of several adaptation\\nmethods on WikiSQL and MNLI-matched. LoRA exhibits better scalability and task performance.\\nSee Section F.2 for more details on the plotted data points.\\n6 R ELATED WORKS\\nTransformer Language Models. Transformer (Vaswani et al., 2017) is a sequence-to-sequence\\narchitecture that makes heavy use of self-attention. Radford et al. (a) applied it to autoregressive lan-\\nguage modeling by using a stack of Transformer decoders. Since then, Transformer-based language\\nmodels have dominated NLP, achieving the state-of-the-art in many tasks. A new paradigm emerged\\nwith BERT (Devlin et al., 2019b) and GPT-2 (Radford et al., b) ‚Äì both are large Transformer lan-\\n8'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 8}, page_content='guage models trained on a large amount of text ‚Äì where Ô¨Åne-tuning on task-speciÔ¨Åc data after pre-\\ntraining on general domain data provides a signiÔ¨Åcant performance gain compared to training on\\ntask-speciÔ¨Åc data directly. Training larger Transformers generally results in better performance and\\nremains an active research direction. GPT-3 (Brown et al., 2020) is the largest single Transformer\\nlanguage model trained to-date with 175B parameters.\\nPrompt Engineering and Fine-Tuning. While GPT-3 175B can adapt its behavior with just a\\nfew additional training examples, the result depends heavily on the input prompt (Brown et al.,\\n2020). This necessitates an empirical art of composing and formatting the prompt to maximize a\\nmodel‚Äôs performance on a desired task, which is known as prompt engineering or prompt hacking.\\nFine-tuning retrains a model pre-trained on general domains to a speciÔ¨Åc task Devlin et al. (2019b);\\nRadford et al. (a). Variants of it include learning just a subset of the parameters Devlin et al. (2019b);\\nCollobert & Weston (2008), yet practitioners often retrain all of them to maximize the downstream\\nperformance. However, the enormity of GPT-3 175B makes it challenging to perform Ô¨Åne-tuning in\\nthe usual way due to the large checkpoint it produces and the high hardware barrier to entry since it\\nhas the same memory footprint as pre-training.\\nParameter-EfÔ¨Åcient Adaptation. Many have proposed inserting adapter layers between existing\\nlayers in a neural network (Houlsby et al., 2019; RebufÔ¨Å et al., 2017; Lin et al., 2020). Our method\\nuses a similar bottleneck structure to impose a low-rank constraint on the weight updates. The\\nkey functional difference is that our learned weights can be merged with the main weights during\\ninference, thus not introducing any latency, which is not the case for the adapter layers (Section 3).\\nA comtenporary extension of adapter is COMPACTER (Mahabadi et al., 2021), which essentially\\nparametrizes the adapter layers using Kronecker products with some predetermined weight sharing\\nscheme. Similarly, combining LoRA with other tensor product-based methods could potentially\\nimprove its parameter efÔ¨Åciency, which we leave to future work. More recently, many proposed\\noptimizing the input word embeddings in lieu of Ô¨Åne-tuning, akin to a continuous and differentiable\\ngeneralization of prompt engineering (Li & Liang, 2021; Lester et al., 2021; Hambardzumyan et al.,\\n2020; Liu et al., 2021). We include comparisons with Li & Liang (2021) in our experiment section.\\nHowever, this line of works can only scale up by using more special tokens in the prompt, which\\ntake up available sequence length for task tokens when positional embeddings are learned.\\nLow-Rank Structures in Deep Learning. Low-rank structure is very common in machine learn-\\ning. A lot of machine learning problems have certain intrinsic low-rank structure (Li et al., 2016;\\nCai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013). Moreover, it is known that for many\\ndeep learning tasks, especially those with a heavily over-parametrized neural network, the learned\\nneural network will enjoy low-rank properties after training (Oymak et al., 2019). Some prior works\\neven explicitly impose the low-rank constraint when training the original neural network (Sainath\\net al., 2013; Povey et al., 2018; Zhang et al., 2014; Jaderberg et al., 2014; Zhao et al., 2016; Kho-\\ndak et al., 2021; Denil et al., 2014); however, to the best of our knowledge, none of these works\\nconsiders low-rank update to a frozen model for adaptation to downstream tasks . In theory liter-\\nature, it is known that neural networks outperform other classical learning methods, including the\\ncorresponding (Ô¨Ånite-width) neural tangent kernels (Allen-Zhu et al., 2019; Li & Liang, 2018) when\\nthe underlying concept class has certain low-rank structure (Ghorbani et al., 2020; Allen-Zhu & Li,\\n2019; Allen-Zhu & Li, 2020a). Another theoretical result in Allen-Zhu & Li (2020b) suggests that\\nlow-rank adaptations can be useful for adversarial training. In sum, we believe that our proposed\\nlow-rank adaptation update is well-motivated by the literature.\\n7 U NDERSTANDING THE LOW-RANK UPDATES\\nGiven the empirical advantage of LoRA, we hope to further explain the properties of the low-rank\\nadaptation learned from downstream tasks. Note that the low-rank structure not only lowers the\\nhardware barrier to entry which allows us to run multiple experiments in parallel, but also gives\\nbetter interpretability of how the update weights are correlated with the pre-trained weights. We\\nfocus our study on GPT-3 175B, where we achieved the largest reduction of trainable parameters\\n(up to 10,000√ó) without adversely affecting task performances.\\nWe perform a sequence of empirical studies to answer the following questions: 1) Given a parameter\\nbudget constraint, which subset of weight matrices in a pre-trained Transformer should we adapt\\n9'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 9}, page_content='to maximize downstream performance? 2) Is the ‚Äúoptimal‚Äù adaptation matrix ‚àÜWreally rank-\\ndeÔ¨Åcient ? If so, what is a good rank to use in practice? 3) What is the connection between ‚àÜWand\\nW? Does ‚àÜWhighly correlate with W? How large is ‚àÜWcomparing to W?\\nWe believe that our answers to question (2) and (3) shed light on the fundamental principles of using\\npre-trained language models for downstream tasks, which is a critical topic in NLP.\\n7.1 W HICH WEIGHT MATRICES IN TRANSFORMER SHOULD WEAPPLY LORA TO?\\nGiven a limited parameter budget, which types of weights should we adapt with LoRA to obtain\\nthe best performance on downstream tasks? As mentioned in Section 4.2, we only consider weight\\nmatrices in the self-attention module. We set a parameter budget of 18M (roughly 35MB if stored\\nin FP16) on GPT-3 175B, which corresponds to r= 8if we adapt one type of attention weights or\\nr= 4if we adapt two types, for all 96 layers. The result is presented in Table 5.\\n# of Trainable Parameters = 18M\\nWeight Type WqWkWvWoWq,WkWq,WvWq,Wk,Wv,Wo\\nRankr 8 8 8 8 4 4 2\\nWikiSQL (¬±0.5%) 70.4 70.0 73.0 73.2 71.4 73.7 73.7\\nMultiNLI (¬±0.1%) 91.0 90.8 91.0 91.3 91.3 91.3 91.7\\nTable 5: Validation accuracy on WikiSQL and MultiNLI after applying LoRA to different types of\\nattention weights in GPT-3, given the same number of trainable parameters. Adapting both Wqand\\nWvgives the best performance overall. We Ô¨Ånd the standard deviation across random seeds to be\\nconsistent for a given dataset, which we report in the Ô¨Årst column.\\nNote that putting all the parameters in ‚àÜWqor‚àÜWkresults in signiÔ¨Åcantly lower performance,\\nwhile adapting both WqandWvyields the best result. This suggests that even a rank of four\\ncaptures enough information in ‚àÜWsuch that it is preferable to adapt more weight matrices than\\nadapting a single type of weights with a larger rank.\\n7.2 W HAT IS THE OPTIMAL RANKrFOR LORA?\\nWe turn our attention to the effect of rank ron model performance. We adapt {Wq,Wv},\\n{Wq,Wk,Wv,Wc}, and justWqfor a comparison.\\nWeight Type r= 1r= 2r= 4r= 8r= 64\\nWikiSQL(¬±0.5%)Wq 68.8 69.6 70.5 70.4 70.0\\nWq,Wv 73.4 73.3 73.7 73.8 73.5\\nWq,Wk,Wv,Wo 74.1 73.7 74.0 74.0 73.9\\nMultiNLI (¬±0.1%)Wq 90.7 90.9 91.1 90.7 90.7\\nWq,Wv 91.3 91.4 91.3 91.6 91.4\\nWq,Wk,Wv,Wo 91.2 91.7 91.7 91.5 91.4\\nTable 6: Validation accuracy on WikiSQL and MultiNLI with different rank r. To our surprise, a\\nrank as small as one sufÔ¨Åces for adapting both WqandWvon these datasets while training Wqalone\\nneeds a larger r. We conduct a similar experiment on GPT-2 in Section H.2.\\nTable 6 shows that, surprisingly, LoRA already performs competitively with a very small r(more\\nso for{Wq,Wv}than justWq). This suggests the update matrix ‚àÜWcould have a very small\\n‚Äúintrinsic rank‚Äù.6To further support this Ô¨Ånding, we check the overlap of the subspaces learned by\\ndifferent choices of rand by different random seeds. We argue that increasing rdoes not cover a\\nmore meaningful subspace, which suggests that a low-rank adaptation matrix is sufÔ¨Åcient.\\n6However, we do not expect a small rto work for every task or dataset. Consider the following thought\\nexperiment: if the downstream task were in a different language than the one used for pre-training, retraining\\nthe entire model (similar to LoRA with r=dmodel ) could certainly outperform LoRA with a small r.\\n10'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 10}, page_content='Subspace similarity between different r.GivenAr=8andAr=64which are the learned adapta-\\ntion matrices with rank r= 8and64using the same pre-trained model , we perform singular value\\ndecomposition and obtain the right-singular unitary matrices UAr=8andUAr=64.7We hope to an-\\nswer: how much of the subspace spanned by the top isingular vectors in UAr=8(for1‚â§i‚â§8) is\\ncontained in the subspace spanned by top jsingular vectors of UAr=64(for1‚â§j‚â§64)? We mea-\\nsure this quantity with a normalized subspace similarity based on the Grassmann distance (See Ap-\\npendix G for a more formal discussion)\\nœÜ(Ar=8,Ar=64,i,j) =||Ui‚ä§\\nAr=8Uj\\nAr=64||2\\nF\\nmin(i,j)‚àà[0,1] (4)\\nwhereUi\\nAr=8represents the columns of UAr=8corresponding to the top- isingular vectors.\\nœÜ(¬∑)has a range of [0,1], where 1represents a complete overlap of subspaces and 0a complete\\nseparation. See Figure 3 for how œÜchanges as we vary iandj. We only look at the 48th layer\\n(out of 96) due to space constraint, but the conclusion holds for other layers as well, as shown\\nin Section H.1.\\n0.00.20.40.60.81.0\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\nj12345678iWq\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\njWv\\n12345678\\njWq\\n12345678\\njWv\\n(Ar=64,Ar=8,i,j)\\nFigure 3: Subspace similarity between column vectors of Ar=8andAr=64for both ‚àÜWqand‚àÜWv.\\nThe third and the fourth Ô¨Ågures zoom in on the lower-left triangle in the Ô¨Årst two Ô¨Ågures. The top\\ndirections in r= 8are included in r= 64 , and vice versa.\\nWe make an important observation from Figure 3.\\nDirections corresponding to the top singular vector overlap signiÔ¨Åcantly between\\nAr=8andAr=64, while others do not. SpeciÔ¨Åcally, ‚àÜWv(resp. ‚àÜWq) ofAr=8\\nand‚àÜWv(resp. ‚àÜWq) ofAr=64share a subspace of dimension 1 with normalized\\nsimilarity>0.5, providing an explanation of why r= 1 performs quite well in our\\ndownstream tasks for GPT-3.\\nSince bothAr=8andAr=64are learned using the same pre-trained model, Figure 3 indicates that\\nthe top singular-vector directions of Ar=8andAr=64are the most useful, while other directions\\npotentially contain mostly random noises accumulated during training. Hence, the adaptation matrix\\ncan indeed have a very low rank.\\nSubspace similarity between different random seeds. We further conÔ¨Årm this by plotting the\\nnormalized subspace similarity between two randomly seeded runs with r= 64 , shown in Figure 4.\\n‚àÜWqappears to have a higher ‚Äúintrinsic rank‚Äù than ‚àÜWv, since more common singular value direc-\\ntions are learned by both runs for ‚àÜWq, which is in line with our empirical observation in Table 6.\\nAs a comparison, we also plot two random Gaussian matrices, which do not share any common\\nsingular value directions with each other.\\n7.3 H OWDOES THE ADAPTATION MATRIX ‚àÜWCOMPARE TO W?\\nWe further investigate the relationship between ‚àÜWandW. In particular, does ‚àÜWhighly correlate\\nwithW? (Or mathematically, is ‚àÜWmostly contained in the top singular directions of W?) Also,\\n7Note that a similar analysis can be carried out with Band the left-singular unitary matrices ‚Äì we stick with\\nAfor our experiments.\\n11'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 11}, page_content='0.00.10.20.30.40.5\\n1\\n5\\n10\\n15\\n20\\n25\\n30\\n34\\n39\\n44\\n49\\n54\\n59\\nj1\\n8\\n16\\n24\\n32\\n40\\n48\\n56iWq\\n1\\n5\\n10\\n15\\n20\\n25\\n30\\n34\\n39\\n44\\n49\\n54\\n59\\nj(Ar=64,A‚Ä≤r=64,i,j)\\nWv\\n1\\n5\\n10\\n15\\n20\\n25\\n30\\n34\\n39\\n44\\n49\\n54\\n59\\njRandom GaussianFigure 4: Left and Middle: Normalized subspace similarity between the column vectors of Ar=64\\nfrom two random seeds, for both ‚àÜWqand‚àÜWvin the 48-th layer. Right: the same heat-map\\nbetween the column vectors of two random Gaussian matrices. See Section H.1 for other layers.\\nhow ‚Äúlarge‚Äù is ‚àÜWcomparing to its corresponding directions in W? This can shed light on the\\nunderlying mechanism for adapting pre-trained language models.\\nTo answer these questions, we project Wonto ther-dimensional subspace of ‚àÜWby comput-\\ningU‚ä§WV‚ä§, withU/Vbeing the left/right singular-vector matrix of ‚àÜW. Then, we com-\\npare the Frobenius norm between ‚à•U‚ä§WV‚ä§‚à•Fand‚à•W‚à•F. As a comparison, we also compute\\n‚à•U‚ä§WV‚ä§‚à•Fby replacing U,V with the top rsingular vectors of Wor a random matrix.\\nr= 4 r= 64\\n‚àÜWqWq Random ‚àÜWqWq Random\\n||U‚ä§WqV‚ä§||F= 0.32 21.67 0.02 1.90 37.71 0.33\\n||Wq||F= 61.95||‚àÜWq||F= 6.91||‚àÜWq||F= 3.57\\nTable 7: The Frobenius norm of U‚ä§WqV‚ä§whereUandVare the left/right top rsingular vector\\ndirections of either (1) ‚àÜWq, (2)Wq, or (3) a random matrix. The weight matrices are taken from\\nthe 48th layer of GPT-3.\\nWe draw several conclusions from Table 7. First, ‚àÜWhas a stronger correlation with Wcompared\\nto a random matrix, indicating that ‚àÜWampliÔ¨Åes some features that are already in W. Second,\\ninstead of repeating the top singular directions of W,‚àÜWonly ampliÔ¨Åes directions that are not\\nemphasized in W. Third, the ampliÔ¨Åcation factor is rather huge: 21.5‚âà6.91/0.32forr= 4.\\nSee Section H.4 for why r= 64 has a smaller ampliÔ¨Åcation factor. We also provide a visualization\\nin Section H.3 for how the correlation changes as we include more top singular directions from Wq.\\nThis suggests that the low-rank adaptation matrix potentially ampliÔ¨Åes the important features for\\nspeciÔ¨Åc downstream tasks that were learned but not emphasized in the general pre-training model .\\n8 C ONCLUSION AND FUTURE WORK\\nFine-tuning enormous language models is prohibitively expensive in terms of the hardware required\\nand the storage/switching cost for hosting independent instances for different tasks. We propose\\nLoRA, an efÔ¨Åcient adaptation strategy that neither introduces inference latency nor reduces input\\nsequence length while retaining high model quality. Importantly, it allows for quick task-switching\\nwhen deployed as a service by sharing the vast majority of the model parameters. While we focused\\non Transformer language models, the proposed principles are generally applicable to any neural\\nnetworks with dense layers.\\nThere are many directions for future works. 1) LoRA can be combined with other efÔ¨Åcient adapta-\\ntion methods, potentially providing orthogonal improvement. 2) The mechanism behind Ô¨Åne-tuning\\nor LoRA is far from clear ‚Äì how are features learned during pre-training transformed to do well\\non downstream tasks? We believe that LoRA makes it more tractable to answer this than full Ô¨Åne-\\n12'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 12}, page_content='tuning. 3) We mostly depend on heuristics to select the weight matrices to apply LoRA to. Are\\nthere more principled ways to do it? 4) Finally, the rank-deÔ¨Åciency of ‚àÜWsuggests that Wcould\\nbe rank-deÔ¨Åcient as well, which can also be a source of inspiration for future works.\\nREFERENCES\\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains the\\nEffectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs] , December 2020. URL\\nhttp://arxiv.org/abs/2012.13255 .\\nZeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn EfÔ¨Åciently, Going Beyond Kernels? In\\nNeurIPS , 2019. Full version available at http://arxiv.org/abs/1905.10337 .\\nZeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep\\nlearning. arXiv preprint arXiv:2001.04413 , 2020a.\\nZeyuan Allen-Zhu and Yuanzhi Li. Feature puriÔ¨Åcation: How adversarial training performs robust\\ndeep learning. arXiv preprint arXiv:2005.10190 , 2020b.\\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-\\nparameterization. In ICML , 2019. Full version available at http://arxiv.org/abs/1811.\\n03962 .\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\\nIlya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165\\n[cs], July 2020. URL http://arxiv.org/abs/2005.14165 .\\nJian-Feng Cai, Emmanuel J Cand `es, and Zuowei Shen. A singular value thresholding algorithm for\\nmatrix completion. SIAM Journal on optimization , 20(4):1956‚Äì1982, 2010.\\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task\\n1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of\\nthe 11th International Workshop on Semantic Evaluation (SemEval-2017) , 2017. doi: 10.18653/\\nv1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001 .\\nRonan Collobert and Jason Weston. A uniÔ¨Åed architecture for natural language processing: deep\\nneural networks with multitask learning. In Proceedings of the 25th international conference\\non Machine learning , ICML ‚Äô08, pp. 160‚Äì167, New York, NY , USA, July 2008. Association\\nfor Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URL\\nhttps://doi.org/10.1145/1390156.1390177 .\\nMisha Denil, Babak Shakibi, Laurent Dinh, Marc‚ÄôAurelio Ranzato, and Nando de Freitas. Predicting\\nparameters in deep learning, 2014.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\\nbidirectional transformers for language understanding, 2019a.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep\\nBidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs] , May 2019b.\\nURL http://arxiv.org/abs/1810.04805 . arXiv: 1810.04805.\\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\\nInProceedings of the Third International Workshop on Paraphrasing (IWP2005) , 2005. URL\\nhttps://aclanthology.org/I05-5002 .\\nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlg\\nchallenge: Generating text from rdf data. In Proceedings of the 10th International Conference on\\nNatural Language Generation , pp. 124‚Äì133, 2017.\\n13'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 13}, page_content='Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural\\nnetworks outperform kernel methods? arXiv preprint arXiv:2006.13409 , 2020.\\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-\\nannotated dialogue dataset for abstractive summarization. CoRR , abs/1911.12237, 2019. URL\\nhttp://arxiv.org/abs/1911.12237 .\\nLars Grasedyck, Daniel Kressner, and Christine Tobler. A literature survey of low-rank tensor\\napproximation techniques. GAMM-Mitteilungen , 36(1):53‚Äì78, 2013.\\nJihun Ham and Daniel D. Lee. Grassmann discriminant analysis: a unifying view on subspace-based\\nlearning. In ICML , pp. 376‚Äì383, 2008. URL https://doi.org/10.1145/1390156.\\n1390204 .\\nKaren Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial\\nReProgramming. arXiv:2101.00121 [cs] , December 2020. URL http://arxiv.org/abs/\\n2101.00121 . arXiv: 2101.00121.\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert\\nwith disentangled attention, 2021.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,\\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-EfÔ¨Åcient Transfer Learning\\nfor NLP. arXiv:1902.00751 [cs, stat] , June 2019. URL http://arxiv.org/abs/1902.\\n00751 .\\nMax Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks\\nwith low rank expansions. arXiv preprint arXiv:1405.3866 , 2014.\\nMikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicol `o Fusi. Initialization and regularization\\nof factorized neural layers, 2021.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\\ncomputation and automatic sharding, 2020.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-EfÔ¨Åcient Prompt\\nTuning. arXiv:2104.08691 [cs] , April 2021. URL http://arxiv.org/abs/2104.08691 .\\narXiv: 2104.08691.\\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Di-\\nmension of Objective Landscapes. arXiv:1804.08838 [cs, stat] , April 2018a. URL http:\\n//arxiv.org/abs/1804.08838 . arXiv: 1804.08838.\\nXiang Lisa Li and Percy Liang. PreÔ¨Åx-Tuning: Optimizing Continuous Prompts for Generation.\\narXiv:2101.00190 [cs] , January 2021. URL http://arxiv.org/abs/2101.00190 .\\nYuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient\\ndescent on structured data. In Advances in Neural Information Processing Systems , 2018.\\nYuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank ap-\\nproximation via alternating minimization. In International Conference on Machine Learning , pp.\\n2358‚Äì2367. PMLR, 2016.\\nYuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized\\nmatrix sensing and neural networks with quadratic activations. In Conference On Learning The-\\nory, pp. 2‚Äì47. PMLR, 2018b.\\nZhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\\nvia parameter-efÔ¨Åcient transfer learning. In Findings of the Association for Computational Lin-\\nguistics: EMNLP 2020 , pp. 441‚Äì459, Online, November 2020. Association for Computational\\nLinguistics. doi: 10.18653/v1/2020.Ô¨Åndings-emnlp.41. URL https://aclanthology.\\norg/2020.findings-emnlp.41 .\\n14'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 14}, page_content='Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT\\nUnderstands, Too. arXiv:2103.10385 [cs] , March 2021. URL http://arxiv.org/abs/\\n2103.10385 . arXiv: 2103.10385.\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\\napproach, 2019.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101 , 2017.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: EfÔ¨Åcient low-rank\\nhypercomplex adapter layers, 2021.\\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,\\nXiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured\\ndata record to text generation. arXiv preprint arXiv:2007.02871 , 2020.\\nJekaterina Novikova, Ond Àárej Du Àásek, and Verena Rieser. The e2e dataset: New challenges for end-\\nto-end generation. arXiv preprint arXiv:1706.09254 , 2017.\\nSamet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran-\\ntees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprint\\narXiv:1906.05392 , 2019.\\nJonas Pfeiffer, Aishwarya Kamath, Andreas R ¬®uckl¬¥e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\\nfusion: Non-destructive task composition for transfer learning, 2021.\\nDaniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and San-\\njeev Khudanpur. Semi-orthogonal low-rank matrix factorization for deep neural networks. In\\nInterspeech , pp. 3743‚Äì3747, 2018.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Under-\\nstanding by Generative Pre-Training. pp. 12, a.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\\nModels are Unsupervised Multitask Learners. pp. 24, b.\\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don‚Äôt know: Unanswerable questions\\nfor squad. CoRR , abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822 .\\nSylvestre-Alvise RebufÔ¨Å, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with\\nresidual adapters. arXiv:1705.08045 [cs, stat] , November 2017. URL http://arxiv.org/\\nabs/1705.08045 . arXiv: 1705.08045.\\nAndreas R ¬®uckl¬¥e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and\\nIryna Gurevych. Adapterdrop: On the efÔ¨Åciency of adapters in transformers, 2020.\\nTara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low-\\nrank matrix factorization for deep neural network training with high-dimensional output targets.\\nIn2013 IEEE international conference on acoustics, speech and signal processing , pp. 6655‚Äì\\n6659. IEEE, 2013.\\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model par-\\nallelism, 2020.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,\\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\\ntreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\\nProcessing , pp. 1631‚Äì1642, Seattle, Washington, USA, October 2013. Association for Computa-\\ntional Linguistics. URL https://aclanthology.org/D13-1170 .\\n15'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 15}, page_content='Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\n≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st In-\\nternational Conference on Neural Information Processing Systems , pp. 6000‚Äì6010, 2017.\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\\nGlue: A multi-task benchmark and analysis platform for natural language understanding, 2019.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language\\nunderstanding systems, 2020.\\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\\narXiv preprint arXiv:1805.12471 , 2018.\\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-\\ntence understanding through inference. In Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technolo-\\ngies, Volume 1 (Long Papers) , pp. 1112‚Äì1122, New Orleans, Louisiana, June 2018. Association\\nfor Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb.\\norg/anthology/N18-1101 .\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\\nPierric Cistac, Tim Rault, R ¬¥emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-\\nger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art\\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing: System Demonstrations , pp. 38‚Äì45, Online, October 2020. As-\\nsociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\\n2020.emnlp-demos.6 .\\nGreg Yang and Edward J. Hu. Feature Learning in InÔ¨Ånite-Width Neural Networks.\\narXiv:2011.14522 [cond-mat] , May 2021. URL http://arxiv.org/abs/2011.14522 .\\narXiv: 2011.14522.\\nElad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. BitÔ¨Åt: Simple parameter-efÔ¨Åcient Ô¨Åne-tuning\\nfor transformer-based masked language-models, 2021.\\nYu Zhang, Ekapol Chuangsuwanich, and James Glass. Extracting deep neural network bottleneck\\nfeatures using low-rank matrix factorization. In 2014 IEEE international conference on acoustics,\\nspeech and signal processing (ICASSP) , pp. 185‚Äì189. IEEE, 2014.\\nYong Zhao, Jinyu Li, and Yifan Gong. Low-rank plus diagonal adaptation for deep neural networks.\\nIn2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) ,\\npp. 5005‚Äì5009. IEEE, 2016.\\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from\\nnatural language using reinforcement learning. CoRR , abs/1709.00103, 2017. URL http://\\narxiv.org/abs/1709.00103 .\\nA L ARGE LANGUAGE MODELS STILL NEED PARAMETER UPDATES\\nFew-shot learning, or prompt engineering, is very advantageous when we only have a handful of\\ntraining samples. However, in practice, we can often afford to curate a few thousand or more training\\nexamples for performance-sensitive applications. As shown in Table 8, Ô¨Åne-tuning improves the\\nmodel performance drastically compared to few-shot learning on datasets large and small. We take\\nthe GPT-3 few-shot result on RTE from the GPT-3 paper (Brown et al., 2020). For MNLI-matched,\\nwe use two demonstrations per class and six in-context examples in total.\\n16'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 16}, page_content='Method MNLI-m (Val. Acc./%) RTE (Val. Acc./%)\\nGPT-3 Few-Shot 40.6 69.0\\nGPT-3 Fine-Tuned 89.5 85.4\\nTable 8: Fine-tuning signiÔ¨Åcantly outperforms few-shot learning on GPT-3 (Brown et al., 2020).\\nB I NFERENCE LATENCY INTRODUCED BY ADAPTER LAYERS\\nAdapter layers are external modules added to a pre-trained model in a sequential manner, whereas\\nour proposal, LoRA, can be seen as external modules added in a parallel manner. Consequently,\\nadapter layers must be computed in addition to the base model, inevitably introducing additional\\nlatency. While as pointed out in R ¬®uckl¬¥e et al. (2020), the latency introduced by adapter layers can\\nbe mitigated when the model batch size and/or sequence length is large enough to full utilize the\\nhardware parallelism. We conÔ¨Årm their observation with a similar latency study on GPT-2 medium\\nand point out that there are scenarios, notably online inference where the batch size is small, where\\nthe added latency can be signiÔ¨Åcant.\\nWe measure the latency of a single forward pass on an NVIDIA Quadro RTX8000 by averaging\\nover 100 trials. We vary the input batch size, sequence length, and the adapter bottleneck dimension\\nr. We test two adapter designs: the original one by Houlsby et al. (2019), which we call AdapterH,\\nand a recent, more efÔ¨Åcient variant by Lin et al. (2020), which we call AdapterL. See Section 5.1\\nfor more details on the designs. We plot the slow-down in percentage compared to the no-adapter\\nbaseline in Figure 5.\\n05101520253035\\n0 10 100 250AdapterH rSeq Len = 128 Seq Len = 256 Seq Len = 512\\n1 2 4 8 16 32\\nBatch Size0 10 100 250AdapterL r\\n1 2 4 8 16 32\\nBatch Size1 2 4 8 16 32\\nBatch Size\\nFigure 5: Percentage slow-down of inference latency compared to the no-adapter ( r= 0) baseline.\\nThe top row shows the result for AdapterHand the bottom row AdapterL. Larger batch size and\\nsequence length help to mitigate the latency, but the slow-down can be as high as over 30% in an\\nonline, short-sequence-length scenario. We tweak the colormap for better visibility.\\nC D ATASET DETAILS\\nGLUE Benchmark is a wide-ranging collection of natural language understanding tasks. It includes\\nMNLI (inference, Williams et al. (2018)), SST-2 (sentiment analysis, Socher et al. (2013)), MRPC\\n(paraphrase detection, Dolan & Brockett (2005)), CoLA (linguistic acceptability, Warstadt et al.\\n(2018)), QNLI (inference, Rajpurkar et al. (2018)), QQP8(question-answering), RTE (inference),\\n8https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs\\n17'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 17}, page_content='and STS-B (textual similarity, Cer et al. (2017)). The broad coverage makes GLUE benchmark a\\nstandard metric to evaluate NLU models such as RoBERTa and DeBERTa. The individual datasets\\nare released under different permissive licenses.\\nWikiSQL is introduced in Zhong et al. (2017) and contains 56,355/8,421training/validation ex-\\namples. The task is to generate SQL queries from natural language questions and table schemata.\\nWe encode context as x={table schema ,query}and target as y={SQL}. The dataset is release\\nunder the BSD 3-Clause License.\\nSAMSum is introduced in Gliwa et al. (2019) and contains 14,732/819training/test examples. It\\nconsists of staged chat conversations between two people and corresponding abstractive summaries\\nwritten by linguists. We encode context as ‚Äù \\\\n‚Äù concatenated utterances followed by a ‚Äù \\\\n\\\\n‚Äù,\\nand target as y={summary}. The dataset is released under the non-commercial licence: Creative\\nCommons BY-NC-ND 4.0.\\nE2E NLG Challenge was Ô¨Årst introduced in Novikova et al. (2017) as a dataset for training end-to-\\nend, data-driven natural language generation systems and is commonly used for data-to-text evalua-\\ntion. The E2E dataset consists of roughly 42,000training, 4,600validation, and 4,600test exam-\\nples from the restaurant domain. Each source table used as input can have multiple references. Each\\nsample input (x,y)consists of a sequence of slot-value pairs, along with a corresponding natural\\nlanguage reference text. The dataset is released under Creative Commons BY-NC-SA 4.0.\\nDART is an open-domain data-to-text dataset described in Nan et al. (2020). DART inputs are\\nstructured as sequences of ENTITY ‚Äî RELATION ‚Äî ENTITY triples. With 82Kexamples in\\ntotal, DART is a signiÔ¨Åcantly larger and more complex data-to-text task compared to E2E. The\\ndataset is released under the MIT license.\\nWebNLG is another commonly used dataset for data-to-text evaluation (Gardent et al., 2017). With\\n22Kexamples in total WebNLG comprises 14 distinct categories, nine of which are seen during\\ntraining. Since Ô¨Åve of the 14 total categories are not seen during training, but are represented in\\nthe test set, evaluation is typically broken out by ‚Äúseen‚Äù categories (S), ‚Äúunseen‚Äù categories (U)\\nand ‚Äúall‚Äù (A). Each input example is represented by a sequence of SUBJECT ‚Äî PROPERTY ‚Äî\\nOBJECT triples. The dataset is released under Creative Commons BY-NC-SA 4.0.\\nD H YPERPARAMETERS USED IN EXPERIMENTS\\nD.1 R OBERT A\\nWe train using AdamW with a linear learning rate decay schedule. We sweep learning rate, number\\nof training epochs, and batch size for LoRA. Following Liu et al. (2019), we initialize the LoRA\\nmodules to our best MNLI checkpoint when adapting to MRPC, RTE, and STS-B, instead of the\\nusual initialization; the pre-trained model stays frozen for all tasks. We report the median over 5\\nrandom seeds; the result for each run is taken from the best epoch. For a fair comparison with the\\nsetup in Houlsby et al. (2019) and Pfeiffer et al. (2021), we restrict the model sequence length to 128\\nand used a Ô¨Åxed batch size for all tasks. Importantly, we start with the pre-trained RoBERTa large\\nmodel when adapting to MRPC, RTE, and STS-B, instead of a model already adapted to MNLI.\\nThe runs with this restricted setup are marked with ‚Ä†. See the hyperparameters used in our runs\\nin Table 9.\\nD.2 D EBERT A\\nWe again train using AdamW with a linear learning rate decay schedule. Following He et al. (2021),\\nwe tune learning rate, dropout probability, warm-up steps, and batch size. We use the same model\\nsequence length used by (He et al., 2021) to keep our comparison fair. Following He et al. (2021),\\nwe initialize the LoRA modules to our best MNLI checkpoint when adapting to MRPC, RTE, and\\nSTS-B, instead of the usual initialization; the pre-trained model stays frozen for all tasks. We report\\nthe median over 5 random seeds; the result for each run is taken from the best epoch. See the\\nhyperparameters used in our runs in Table 10.\\n18'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 18}, page_content='Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B\\nOptimizer AdamW\\nWarmup Ratio 0.06\\nLR Schedule Linear\\nRoBERTa base\\nLoRABatch Size 16 16 16 32 32 16 32 16\\n# Epochs 30 60 30 80 25 25 80 40\\nLearning Rate 5E-04 5E-04 4E-04 4E-04 4E-04 5E-04 5E-04 4E-04\\nLoRA ConÔ¨Åg. rq=rv= 8\\nLoRAŒ± 8\\nMax Seq. Len. 512\\nRoBERTa large\\nLoRABatch Size 4 4 4 4 4 4 8 8\\n# Epochs 10 10 20 20 10 20 20 30\\nLearning Rate 3E-04 4E-04 3E-04 2E-04 2E-04 3E-04 4E-04 2E-04\\nLoRA ConÔ¨Åg. rq=rv= 8\\nLoRAŒ± 16\\nMax Seq. Len. 128 128 512 128 512 512 512 512\\nRoBERTa large\\nLoRA‚Ä†Batch Size 4\\n# Epochs 10 10 20 20 10 20 20 10\\nLearning Rate 3E-04 4E-04 3E-04 2E-04 2E-04 3E-04 4E-04 2E-04\\nLoRA ConÔ¨Åg. rq=rv= 8\\nLoRAŒ± 16\\nMax Seq. Len. 128\\nRoBERTa large\\nAdptP(3M)‚Ä†Batch Size 32\\n# Epochs 10 20 20 20 10 20 20 20\\nLearning Rate 3E-05 3E-05 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\\nBottleneckr 64\\nMax Seq. Len. 128\\nRoBERTa large\\nAdptP(0.8M)‚Ä†Batch Size 32\\n# Epochs 5 20 20 20 10 20 20 20\\nLearning Rate 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\\nBottleneckr 16\\nMax Seq. Len. 128\\nRoBERTa large\\nAdptH(6M)‚Ä†Batch Size 32\\n# Epochs 10 5 10 10 5 20 20 10\\nLearning Rate 3E-05 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\\nBottleneckr 64\\nMax Seq. Len. 128\\nRoBERTa large\\nAdptH(0.8M)‚Ä†Batch Size 32\\n# Epochs 10 5 10 10 5 20 20 10\\nLearning Rate 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04 3E-04\\nBottleneckr 8\\nMax Seq. Len. 128\\nTable 9: The hyperparameters we used for RoBERTa on the GLUE benchmark.\\nD.3 GPT-2\\nWe train all of our GPT-2 models using AdamW (Loshchilov & Hutter, 2017) with a linear learning\\nrate schedule for 5 epochs. We use the batch size, learning rate, and beam search beam size described\\nin Li & Liang (2021). Accordingly, we also tune the above hyperparameters for LoRA. We report the\\nmean over 3 random seeds; the result for each run is taken from the best epoch. The hyperparameters\\nused for LoRA in GPT-2 are listed in Table 11. For those used for other baselines, see Li & Liang\\n(2021).\\nD.4 GPT-3\\nFor all GPT-3 experiments, we train using AdamW (Loshchilov & Hutter, 2017) for 2 epochs with\\na batch size of 128 samples and a weight decay factor of 0.1. We use a sequence length of 384 for\\n19'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 19}, page_content='Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B\\nOptimizer AdamW\\nWarmup Ratio 0.1\\nLR Schedule Linear\\nDeBERTa XXL\\nLoRABatch Size 8 8 32 4 6 8 4 4\\n# Epochs 5 16 30 10 8 11 11 10\\nLearning Rate 1E-04 6E-05 2E-04 1E-04 1E-04 1E-04 2E-04 2E-04\\nWeight Decay 0 0.01 0.01 0 0.01 0.01 0.01 0.1\\nCLS Dropout 0.15 0 0 0.1 0.1 0.2 0.2 0.2\\nLoRA ConÔ¨Åg. rq=rv= 8\\nLoRAŒ± 8\\nMax Seq. Len. 256 128 128 64 512 320 320 128\\nTable 10: The hyperparameters for DeBERTa XXL on tasks included in the GLUE benchmark.\\nDataset E2E WebNLG DART\\nTraining\\nOptimizer AdamW\\nWeight Decay 0.01 0.01 0.0\\nDropout Prob 0.1 0.1 0.0\\nBatch Size 8\\n# Epoch 5\\nWarmup Steps 500\\nLearning Rate Schedule Linear\\nLabel Smooth 0.1 0.1 0.0\\nLearning Rate 0.0002\\nAdaptation rq=rv= 4\\nLoRAŒ± 32\\nInference\\nBeam Size 10\\nLength Penalty 0.9 0.8 0.8\\nno repeat ngram size 4\\nTable 11: The hyperparameters for GPT-2 LoRA on E2E, WebNLG and DART.\\nWikiSQL (Zhong et al., 2017), 768 for MNLI (Williams et al., 2018), and 2048 for SAMSum (Gliwa\\net al., 2019). We tune learning rate for all method-dataset combinations. See Section D.4 for more\\ndetails on the hyperparameters used. For preÔ¨Åx-embedding tuning, we Ô¨Ånd the optimal lpandli\\nto be 256 and 8, respectively, totalling 3.2Mtrainable parameters. We use lp= 8 andli= 8 for\\npreÔ¨Åx-layer tuning with 20.2Mtrainable parameters to obtain the overall best performance. We\\npresent two parameter budgets for LoRA: 4.7M ( rq=rv= 1orrv= 2) and 37.7M ( rq=rv= 8\\norrq=rk=rv=ro= 2). We report the best validation performance from each run. The training\\nhyperparameters used in our GPT-3 experiments are listed in Table 12.\\nE C OMBINING LORA WITH PREFIX TUNING\\nLoRA can be naturally combined with existing preÔ¨Åx-based approaches. In this section, we evaluate\\ntwo combinations of LoRA and variants of preÔ¨Åx-tuning on WikiSQL and MNLI.\\nLoRA+PreÔ¨ÅxEmbed (LoRA+PE) combines LoRA with preÔ¨Åx-embedding tuning, where we insert\\nlp+lispecial tokens whose embeddings are treated as trainable parameters. For more on preÔ¨Åx-\\nembedding tuning, see Section 5.1.\\nLoRA+PreÔ¨ÅxLayer (LoRA+PL) combines LoRA with preÔ¨Åx-layer tuning. We also insert lp+li\\nspecial tokens; however, instead of letting the hidden representations of these tokens evolve natu-\\n20'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 20}, page_content='Hyperparameters Fine-Tune PreEmbed PreLayer BitFit AdapterHLoRA\\nOptimizer AdamW\\nBatch Size 128\\n# Epoch 2\\nWarmup Tokens 250,000\\nLR Schedule Linear\\nLearning Rate 5.00E-06 5.00E-04 1.00E-04 1.6E-03 1.00E-04 2.00E-04\\nTable 12: The training hyperparameters used for different GPT-3 adaption methods. We use the\\nsame hyperparameters for all datasets after tuning learning rate.\\nrally, we replace them after every Transformer block with an input agnostic vector. Thus, both the\\nembeddings and subsequent Transformer block activations are treated as trainable parameters. For\\nmore on preÔ¨Åx-layer tuning, see Section 5.1.\\nIn Table 15, we show the evaluation results of LoRA+PE and LoRA+PL on WikiSQL and MultiNLI.\\nFirst of all, LoRA+PE signiÔ¨Åcantly outperforms both LoRA and preÔ¨Åx-embedding tuning on\\nWikiSQL, which indicates that LoRA is somewhat orthogonal to preÔ¨Åx-embedding tuning. On\\nMultiNLI, the combination of LoRA+PE doesn‚Äôt perform better than LoRA, possibly because LoRA\\non its own already achieves performance comparable to the human baseline. Secondly, we notice\\nthat LoRA+PL performs slightly worse than LoRA even with more trainable parameters. We at-\\ntribute this to the fact that preÔ¨Åx-layer tuning is very sensitive to the choice of learning rate and thus\\nmakes the optimization of LoRA weights more difÔ¨Åcult in LoRA+PL.\\nF A DDITIONAL EMPIRICAL EXPERIMENTS\\nF.1 A DDITIONAL EXPERIMENTS ON GPT-2\\nWe also repeat our experiment on DART (Nan et al., 2020) and WebNLG (Gardent et al., 2017)\\nfollowing the setup of Li & Liang (2021). The result is shown in Table 13. Similar to our result\\non E2E NLG Challenge, reported in Section 5, LoRA performs better than or at least on-par with\\npreÔ¨Åx-based approaches given the same number of trainable parameters.\\nMethod # Trainable DART\\nParameters BLEU‚ÜëMET‚ÜëTER‚Üì\\nGPT-2 Medium\\nFine-Tune 354M 46.2 0.39 0.46\\nAdapterL0.37M 42.4 0.36 0.48\\nAdapterL11M 45.2 0.38 0.46\\nFTTop224M 41.0 0.34 0.56\\nPrefLayer 0.35M 46.4 0.38 0.46\\nLoRA 0.35M 47.1¬±.2 0.39 0.46\\nGPT-2 Large\\nFine-Tune 774M 47.0 0.39 0.46\\nAdapterL0.88M 45.7¬±.1 0.38 0.46\\nAdapterL23M 47.1¬±.1 0.39 0.45\\nPrefLayer 0.77M 46.7 0.38 0.45\\nLoRA 0.77M 47.5¬±.1 0.39 0.45\\nTable 13: GPT-2 with different adaptation methods on DART. The variances of MET and TER are\\nless than 0.01for all adaption approaches.\\n21'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 21}, page_content='Method WebNLG\\nBLEU‚Üë MET‚Üë TER‚Üì\\nU S A U S A U S A\\nGPT-2 Medium\\nFine-Tune (354M) 27.7 64.2 46.5 .30 .45 .38 .76 .33 .53\\nAdapterL(0.37M) 45.1 54.5 50.2 .36 .39 .38 .46 .40 .43\\nAdapterL(11M) 48.3 60.4 54.9 .38 .43 .41 .45 .35 .39\\nFTTop2(24M) 18.9 53.6 36.0 .23 .38 .31 .99 .49 .72\\nPreÔ¨Åx (0.35M) 45.6 62.9 55.1 .38 .44 .41 .49 .35 .40\\nLoRA (0.35M) 46.7¬±.462.1¬±.255.3¬±.2.38 .44 .41 .46 .33 .39\\nGPT-2 Large\\nFine-Tune (774M) 43.1 65.3 55.5 .38 .46 .42 .53 .33 .42\\nAdapterL(0.88M) 49.8¬±.061.1¬±.056.0¬±.0.38 .43 .41 .44 .35 .39\\nAdapterL(23M) 49.2¬±.164.7¬±.257.7¬±.1.39 .46 .43 .46 .33 .39\\nPreÔ¨Åx (0.77M) 47.7 63.4 56.3 .39 .45 .42 .48 .34 .40\\nLoRA (0.77M) 48.4¬±.364.0¬±.357.0¬±.1.39 .45 .42 .45 .32 .38\\nTable 14: GPT-2 with different adaptation methods on WebNLG. The variances of MET and TER\\nare less than 0.01for all the experiments we ran. ‚ÄúU‚Äù indicates unseen categories, ‚ÄúS‚Äù indicates seen\\ncategories, and ‚ÄúA‚Äù indicates all categories in the test set of WebNLG.\\nF.2 A DDITIONAL EXPERIMENTS ON GPT-3\\nWe present additional runs on GPT-3 with different adaptation methods in Table 15. The focus is on\\nidentifying the trade-off between performance and the number of trainable parameters.\\nF.3 L OW-DATA REGIME\\nTo evaluate the performance of different adaptation approaches in the low-data regime. we randomly\\nsample 100, 1k and 10k training examples from the full training set of MNLI to form the low-data\\nMNLI-ntasks. In Table 16, we show the performance of different adaptation approaches on MNLI-\\nn. To our surprise, PreÔ¨ÅxEmbed and PreÔ¨ÅxLayer performs very poorly on MNLI-100 dataset, with\\nPreÔ¨ÅxEmbed performing only slightly better than random chance (37.6% vs. 33.3%). PreÔ¨ÅxLayer\\nperforms better than PreÔ¨ÅxEmbed but is still signiÔ¨Åcantly worse than Fine-Tune or LoRA on MNLI-\\n100. The gap between preÔ¨Åx-based approaches and LoRA/Fine-tuning becomes smaller as we in-\\ncrease the number of training examples, which might suggest that preÔ¨Åx-based approaches are not\\nsuitable for low-data tasks in GPT-3. LoRA achieves better performance than Ô¨Åne-tuning on both\\nMNLI-100 and MNLI-Full, and comparable results on MNLI-1k and MNLI-10K considering the\\n(¬±0.3) variance due to random seeds.\\nThe training hyperparameters of different adaptation approaches on MNLI-n are reported in Ta-\\nble 17. We use a smaller learning rate for PreÔ¨ÅxLayer on the MNLI-100 set, as the training loss does\\nnot decrease with a larger learning rate.\\nG M EASURING SIMILARITY BETWEEN SUBSPACES\\nIn this paper we use the measure œÜ(A,B,i,j ) =œà(Ui\\nA,Uj\\nB) =‚à•Ui‚ä§\\nAUB‚à•2\\nF\\nmin{i,j}to measure the subspace\\nsimilarity between two column orthonormal matrices Ui\\nA‚ààRd√óiandUj\\nB‚ààRd√ój, obtained by\\ntaking columns of the left singular matrices of AandB. We point out that this similarity is simply\\na reverse of the standard Projection Metric that measures distance between subspaces Ham & Lee\\n(2008).\\n22'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 22}, page_content='Method Hyperparameters # Trainable Parameters WikiSQL MNLI-m\\nFine-Tune - 175B 73.8 89.5\\nPreÔ¨ÅxEmbedlp= 32,li= 8 0.4 M 55.9 84.9\\nlp= 64,li= 8 0.9 M 58.7 88.1\\nlp= 128,li= 8 1.7 M 60.6 88.0\\nlp= 256,li= 8 3.2 M 63.1 88.6\\nlp= 512,li= 8 6.4 M 55.9 85.8\\nPreÔ¨ÅxLayerlp= 2,li= 2 5.1 M 68.5 89.2\\nlp= 8,li= 0 10.1 M 69.8 88.2\\nlp= 8,li= 8 20.2 M 70.1 89.5\\nlp= 32,li= 4 44.1 M 66.4 89.6\\nlp= 64,li= 0 76.1 M 64.9 87.9\\nAdapterHr= 1 7.1 M 71.9 89.8\\nr= 4 21.2 M 73.2 91.0\\nr= 8 40.1 M 73.2 91.5\\nr= 16 77.9 M 73.2 91.5\\nr= 64 304.4 M 72.6 91.5\\nLoRArv= 2 4.7 M 73.4 91.7\\nrq=rv= 1 4.7 M 73.4 91.3\\nrq=rv= 2 9.4 M 73.3 91.4\\nrq=rk=rv=ro= 1 9.4 M 74.1 91.2\\nrq=rv= 4 18.8 M 73.7 91.3\\nrq=rk=rv=ro= 2 18.8 M 73.7 91.7\\nrq=rv= 8 37.7 M 73.8 91.6\\nrq=rk=rv=ro= 4 37.7 M 74.0 91.7\\nrq=rv= 64 301.9 M 73.6 91.4\\nrq=rk=rv=ro= 64 603.8 M 73.9 91.4\\nLoRA+PErq=rv= 8,lp= 8,li= 4 37.8 M 75.0 91.4\\nrq=rv= 32,lp= 8,li= 4 151.1 M 75.9 91.1\\nrq=rv= 64,lp= 8,li= 4 302.1 M 76.2 91.3\\nLoRA+PL rq=rv= 8,lp= 8,li= 4 52.8 M 72.9 90.2\\nTable 15: Hyperparameter analysis of different adaptation approaches on WikiSQL and MNLI. Both\\npreÔ¨Åx-embedding tuning (PreÔ¨ÅxEmbed) and preÔ¨Åx-layer tuning (PreÔ¨ÅxLayer) perform worse as we\\nincrease the number of trainable parameters, while LoRA‚Äôs performance stabilizes. Performance is\\nmeasured in validation accuracy.\\nMethod MNLI(m)-100 MNLI(m)-1k MNLI(m)-10k MNLI(m)-392K\\nGPT-3 (Fine-Tune) 60.2 85.8 88.9 89.5\\nGPT-3 (PreÔ¨ÅxEmbed) 37.6 75.2 79.5 88.6\\nGPT-3 (PreÔ¨ÅxLayer) 48.3 82.5 85.9 89.6\\nGPT-3 (LoRA) 63.8 85.6 89.2 91.7\\nTable 16: Validation accuracy of different methods on subsets of MNLI using GPT-3 175B. MNLI-\\nndescribes a subset with ntraining examples. We evaluate with the full validation set. LoRA\\nperforms exhibits favorable sample-efÔ¨Åciency compared to other methods, including Ô¨Åne-tuning.\\nTo be concrete, let the singular values of Ui‚ä§\\nAUj\\nBto beœÉ1,œÉ2,¬∑¬∑¬∑,œÉpwherep= min{i,j}. We\\nknow that the Projection Metric Ham & Lee (2008) is deÔ¨Åned as:\\nd(Ui\\nA,Uj\\nB) =\\ued6a\\ued6b\\ued6b‚àöp‚àíp‚àë\\ni=1œÉ2\\ni‚àà[0,‚àöp]\\n23'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 23}, page_content='Hyperparameters Adaptation MNLI-100 MNLI-1k MNLI-10K MNLI-392K\\nOptimizer - AdamW\\nWarmup Tokens - 250,000\\nLR Schedule - Linear\\nBatch Size - 20 20 100 128\\n# Epoch - 40 40 4 2\\nLearning RateFineTune 5.00E-6\\nPreÔ¨ÅxEmbed 2.00E-04 2.00E-04 4.00E-04 5.00E-04\\nPreÔ¨ÅxLayer 5.00E-05 5.00E-05 5.00E-05 1.00E-04\\nLoRA 2.00E-4\\nPreÔ¨ÅxEmbed lp 16 32 64 256\\nAdaptation- PreÔ¨ÅxEmbed li 8\\nSpeciÔ¨Åc PreÔ¨ÅxTune lp=li= 8\\nLoRA rq=rv= 8\\nTable 17: The hyperparameters used for different GPT-3 adaptation methods on MNLI(m)- n.\\nwhere our similarity is deÔ¨Åned as:\\nœÜ(A,B,i,j ) =œà(Ui\\nA,Uj\\nB) =‚àëp\\ni=1œÉ2\\ni\\np=1\\np(\\n1‚àíd(Ui\\nA,Uj\\nB)2)\\nThis similarity satisÔ¨Åes that if Ui\\nAandUj\\nBshare the same column span, then œÜ(A,B,i,j ) = 1 . If\\nthey are completely orthogonal, then œÜ(A,B,i,j ) = 0 . Otherwise, œÜ(A,B,i,j )‚àà(0,1).\\nH A DDITIONAL EXPERIMENTS ON LOW-RANK MATRICES\\nWe present additional results from our investigation into the low-rank update matrices.\\nH.1 C ORRELATION BETWEEN LORA M ODULES\\nSee Figure 6 and Figure 7 for how the results presented in Figure 3 and Figure 4 generalize to other\\nlayers.\\nH.2 E FFECT OFrONGPT-2\\nWe repeat our experiment on the effect of r(Section 7.2) in GPT-2. Using the E2E NLG Challenge\\ndataset as an example, we report the validation loss and test metrics achieved by different choices\\nofrafter training for 26,000 steps. We present our result in Table 18. The optimal rank for GPT-2\\nMedium is between 4 and 16 depending on the metric used, which is similar to that for GPT-3 175B.\\nNote that the relationship between model size and the optimal rank for adaptation is still an open\\nquestion.\\nH.3 C ORRELATION BETWEEN WAND ‚àÜW\\nSee Figure 8 for the normalized subspace similarity between Wand‚àÜWwith varying r.\\nNote again that ‚àÜWdoes not contain the top singular directions of W, since the similarity between\\nthe top 4 directions in ‚àÜWand the top-10% of those in Wbarely exceeds 0.2. This gives evidence\\nthat‚àÜWcontains those ‚Äútask-speciÔ¨Åc‚Äù directions that are otherwise notemphasized in W.\\nAn interesting next question to answer, is how ‚Äústrong‚Äù do we need to amplify those task-speciÔ¨Åc\\ndirections, in order for the model adaptation to work well?\\n24'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 24}, page_content='0.00.20.40.60.81.0\\n12345678Layer 1\\niWq\\n Wv\\n Wq\\n Wv\\n12345678Layer 32\\ni\\n12345678Layer 64\\ni\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\nj12345678Layer 96\\ni\\n1\\n6\\n12\\n18\\n23\\n29\\n35\\n40\\n46\\n52\\n58\\nj12345678\\nj12345678\\nj(Ar=8,Ar=64,i,j)\\nFigure 6: Normalized subspace similarity between the column vectors of Ar=8andAr=64for both\\n‚àÜWqand‚àÜWvfrom the 1st, 32nd, 64th, and 96th layers in a 96-layer Transformer.\\nH.4 A MPLIFICATION FACTOR\\nOne can naturally consider a feature ampliÔ¨Åcation factor as the ratio‚à•‚àÜW‚à•F\\n‚à•U‚ä§WV‚ä§‚à•F, whereUandV\\nare the left- and right-singular matrices of the SVD decomposition of ‚àÜW. (RecallUU‚ä§WV‚ä§V\\ngives the ‚Äúprojection‚Äù of Wonto the subspace spanned by ‚àÜW.)\\nIntuitively, when ‚àÜWmostly contains task-speciÔ¨Åc directions, this quantity measures how much of\\nthem are ampliÔ¨Åed by ‚àÜW. As shown in Section 7.3, for r= 4, this ampliÔ¨Åcation factor is as large\\nas 20. In other words, there are (generally speaking) four feature directions in each layer (out of the\\nentire feature space from the pre-trained model W), that need to be ampliÔ¨Åed by a very large factor\\n20, in order to achieve our reported accuracy for the downstream speciÔ¨Åc task. And, one should\\nexpect a very different set of feature directions to be ampliÔ¨Åed for each different downstream task.\\nOne may notice, however, for r= 64 , this ampliÔ¨Åcation factor is only around 2, meaning that\\nmost directions learned in ‚àÜWwithr= 64 arenotbeing ampliÔ¨Åed by much. This should not\\nbe surprising, and in fact gives evidence (once again) that the intrinsic rank needed to represent\\nthe ‚Äútask-speciÔ¨Åc directions‚Äù (thus for model adaptation) is low. In contrast, those directions in the\\nrank-4 version of ‚àÜW(corresponding to r= 4) are ampliÔ¨Åed by a much larger factor 20.\\n25'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Tutorial_paper.pdf', 'page': 25}, page_content='0.00.10.20.30.40.50.60.70.8\\n1\\n7\\n13\\n19\\n25\\n31\\n37\\n43\\n49\\n55\\n61Layer 1\\niWq\\n Wv\\nLayer 32Wq\\n Wv\\n1\\n6\\n11\\n16\\n21\\n26\\n31\\n36\\n41\\n46\\n51\\n56\\n61\\nj1\\n7\\n13\\n19\\n25\\n31\\n37\\n43\\n49\\n55\\n61Layer 64\\ni\\n1\\n6\\n11\\n16\\n21\\n26\\n31\\n36\\n41\\n46\\n51\\n56\\n61\\nj\\n1\\n6\\n11\\n16\\n21\\n26\\n31\\n36\\n41\\n46\\n51\\n56\\n61\\njLayer 96\\n1\\n6\\n11\\n16\\n21\\n26\\n31\\n36\\n41\\n46\\n51\\n56\\n61\\nj(Ar=64,A‚Ä≤r=64,i,j)\\nFigure 7: Normalized subspace similarity between the column vectors of Ar=64from two randomly\\nseeded runs, for both ‚àÜWqand‚àÜWvfrom the 1st, 32nd, 64th, and 96th layers in a 96-layer Trans-\\nformer.\\nRankrvalloss BLEU NIST METEOR ROUGE L CIDEr\\n1 1.23 68.72 8.7215 0.4565 0.7052 2.4329\\n2 1.21 69.17 8.7413 0.4590 0.7052 2.4639\\n4 1.18 70.38 8.8439 0.4689 0.7186 2.5349\\n8 1.17 69.57 8.7457 0.4636 0.7196 2.5196\\n16 1.16 69.61 8.7483 0.4629 0.7177 2.4985\\n32 1.16 69.33 8.7736 0.4642 0.7105 2.5255\\n64 1.16 69.24 8.7174 0.4651 0.7180 2.5070\\n128 1.16 68.73 8.6718 0.4628 0.7127 2.5030\\n256 1.16 68.92 8.6982 0.4629 0.7128 2.5012\\n512 1.16 68.78 8.6857 0.4637 0.7128 2.5025\\n1024 1.17 69.37 8.7495 0.4659 0.7149 2.5090\\nTable 18: Validation loss and test set metrics on E2E NLG Challenge achieved by LoRA with\\ndifferent rank rusing GPT-2 Medium. Unlike on GPT-3 where r= 1sufÔ¨Åces for many tasks, here\\nthe performance peaks at r= 16 for validation loss and r= 4 for BLEU, suggesting the GPT-2\\nMedium has a similar intrinsic rank for adaptation compared to GPT-3 175B. Note that some of our\\nhyperparameters are tuned on r= 4, which matches the parameter count of another baseline, and\\nthus might not be optimal for other choices of r.\\n0.1000.1250.1500.1750.200\\nj451\\n555\\n658\\n762\\n865\\n969\\n1072\\n1176i(Wq,Ar=4,i,j)\\njWq\\n(Wq,Ar=8,i,j)\\nj(Wq,Ar=64,i,j)\\njRandom\\n(Wq,Arand,i,j)\\nFigure 8: Normalized subspace similarity between the singular directions of Wqand those of ‚àÜWq\\nwith varying rand a random baseline. ‚àÜWqampliÔ¨Åes directions that are important but not empha-\\nsized inW.‚àÜWwith a larger rtends to pick up more directions that are already emphasized in\\nW.\\n26'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 0}, page_content='Large Language Models: A Survey\\nShervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu\\nRichard Socher, Xavier Amatriain, Jianfeng Gao\\nAbstract ‚ÄîLarge Language Models (LLMs) have drawn a\\nlot of attention due to their strong performance on a wide\\nrange of natural language tasks, since the release of ChatGPT\\nin November 2022. LLMs‚Äô ability of general-purpose language\\nunderstanding and generation is acquired by training billions of\\nmodel‚Äôs parameters on massive amounts of text data, as predicted\\nby scaling laws [1], [2]. The research area of LLMs, while very\\nrecent, is evolving rapidly in many different ways. In this paper,\\nwe review some of the most prominent LLMs, including three\\npopular LLM families (GPT, LLaMA, PaLM), and discuss their\\ncharacteristics, contributions and limitations. We also give an\\noverview of techniques developed to build, and augment LLMs.\\nWe then survey popular datasets prepared for LLM training,\\nfine-tuning, and evaluation, review widely used LLM evaluation\\nmetrics, and compare the performance of several popular LLMs\\non a set of representative benchmarks. Finally, we conclude\\nthe paper by discussing open challenges and future research\\ndirections.\\nI. I NTRODUCTION\\nLanguage modeling is a long-standing research topic, dat-\\ning back to the 1950s with Shannon‚Äôs application of informa-\\ntion theory to human language, where he measured how well\\nsimple n-gram language models predict or compress natural\\nlanguage text [3]. Since then, statistical language modeling\\nbecame fundamental to many natural language understanding\\nand generation tasks, ranging from speech recognition, ma-\\nchine translation, to information retrieval [4], [5], [6].\\nThe recent advances on transformer-based large language\\nmodels (LLMs), pretrained on Web-scale text corpora, signif-\\nicantly extended the capabilities of language models (LLMs).\\nFor example, OpenAI‚Äôs ChatGPT and GPT-4 can be used not\\nonly for natural language processing, but also as general task\\nsolvers to power Microsoft‚Äôs Co-Pilot systems, for instance,\\ncan follow human instructions of complex new tasks per-\\nforming multi-step reasoning when needed. LLMs are thus\\nbecoming the basic building block for the development of\\ngeneral-purpose AI agents or artificial general intelligence\\n(AGI).\\nAs the field of LLMs is moving fast, with new findings,\\nmodels and techniques being published in a matter of months\\nor weeks [7], [8], [9], [10], [11], AI researchers and practi-\\ntioners often find it challenging to figure out the best recipes\\nto build LLM-powered AI systems for their tasks. This paper\\ngives a timely survey of the recent advances on LLMs. We\\nhope this survey will prove a valuable and accessible resource\\nfor students, researchers and developers.\\nLLMs are large-scale, pre-trained, statistical language mod-\\nels based on neural networks. The recent success of LLMs is\\nan accumulation of decades of research and development of\\nlanguage models, which can be categorized into four wavesthat have different starting points and velocity: statistical lan-\\nguage models, neural language models, pre-trained language\\nmodels and LLMs.\\nStatistical language models (SLMs) view text as a sequence\\nof words, and estimate the probability of text as the product\\nof their word probabilities. The dominating form of SLMs\\nare Markov chain models known as the n-gram models,\\nwhich compute the probability of a word conditioned on its\\nimmediate proceeding n‚àí1words. Since word probabilities\\nare estimated using word and n-gram counts collected from\\ntext corpora, the model needs to deal with data sparsity (i.e.,\\nassigning zero probabilities to unseen words or n-grams) by\\nusing smoothing , where some probability mass of the model\\nis reserved for unseen n-grams [12]. N-gram models are\\nwidely used in many NLP systems. However, these models\\nare incomplete in that they cannot fully capture the diversity\\nand variability of natural language due to data sparsity.\\nEarly neural language models (NLMs) [13], [14], [15], [16]\\ndeal with data sparsity by mapping words to low-dimensional\\ncontinuous vectors (embedding vectors) and predict the next\\nword based on the aggregation of the embedding vectors of\\nits proceeding words using neural networks. The embedding\\nvectors learned by NLMs define a hidden space where the\\nsemantic similarity between vectors can be readily computed\\nas their distance. This opens the door to computing semantic\\nsimilarity of any two inputs regardless their forms (e.g., queries\\nvs. documents in Web search [17], [18], sentences in different\\nlanguages in machine translation [19], [20]) or modalities (e.g.,\\nimage and text in image captioning [21], [22]). Early NLMs are\\ntask-specific models, in that they are trained on task-specific\\ndata and their learned hidden space is task-specific.\\nPre-trained language models (PLMs), unlike early NLMs,\\nare task-agnostic. This generality also extends to the learned\\nhidden embedding space. The training and inference of PLMs\\nfollows the pre-training and fine-tuning paradigm, where lan-\\nguage models with recurrent neural networks [23] or trans-\\nformers [24], [25], [26] are pre-trained on Web-scale unlabeled\\ntext corpora for general tasks such as word prediction, and then\\nfinetuned to specific tasks using small amounts of (labeled)\\ntask-specific data. Recent surveys on PLMs include [8], [27],\\n[28].\\nLarge language models (LLMs) mainly refer to\\ntransformer-based neural language models1that contain\\ntens to hundreds of billions of parameters, which are pre-\\ntrained on massive text data, such as PaLM [31], LLaMA\\n[32], and GPT-4 [33], as summarized in Table III. Compared\\n1Recently, several very promising non-transformer LLMs have been pro-\\nposed, such as the LLMs based on structured state space models [29], [30].\\nSee Section VII for more details.arXiv:2402.06196v2  [cs.CL]  20 Feb 2024'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 1}, page_content='Emerging\\n Basic\\n Augmented\\nLLM Capabilities\\nReasoning\\nCoding\\nComprehension\\nMultilingual\\nTool\\nutilization\\nWorld\\nknowledge\\nInstruction\\nfollowing\\nIn-context\\nlearning\\nInteracting\\nwith users\\nSelf-improvement\\nMulti choice QA\\n Wikipedia QA\\nXNLI\\nCrosslingual QA\\nCrosslingual T asks\\nTranslation\\nReading Comprehension\\nMulti choice QA\\nBoolean QA\\nSimplification\\nSummarization\\nFunction Calling\\nAPI calling\\nLogical\\nSymbolic\\nCommon Sense\\nArithmetic\\nTurn based\\nCompletion\\nTask definition\\nFew-shot\\nSymbolic\\nreference\\nPos/Neg example\\nStep by step\\nsolving\\nTool planning\\nTask\\ndecomposition\\nVirtual acting\\nPhysical acting\\nKnowledge base\\nutilization\\nAssignment\\nplanning\\nSelf-cirtisim\\nSelf-refinementFig. 1: LLM Capabilities.\\nto PLMs, LLMs are not only much larger in model size, but\\nalso exhibit stronger language understanding and generation\\nabilities, and more importantly, emergent abilities that are\\nnot present in smaller-scale language models. As illustrated\\nin Fig. 1, these emergent abilities include (1) in-context\\nlearning, where LLMs learn a new task from a small set\\nof examples presented in the prompt at inference time, (2)\\ninstruction following, where LLMs, after instruction tuning,\\ncan follow the instructions for new types of tasks without\\nusing explicit examples, and (3) multi-step reasoning, where\\nLLMs can solve a complex task by breaking down that task\\ninto intermediate reasoning steps as demonstrated in the\\nchain-of-thought prompt [34]. LLMs can also be augmented\\nby using external knowledge and tools [35], [36] so that\\nthey can effectively interact with users and environment [37],\\nand continually improve itself using feedback data collected\\nthrough interactions (e.g. via reinforcement learning with\\nhuman feedback (RLHF)).\\nThrough advanced usage and augmentation techniques,\\nLLMs can be deployed as so-called AI agents: artificial entities\\nthat sense their environment, make decisions, and take actions.\\nPrevious research has focused on developing agents for specific\\ntasks and domains. The emergent abilities demonstrated by\\nLLMs make it possible to build general-purpose AI agents\\nbased on LLMs. While LLMs are trained to produce responses\\nin static settings, AI agents need to take actions to interact with\\ndynamic environment. Therefore, LLM-based agents often\\nneed to augment LLMs to e.g., obtain updated information\\nfrom external knowledge bases, verify whether a system action\\nproduces the expected result, and cope with when things do\\nnot go as expected, etc. We will discuss in detail LLM-based\\nagents in Section IV.\\nIn the rest of this paper, Section II presents an overview of\\nstate of the art of LLMs, focusing on three LLM families (GPT,\\nLLaMA and PaLM) and other representative models. Section\\nIII discusses how LLMs are built. Section IV discusses howLLMs are used, and augmented for real-world applications\\nSections V and VI review popular datasets and benchmarks for\\nevaluating LLMs, and summarize the reported LLM evaluation\\nresults. Finally, Section VII concludes the paper by summa-\\nrizing the challenges and future research directions.\\nII. L ARGE LANGUAGE MODELS\\nIn this section we start with a review of early pre-trained\\nneural language models as they are the base of LLMs, and\\nthen focus our discussion on three families of LLMs: GPT,\\nLlaMA, and PaLM. Table I provides an overview of some of\\nthese models and their characteristics.\\nA. Early Pre-trained Neural Language Models\\nLanguage modeling using neural networks was pioneered\\nby [38], [39], [40]. Bengio et al. [13] developed one of the first\\nneural language models (NLMs) that are comparable to n-gram\\nmodels. Then, [14] successfully applied NLMs to machine\\ntranslation. The release of RNNLM (an open source NLM\\ntoolkit) by Mikolov [41], [42] helped significantly popularize\\nNLMs. Afterwards, NLMs based on recurrent neural networks\\n(RNNs) and their variants, such as long short-term memory\\n(LSTM) [19] and gated recurrent unit (GRU) [20], were widely\\nused for many natural language applications including machine\\ntranslation, text generation and text classification [43].\\nThen, the invention of the Transformer architecture [44]\\nmarks another milestone in the development of NLMs. By\\napplying self-attention to compute in parallel for every word\\nin a sentence or document an ‚Äúattention score‚Äù to model the\\ninfluence each word has on another, Transformers allow for\\nmuch more parallelization than RNNs, which makes it possible\\nto efficiently pre-train very big language models on large\\namounts of data on GPUs. These pre-trained language models\\n(PLMs) can be fine-tuned for many downstream tasks.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 2}, page_content='Paper Strcuture\\nEarly Pre-trained\\nLanguage ModelsIILarge Language Models\\nAIII HOW LLMS ARE BUIL T\\nA\\nData Cleaning BLarge Language\\nModel FamiliesB\\nOther Representative\\nLLMsCDominant LLM\\nArchitectures\\nTokenizations C\\nPositional Encoding DModel Pre-training E\\nFine-tuning and\\nInstruction T uningF\\nAlignment G\\nDecoding StrategiesH\\nI HOW LLMS ARE USED AND AUGMENTED\\nA\\nBLLM limitationsCost-Effective T raining/Inference,\\nAdaptation & CompressionI\\nUsing LLMs: Prompt Design\\nand Engineering\\nCAugmenting LLMs through\\nexternal knowledge - RAG\\nD Using External T ools\\nE LLM AgentsV  POPULAR DA TASETS FOR LLMS\\nADatasets for Basic T asks: language\\nmodeling/understanding/generation\\nB Datasets for Emergent: ICL, reasoning,\\ninstruction following\\nCDatasets for Augmented: using\\nexternal knowledge/tools\\nVI PROMINENT LLMS‚Äô  PERFORMANCE\\nON BENCHMARKS\\nA\\nBVII CHALLENGES AND FUTURE DIRECTIONS\\nASmaller and more efficient\\nLanguage ModelsLLMs‚Äô  Performance on Different T asksPopular Metrics for Evaluating LLMs\\nBNew Post-attention\\nArchitectural Paradigms\\nC Multi-modal Models\\nDImproved LLM Usage and\\nAugmentation techniques\\nDSecurity and\\nEthical/Responsible AIFig. 2: The paper structure.\\nWe group early popular Transformer-based PLMs, based on\\ntheir neural architectures, into three main categories: encoder-\\nonly, decoder-only, and encoder-decoder models. Comprehen-\\nsive surveys of early PLMs are provided in [43], [28].\\n1) Encoder-only PLMs: As the name suggests, the encoder-\\nonly models only consist of an encoder network. These models\\nare originally developed for language understanding tasks,\\nsuch as text classification, where the models need to predict a\\nclass label for an input text. Representative encoder-only mod-\\nels include BERT and its variants, e.g., RoBERTa, ALBERT,\\nDeBERTa, XLM, XLNet, UNILM, as to be described below.BERT (Birectional Encoder Representations from Trans-\\nformers) [24] is one of the most widely used encoder-only\\nlanguage models. BERT consists of three modules: (1) an\\nembedding module that converts input text into a sequence\\nof embedding vectors, (2) a stack of Transformer encoders\\nthat converts embedding vectors into contextual representation\\nvectors, and (3) a fully connected layer that converts the\\nrepresentation vectors (at the final layer) to one-hot vectors.\\nBERT is pre-trained uses two objectives: masked language\\nmodeling (MLM) and next sentence prediction. The pre-trained\\nBERT model can be fine-tuned by adding a classifier layer\\nfor many language understanding tasks, ranging from text'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 3}, page_content='TABLE I: High-level Overview of Popular Language Models\\nType Model Name #Parameters Release Base Models Open\\nSource#Tokens Training dataset\\nBERT 110M, 340M 2018 - ‚úì 137B BooksCorpus, English Wikipedia\\nRoBERTa 355M 2019 - ‚úì 2.2T BooksCorpus, English Wikipedia, CC-NEWS,\\nSTORIES (a subset of Common Crawl), Reddit\\nEncoder-OnlyALBERT 12M, 18M, 60M,\\n235M2019 - ‚úì 137B BooksCorpus, English Wikipedia\\nDeBERTa - 2020 - ‚úì - BooksCorpus, English Wikipedia, STORIES, Red-\\ndit content\\nXLNet 110M, 340M 2019 - ‚úì 32.89B BooksCorpus, English Wikipedia, Giga5, Com-\\nmon Crawl, ClueWeb 2012-B\\nDecoder-onlyGPT-1 120M 2018 - ‚úì 1.3B BooksCorpus\\nGPT-2 1.5B 2019 - ‚úì 10B Reddit outbound\\nT5 (Base) 223M 2019 - ‚úì 156B Common Crawl\\nEncoder-DecoderMT5 (Base) 300M 2020 - ‚úì - New Common Crawl-based dataset in 101 lan-\\nguages (m Common Crawl)\\nBART (Base) 139M 2019 - ‚úì - Corrupting text\\nGPT-3 125M, 350M,\\n760M, 1.3B, 2.7B,\\n6.7B, 13B, 175B2020 √ó 300B Common Crawl (filtered), WebText2, Books1,\\nBooks2, Wikipedia\\nGPT FamilyCODEX 12B 2021 GPT ‚úì - Public GitHub software repositories\\nWebGPT 760M, 13B, 175B 2021 GPT-3 √ó - ELI5\\nGPT-4 1.76T 2023 - √ó 13T -\\nLLaMA1 7B, 13B, 33B, 65B 2023 - ‚úì 1T, 1.4T Online sources\\nLLaMA2 7B, 13B, 34B, 70B 2023 - ‚úì 2T Online sources\\nAlpaca 7B 2023 LLaMA1 ‚úì - GPT-3.5\\nVicuna-13B 13B 2023 LLaMA1 ‚úì - GPT-3.5\\nLLaMA FamilyKoala 13B 2023 LLaMA ‚úì - Dialogue data\\nMistral-7B 7.3B 2023 ‚úì - -\\nCode Llama 34 2023 LLaMA2 ‚úì 500B Publicly available code\\nLongLLaMA 3B, 7B 2023 OpenLLaMA ‚úì 1T -\\nLLaMA-Pro-8B 8.3B 2024 LLaMA2-7B ‚úì 80B Code and math corpora\\nTinyLlama-1.1B 1.1B 2024 LLaMA1.1B ‚úì 3T SlimPajama, Starcoderdata\\nPaLM 8B, 62B, 540B 2022 - √ó 780B Web documents, books, Wikipedia, conversations,\\nGitHub code\\nU-PaLM 8B, 62B, 540B 2022 - √ó 1.3B Web documents, books, Wikipedia, conversations,\\nGitHub code\\nPaLM FamilyPaLM-2 340B 2023 - ‚úì 3.6T Web documents, books, code, mathematics, con-\\nversational data\\nMed-PaLM 540B 2022 PaLM √ó 780B HealthSearchQA, MedicationQA, LiveQA\\nMed-PaLM 2 - 2023 PaLM 2 √ó - MedQA, MedMCQA, HealthSearchQA, LiveQA,\\nMedicationQA\\nFLAN 137B 2021 LaMDA-PT ‚úì - Web documents, code, dialog data, Wikipedia\\nGopher 280B 2021 - √ó 300B MassiveText\\nERNIE 4.0 10B 2023 - √ó 4TB Chinese text\\nRetro 7.5B 2021 - √ó 600B MassiveText\\nLaMDA 137B 2022 - √ó 168B public dialog data and web documents\\nChinChilla 70B 2022 - √ó 1.4T MassiveText\\nGalactia-120B 120B 2022 - 450B\\nOther Popular LLMsCodeGen 16.1B 2022 - ‚úì - THE PILE, BIGQUERY , BIGPYTHON\\nBLOOM 176B 2022 - ‚úì 366B ROOTS\\nZephyr 7.24B 2023 Mistral-7B ‚úì 800B Synthetic data\\nGrok-0 33B 2023 - √ó - Online source\\nORCA-2 13B 2023 LLaMA2 - 2001B -\\nStartCoder 15.5B 2023 - ‚úì 35B GitHub\\nMPT 7B 2023 - ‚úì 1T RedPajama, m Common Crawl, S2ORC, Common\\nCrawl\\nMixtral-8x7B 46.7B 2023 - ‚úì - Instruction dataset\\nFalcon 180B 180B 2023 - ‚úì 3.5T RefinedWeb\\nGemini 1.8B, 3.25B 2023 ‚úì - Web documents, books, and code, image data,\\naudio data, video data\\nDeepSeek-Coder 1.3B, 6.7B, 33B 2024 - ‚úì 2T GitHub‚Äôs Markdown and StackExchange\\nDocLLM 1B,7B 2024 - √ó 2T IIT-CDIP Test Collection 1.0, DocBank\\nclassification, question answering to language inference. A\\nhigh-level overview of BERT framework is shown in Fig 3. As\\nBERT significantly improved state of the art on a wide range\\nof language understanding tasks when it was published, the AI\\ncommunity was inspired to develop many similar encoder-only\\nlanguage models based on BERT.\\nRoBERTa [25] significantly improves the robustness of\\nBERT using a set of model design choices and training strate-\\ngies, such as modifying a few key hyperparameters, removing\\nthe next-sentence pre-training objective and training with muchlarger mini-batches and learning rates. ALBERT [45] uses two\\nparameter-reduction techniques to lower memory consumption\\nand increase the training speed of BERT: (1) splitting the\\nembedding matrix into two smaller matrices, and (2) using\\nrepeating layers split among groups. DeBERTa (Decoding-\\nenhanced BERT with disentangled attention) [26] improves the\\nBERT and RoBERTa models using two novel techniques. The\\nfirst is the disentangled attention mechanism, where each word\\nis represented using two vectors that encode its content and\\nposition, respectively, and the attention weights among words'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 4}, page_content='Fig. 3: Overall pre-training and fine-tuning procedures for\\nBERT. Courtesy of [24]\\nare computed using disentangled matrices on their contents and\\nrelative positions, respectively. Second, an enhanced mask de-\\ncoder is used to incorporate absolute positions in the decoding\\nlayer to predict the masked tokens in model pre-training. In\\naddition, a novel virtual adversarial training method is used for\\nfine-tuning to improve models‚Äô generalization. ELECTRA [46]\\nuses a new pre-training task, known as replaced token detection\\n(RTD), which is empirically proven to be more sample-efficient\\nthan MLM. Instead of masking the input, RTD corrupts it by\\nreplacing some tokens with plausible alternatives sampled from\\na small generator network. Then, instead of training a model\\nthat predicts the original identities of the corrupted tokens, a\\ndiscriminative model is trained to predict whether a token in\\nthe corrupted input was replaced by a generated sample or not.\\nRTD is more sample-efficient than MLM because the former\\nis defined over all input tokens rather than just the small subset\\nbeing masked out, as illustrated in Fig 4.\\nFig. 4: A comparison between replaced token detection and\\nmasked language modeling. Courtesy of [46].\\nXLMs [47] extended BERT to cross-lingual language\\nmodels using two methods: (1) a unsupervised method that\\nonly relies on monolingual data, and (2) a supervised method\\nthat leverages parallel data with a new cross-lingual language\\nmodel objective, as illustrated in Fig 5. XLMs had obtained\\nstate-of-the-art results on cross-lingual classification, unsuper-\\nvised and supervised machine translation, at the time they were\\nproposed.\\nThere are also encoder-only language models that leverage\\nthe advantages of auto-regressive (decoder) models for model\\ntraining and inference. Two examples are XLNet and UNILM.\\nXLNet [48] is based on Transformer-XL, pre-trained using a\\ngeneralized autoregressive method that enables learning bidi-\\nrectional contexts by maximizing the expected likelihood over\\nFig. 5: Cross-lingual language model pretraining. The MLM\\nobjective is similar to BERT, but with continuous streams\\nof text as opposed to sentence pairs. The TLM objective\\nextends MLM to pairs of parallel sentences. To predict a\\nmasked English word, the model can attend to both the English\\nsentence and its French translation, and is encouraged to align\\nEnglish and French representations. Courtesy of [47].\\nall permutations of the factorization order. UNILM (UNIfied\\npre-trained Language Model) [49] is pre-trained using three\\ntypes of language modeling tasks: unidirectional, bidirectional,\\nand sequence-to-sequence prediction. This is achieved by\\nemploying a shared Transformer network and utilizing specific\\nself-attention masks to control what context the prediction is\\nconditioned on, as illustrated in Fig 6. The pre-trained model\\ncan be fine-tuned for both natural language understanding and\\ngeneration tasks.\\nFig. 6: Overview of unified LM pre-training. The model\\nparameters are shared across the LM objectives (i.e., bidirec-\\ntional LM, unidirectional LM, and sequence-to-sequence LM).\\nCourtesy of [49].\\n2) Decoder-only PLMs: Two of the most widely used\\ndecoder-only PLMs are GPT-1 and GPT-2, developed by\\nOpenAI. These models lay the foundation to more powerful\\nLLMs subsequently, i.e., GPT-3 and GPT-4.\\nGPT-1 [50] demonstrates for the first time that good\\nperformance over a wide range of natural language tasks can be\\nobtained by Generative Pre-Training (GPT) of a decoder-only\\nTransformer model on a diverse corpus of unlabeled text in a\\nself-supervised learning fashion (i.e., next word/token predic-'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 5}, page_content='tion), followed by discriminative fine-tuning on each specific\\ndownstream task (with much fewer samples), as illustrated in\\nFig 7. GPT-1 paves the way for subsequent GPT models, with\\neach version improving upon the architecture and achieving\\nbetter performance on various language tasks.\\nFig. 7: High-level overview of GPT pretraining, and fine-tuning\\nsteps. Courtesy of OpenAI.\\nGPT-2 [51] shows that language models are able to learn\\nto perform specific natural language tasks without any explicit\\nsupervision when trained on a large WebText dataset consisting\\nof millions of webpages. The GPT-2 model follows the model\\ndesigns of GPT-1 with a few modifications: Layer normal-\\nization is moved to the input of each sub-block, additional\\nlayer normalization is added after the final self-attention block,\\ninitialization is modified to account for the accumulation on\\nthe residual path and scaling the weights of residual layers,\\nvocabulary size is expanded to 50,25, and context size is\\nincreased from 512 to 1024 tokens.\\n3) Encoder-Decoder PLMs: In [52], Raffle et al. shows that\\nalmost all NLP tasks can be cast as a sequence-to-sequence\\ngeneration task. Thus, an encoder-decoder language model, by\\ndesign, is a unified model in that it can perform all natural\\nlanguage understanding and generation tasks. Representative\\nencoder-decoder PLMs we will review below are T5, mT5,\\nMASS, and BART.\\nT5 [52] is a Text-to-Text Transfer Transformer (T5) model,\\nwhere transfer learning is effectively exploited for NLP via an\\nintroduction of a unified framework in which all NLP tasks are\\ncast as a text-to-text generation task. mT5 [53] is a multilingual\\nvariant of T5, which is pre-trained on a new Common Crawl-\\nbased dataset consisting of texts in 101 languages.\\nMASS (MAsked Sequence to Sequence pre-training) [54]\\nadopts the encoder-decoder framework to reconstruct a sen-\\ntence fragment given the remaining part of the sentence. The\\nencoder takes a sentence with randomly masked fragment\\n(several consecutive tokens) as input, and the decoder predicts\\nthe masked fragment. In this way, MASS jointly trains the\\nencoder and decoder for language embedding and generation,\\nrespectively.\\nBART [55] uses a standard sequence-to-sequence transla-\\ntion model architecture. It is pre-trained by corrupting text with\\nan arbitrary noising function, and then learning to reconstruct\\nthe original text.B. Large Language Model Families\\nLarge language models (LLMs) mainly refer to\\ntransformer-based PLMs that contain tens to hundreds\\nof billions of parameters. Compared to PLMs reviewed above,\\nLLMs are not only much larger in model size, but also exhibit\\nstronger language understanding and generation and emergent\\nabilities that are not present in smaller-scale models. In what\\nfollows, we review three LLM families: GPT, LLaMA, and\\nPaLM, as illustrated in Fig 8.\\n1)The GPT Family :Generative Pre-trained Transform-\\ners (GPT) are a family of decoder-only Transformer-based\\nlanguage models, developed by OpenAI. This family con-\\nsists of GPT-1, GPT-2, GPT-3, InstrucGPT, ChatGPT, GPT-4,\\nCODEX, and WebGPT. Although early GPT models, such as\\nGPT-1 and GPT-2, are open-source, recent models, such as\\nGPT-3 and GPT-4, are close-source and can only be accessed\\nvia APIs. GPT-1 and GPT-2 models have been discussed in\\nthe early PLM subsection. We start with GPT-3 below.\\nGPT-3 [56] is a pre-trained autoregressive language model\\nwith 175 billion parameters. GPT-3 is widely considered as\\nthe first LLM in that it not only is much larger than previous\\nPLMs, but also for the first time demonstrates emergent\\nabilities that are not observed in previous smaller PLMs. GPT-\\n3 shows the emergent ability of in-context learning, which\\nmeans GPT-3 can be applied to any downstream tasks without\\nany gradient updates or fine-tuning, with tasks and few-shot\\ndemonstrations specified purely via text interaction with the\\nmodel. GPT-3 achieved strong performance on many NLP\\ntasks, including translation, question-answering, and the cloze\\ntasks, as well as several ones that require on-the-fly reasoning\\nor domain adaptation, such as unscrambling words, using a\\nnovel word in a sentence, 3-digit arithmetic. Fig 9 plots the\\nperformance of GPT-3 as a function of the number of examples\\nin in-context prompts.\\nCODEX [57], released by OpenAI in March 2023, is a\\ngeneral-purpose programming model that can parse natural\\nlanguage and generate code in response. CODEX is a de-\\nscendant of GPT-3, fine-tuned for programming applications\\non code corpora collected from GitHub. CODEX powers\\nMicrosoft‚Äôs GitHub Copilot.\\nWebGPT [58] is another descendant of GPT-3, fine-tuned to\\nanswer open-ended questions using a text-based web browser,\\nfacilitating users to search and navigate the web. Specifically,\\nWebGPT is trained in three steps. The first is for WebGPT\\nto learn to mimic human browsing behaviors using human\\ndemonstration data. Then, a reward function is learned to\\npredict human preferences. Finally, WebGPT is refined to\\noptimize the reward function via reinforcement learning and\\nrejection sampling.\\nTo enable LLMs to follow expected human instructions,\\nInstructGPT [59] is proposed to align language models with\\nuser intent on a wide range of tasks by fine-tuning with\\nhuman feedback. Starting with a set of labeler-written prompts\\nand prompts submitted through the OpenAI API, a dataset\\nof labeler demonstrations of the desired model behavior is\\ncollected. Then GPT-3 is fine-tuned on this dataset. Then, a\\ndataset of human-ranked model outputs is collected to further\\nfine-tune the model using reinforcement learning. The method\\nis known Reinforcement Learning from Human Feedback'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 6}, page_content='GPT Family PaLM Family    LLaMA  1/2 Family\\nGPT\\nGPT1\\nGPT2\\nGPT3\\nGPT4\\nGPT3.5 Turbo\\ntext-davinci\\ncode-davinci\\nCODEX\\nInstructGPT\\nWebGPT\\nGPT4 V ision\\nGPT4 Turbo\\nGorilla\\nMistral\\nVigogne\\nStable Beluga2\\nKoala\\nCode LLaMA\\nVicuna\\n Alpaca\\nBaize\\nLong LLaMA\\nGiraf fe\\nGuanaco\\nTulu\\nWizardLM\\nMed-PaLM\\nPaLM-E\\n Med-PaLM2\\nFLAN-PaLM\\n U-PaLM\\nPaLM2\\nPaLM\\nFig. 8: Popular LLM Families.\\nFig. 9: GPT-3 shows that larger models make increasingly\\nefficient use of in-context information. It shows in-context\\nlearning performance on a simple task requiring the model to\\nremove random symbols from a word, both with and without\\na natural language task description. Courtesy of [56].\\n(RLHF), as shown in 10. The resultant InstructGPT models\\nhave shown improvements in truthfulness and reductions in\\ntoxic output generation while having minimal performance\\nregressions on public NLP datasets.\\nFig. 10: The high-level overview of RLHF. Courtesy of [59].\\nThe most important milestone of LLM development is thelaunch of ChatGPT (Chat Generative Pre-trained Transformer)\\n[60] on November 30, 2022. ChatGPT is chatbot that enables\\nusers to steer a conversation to complete a wide range of\\ntasks such as question answering, information seeking, text\\nsummarization, and more. ChatGPT is powered by GPT-3.5\\n(and later by GPT-4), a sibling model to InstructGPT, which\\nis trained to follow an instruction in a prompt and provide a\\ndetailed response.\\nGPT-4 [33] is the latest and most powerful LLM in the\\nGPT family. Launched in March, 2023, GPT-4 is a multi-\\nmodal LLM in that it can take image and text as inputs and\\nproduce text outputs. While still less capable than humans\\nin some of the most challenging real-world scenarios, GPT-4\\nexhibits human-level performance on various professional and\\nacademic benchmarks, including passing a simulated bar exam\\nwith a score around the top 10% of test takers, as shown in\\nFig 11. Like early GPT models, GPT-4 was first pre-trained to\\npredict next tokens on large text corpora, and then fine-tuned\\nwith RLHF to align model behaviors with human-desired ones.\\n2)The LLaMA Family :LLaMA is a collection of founda-\\ntion language models, released by Meta. Unlike GPT models,\\nLLaMA models are open-source, i.e., model weights are\\nreleased to the research community under a noncommercial\\nlicense. Thus, the LLaMA family grows rapidly as these\\nmodels are widely used by many research groups to develop\\nbetter open-source LLMs to compete the closed-source ones or\\nto develop task-specific LLMs for mission-critical applications.\\nThe first set of LLaMA models [32] was released in Febru-\\nary 2023, ranging from 7B to 65B parameters. These models\\nare pre-trained on trillions of tokens, collected from publicly\\navailable datasets. LLaMA uses the transformer architecture of\\nGPT-3, with a few minor architectural modifications, including\\n(1) using a SwiGLU activation function instead of ReLU,\\n(2) using rotary positional embeddings instead of absolute\\npositional embedding, and (3) using root-mean-squared layer-\\nnormalization instead of standard layer-normalization. The\\nopen-source LLaMA-13B model outperforms the proprietary\\nGPT-3 (175B) model on most benchmarks, making it a good\\nbaseline for LLM research.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 7}, page_content='Fig. 11: GPT-4 performance on academic and professional\\nexams, compared with GPT 3.5. Courtesy of [33].\\nIn July 2023, Meta, in partnership with Microsoft, released\\nthe LLaMA-2 collection [61], which include both foundation\\nlanguage models and Chat models finetuned for dialog, known\\nas LLaMA-2 Chat. The LLaMA-2 Chat models were reported\\nto outperform other open-source models on many public\\nbenchmarks. Fig 12 shows the training process of LLaMA-2\\nChat. The process begins with pre-training LLaMA-2 using\\npublicly available online data. Then, an initial version of\\nLLaMA-2 Chat is built via supervised fine-tuning. Subse-\\nquently, the model is iteratively refined using RLHF, rejection\\nsampling and proximal policy optimization. In the RLHF stage,\\nthe accumulation of human feedback for revising the reward\\nmodel is crucial to prevent the reward model from being\\nchanged too much, which could hurt the stability of LLaMA\\nmodel training.\\nFig. 12: Training of LLaMA-2 Chat. Courtesy of [61].\\nAlpaca [62] is fine-tuned from the LLaMA-7B model using\\n52K instruction-following demonstrations generated in the\\nstyle of self-instruct using GPT-3.5 (text-davinci-003). Alpaca\\nis very cost-effective for training, especially for academic\\nresearch. On the self-instruct evaluation set, Alpaca performs\\nsimilarly to GPT-3.5, despite that Alpaca is much smaller.\\nThe Vicuna team has developed a 13B chat model, Vicuna-\\n13B, by fine-tuning LLaMA on user-shared conversationscollected from ShareGPT. Preliminary evaluation using GPT-\\n4 as a evaluator shows that Vicuna-13B achieves more than\\n90% quality of OpenAI‚Äôs ChatGPT, and Google‚Äôs Bard while\\noutperforming other models like LLaMA and Stanford Alpaca\\nin more than 90% of cases. 13 shows the relative response\\nquality of Vicuna and a few other well-known models by\\nGPT-4. Another advantage of Vicuna-13B is its relative limited\\ncomputational demand for model training. The training cost of\\nVicuna-13B is merely $300.\\nFig. 13: Relative Response Quality of Vicuna and a few other\\nwell-known models by GPT-4. Courtesy of Vicuna Team.\\nLike Alpaca and Vicuna, the Guanaco models [63] are also\\nfinetuned LLaMA models using instruction-following data. But\\nthe finetuning is done very efficiently using QLoRA such\\nthat finetuning a 65B parameter model can be done on a\\nsingle 48GB GPU. QLoRA back-propagates gradients through\\na frozen, 4-bit quantized pre-trained language model into Low\\nRank Adapters (LoRA). The best Guanaco model outperforms\\nall previously released models on the Vicuna benchmark,\\nreaching 99.3% of the performance level of ChatGPT while\\nonly requiring 24 hours of fine-tuning on a single GPU.\\nKoala [64] is yet another instruction-following language\\nmodel built on LLaMA, but with a specific focus on interaction\\ndata that include user inputs and responses generated by highly\\ncapable closed-source chat models such as ChatGPT. The\\nKoala-13B model performs competitively with state-of-the-art\\nchat models according to human evaluation based on real-\\nworld user prompts.\\nMistral-7B [65] is a 7B-parameter language model engi-\\nneered for superior performance and efficiency. Mistral-7B\\noutperforms the best open-source 13B model (LLaMA-2-13B)\\nacross all evaluated benchmarks, and the best open-source\\n34B model (LLaMA-34B) in reasoning, mathematics, and code\\ngeneration. This model leverages grouped-query attention for\\nfaster inference, coupled with sliding window attention to\\neffectively handle sequences of arbitrary length with a reduced\\ninference cost.\\nThe LLaMA family is growing rapidly, as more instruction-\\nfollowing models have been built on LLaMA or LLaMA-\\n2, including Code LLaMA [66], Gorilla [67], Giraffe [68],\\nVigogne [69], Tulu 65B [70], Long LLaMA [71], and Stable\\nBeluga2 [72], just to name a few.\\n3)The PaLM Family :The PaLM (Pathways Language\\nModel) family are developed by Google. The first PaLM\\nmodel [31] was announced in April 2022 and remained private\\nuntil March 2023. It is a 540B parameter transformer-based\\nLLM. The model is pre-trained on a high-quality text corpus\\nconsisting of 780 billion tokens that comprise a wide range\\nof natural language tasks and use cases. PaLM is pre-trained'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 8}, page_content='on 6144 TPU v4 chips using the Pathways system, which\\nenables highly efficient training across multiple TPU Pods.\\nPaLM demonstrates continued benefits of scaling by achiev-\\ning state-of-the-art few-shot learning results on hundreds of\\nlanguage understanding and generation benchmarks. PaLM-\\n540B outperforms not only state-of-the-art fine-tuned models\\non a suite of multi-step reasoning tasks, but also on par with\\nhumans on the recently released BIG-bench benchmark.\\nThe U-PaLM models of 8B, 62B, and 540B scales are\\ncontinually trained on PaLM with UL2R, a method of continue\\ntraining LLMs on a few steps with UL2‚Äôs mixture-of-denoiser\\nobjective [73]. An approximately 2x computational savings\\nrate is reported.\\nU-PaLM is later instruction-finetuned as Flan-PaLM [74].\\nCompared to other instruction finetuning work mentioned\\nabove, Flan-PaLM‚Äôs finetuning is performed using a much\\nlarger number of tasks, larger model sizes, and chain-of-\\nthought data. As a result, Flan-PaLM substantially outperforms\\nprevious instruction-following models. For instance, Flan-\\nPaLM-540B, which is instruction-finetuned on 1.8K tasks,\\noutperforms PaLM-540B by a large margin (+9.4% on av-\\nerage). The finetuning data comprises 473 datasets, 146 task\\ncategories, and 1,836 total tasks, as illustrated in Fig 14.\\nFig. 14: Flan-PaLM finetuning consist of 473 datasets in above\\ntask categories. Courtesy of [74].\\nPaLM-2 [75] is a more compute-efficient LLM with bet-\\nter multilingual and reasoning capabilities, compared to its\\npredecessor PaLM. PaLM-2 is trained using a mixture of\\nobjectives. Through extensive evaluations on English, multi-\\nlingual, and reasoning tasks, PaLM-2 significantly improves\\nthe model performance on downstream tasks across different\\nmodel sizes, while simultaneously exhibiting faster and more\\nefficient inference than PaLM.\\nMed-PaLM [76] is a domain-specific PaLM, and is de-\\nsigned to provide high-quality answers to medical questions.\\nMed-PaLM is finetuned on PaLM using instruction prompt\\ntuning, a parameter-efficient method for aligning LLMs to\\nnew domains using a few exemplars. Med-PaLM obtains very\\nencouraging results on many healthcare tasks, although it is\\nstill inferior to human clinicians. Med-PaLM 2 improves Med-\\nPaLM via med-domain finetuning and ensemble prompting[77]. Med-PaLM 2 scored up to 86.5% on the MedQA\\ndataset (i.e., a benchmark combining six existing open ques-\\ntion answering datasets spanning professional medical exams,\\nresearch, and consumer queries), improving upon Med-PaLM\\nby over 19% and setting a new state-of-the-art.\\nC. Other Representative LLMs\\nIn addition to the models discussed in the previous sub-\\nsections, there are other popular LLMs which do not belong\\nto those three model families, yet they have achieved great\\nperformance and have pushed the LLMs field forward. We\\nbriefly describe these LLMs in this subsection.\\nFLAN: In [78], Wei et al. explored a simple method for\\nimproving the zero-shot learning abilities of language models.\\nThey showed that instruction tuning language models on a\\ncollection of datasets described via instructions substantially\\nimproves zero-shot performance on unseen tasks. They take\\na 137B parameter pretrained language model and instruction\\ntune it on over 60 NLP datasets verbalized via natural language\\ninstruction templates. They call this instruction-tuned model\\nFLAN. Fig 15 provides a comparison of instruction tuning\\nwith pretrain‚Äìfinetune and prompting.\\nFig. 15: comparison of instruction tuning with pre-\\ntrain‚Äìfinetune and prompting. Courtesy of [78].\\nGopher: In [79], Rae et al. presented an analysis of\\nTransformer-based language model performance across a wide\\nrange of model scales ‚Äî from models with tens of millions of\\nparameters up to a 280 billion parameter model called Gopher.\\nThese models were evaluated on 152 diverse tasks, achieving\\nstate-of-the-art performance across the majority. The number\\nof layers, the key/value size, and other hyper-parameters of\\ndifferent model sizes are shown in Fig 16.\\nFig. 16: Model architecture details of Gopher with different\\nnumber of parameters. Courtesy of [78].\\nT0:In [80], Sanh et al. developed T0, a system for easily\\nmapping any natural language tasks into a human-readable\\nprompted form. They converted a large set of supervised\\ndatasets, each with multiple prompts with diverse wording.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 9}, page_content='These prompted datasets allow for benchmarking the ability\\nof a model to perform completely held-out tasks. Then, a\\nT0 encoder-decoder model is developed to consume textual\\ninputs and produces target responses. The model is trained on\\na multitask mixture of NLP datasets partitioned into different\\ntasks.\\nERNIE 3.0: In [81], Sun et al. proposed a unified frame-\\nwork named ERNIE 3.0 for pre-training large-scale knowledge\\nenhanced models. It fuses auto-regressive network and auto-\\nencoding network, so that the trained model can be easily tai-\\nlored for both natural language understanding and generation\\ntasks using zero-shot learning, few-shot learning or fine-tuning.\\nThey have trained ERNIE 3.0 with 10 billion parameters\\non a 4TB corpus consisting of plain texts and a large-scale\\nknowledge graph. Fig 17 illustrates the model architecture of\\nErnie 3.0.\\nFig. 17: High-level model architecture of ERNIE 3.0. Courtesy\\nof [81].\\nRETRO: In [82], Borgeaud et al. enhanced auto-regressive\\nlanguage models by conditioning on document chunks re-\\ntrieved from a large corpus, based on local similarity with pre-\\nceding tokens. Using a 2-trillion-token database, the Retrieval-\\nEnhanced Transformer (Retro) obtains comparable perfor-\\nmance to GPT-3 and Jurassic-1 [83] on the Pile, despite using\\n25% fewer parameters. As shown in Fig 18, Retro combines\\na frozen Bert retriever, a differentiable encoder and a chunked\\ncross-attention mechanism to predict tokens based on an order\\nof magnitude more data than what is typically consumed\\nduring training.\\nGLaM: In [84], Du et al. proposed a family of LLMs\\nnamed GLaM (Generalist Language Model), which use a\\nsparsely activated mixture-of-experts architecture to scale the\\nmodel capacity while also incurring substantially less training\\ncost compared to dense variants. The largest GLaM has 1.2\\ntrillion parameters, which is approximately 7x larger than GPT-\\n3. It consumes only 1/3 of the energy used to train GPT-3 and\\nrequires half of the computation flops for inference, while still\\nachieving better overall zero, one and few-shot performance\\nacross 29 NLP tasks. Fig 19 shows the high-level architecture\\nof GLAM.\\nLaMDA: In [85], Thoppilan et al. presented LaMDA, a\\nfamily of Transformer-based neural language models special-\\nized for dialog, which have up to 137B parameters and are\\npre-trained on 1.56T words of public dialog data and web text.\\nFig. 18: Retro architecture. Left: simplified version where a\\nsequence of length n = 12 is split into l = 3 chunks of size\\nm = 4. For each chunk, we retrieve k = 2 neighbours of r =\\n5 tokens each. The retrieval pathway is shown on top. Right:\\nDetails of the interactions in the CCA operator. Causality is\\nmaintained as neighbours of the first chunk only affect the last\\ntoken of the first chunk and tokens from the second chunk.\\nCourtesy of [82].\\nFig. 19: GLaM model architecture. Each MoE layer (the\\nbottom block) is interleaved with a Transformer layer (the\\nupper block). Courtesy of [84].\\nThey showed that fine-tuning with annotated data and enabling\\nthe model to consult external knowledge sources can lead to\\nsignificant improvements towards the two key challenges of\\nsafety and factual grounding.\\nOPT: In [86], Zhang et al. presented Open Pre-trained\\nTransformers (OPT), a suite of decoder-only pre-trained trans-\\nformers ranging from 125M to 175B parameters, which they\\nshare with researchers. The OPT models‚Äô parameters are\\nshown in 20\\nFig. 20: Different OPT Models‚Äô architecture details. Courtesy\\nof [86].'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 10}, page_content='Chinchilla: In [2], Hoffmann et al. investigated the optimal\\nmodel size and number of tokens for training a transformer\\nlanguage model under a given compute budget. By training\\nover 400 language models ranging from 70 million to over\\n16 billion parameters on 5 to 500 billion tokens, they found\\nthat for compute-optimal training, the model size and the\\nnumber of training tokens should be scaled equally: for every\\ndoubling of model size the number of training tokens should\\nalso be doubled. They tested this hypothesis by training a\\npredicted compute-optimal model, Chinchilla, that uses the\\nsame compute budget as Gopher but with 70B parameters and\\n4% more more data.\\nGalactica: In [87], Taylor et al. introduced Galactica, a\\nlarge language model that can store, combine and reason about\\nscientific knowledge. They trained on a large scientific corpus\\nof papers, reference material, knowledge bases and many other\\nsources. Galactica performed well on reasoning, outperforming\\nChinchilla on mathematical MMLU by 41.3% to 35.7%, and\\nPaLM 540B on MATH with a score of 20.4% versus 8.8%.\\nCodeGen: In [88], Nijkamp et al. trained and released\\na family of large language models up to 16.1B parameters,\\ncalled CODEGEN, on natural language and programming\\nlanguage data, and open sourced the training library JAX-\\nFORMER. They showed the utility of the trained model by\\ndemonstrating that it is competitive with the previous state-of-\\nthe-art on zero-shot Python code generation on HumanEval.\\nThey further investigated the multi-step paradigm for program\\nsynthesis, where a single program is factorized into multi-\\nple prompts specifying sub-problems. They also constructed\\nan open benchmark, Multi-Turn Programming Benchmark\\n(MTPB), consisting of 115 diverse problem sets that are\\nfactorized into multi-turn prompts.\\nAlexaTM: In [89], Soltan et al. demonstrated that mul-\\ntilingual large-scale sequence-to-sequence (seq2seq) models,\\npre-trained on a mixture of denoising and Causal Language\\nModeling (CLM) tasks, are more efficient few-shot learners\\nthan decoder-only models on various task. They trained a\\n20 billion parameter multilingual seq2seq model called Alexa\\nTeacher Model (AlexaTM 20B) and showed that it achieves\\nstate-of-the-art (SOTA) performance on 1-shot summarization\\ntasks, outperforming a much larger 540B PaLM decoder\\nmodel. AlexaTM consist of 46 encoder layers, 32 decoder\\nlayers, 32 attention heads, and dmodel = 4096 .\\nSparrow: In [90], Glaese et al. presented Sparrow, an\\ninformation-seeking dialogue agent trained to be more helpful,\\ncorrect, and harmless compared to prompted language model\\nbaselines. They used reinforcement learning from human feed-\\nback to train their models with two new additions to help\\nhuman raters judge agent behaviour. The high-level pipeline\\nof Sparrow model is shown in Fig 21.\\nMinerva: In [91], Lewkowycz et al. introduced Minerva,\\na large language model pretrained on general natural language\\ndata and further trained on technical content, to tackle previous\\nLLM struggle with quantitative reasoning (such as solving\\nmathematics, science, and engineering problems).\\nMoD: In [92], Tay et al. presented a generalized and\\nunified perspective for self-supervision in NLP and show how\\ndifferent pre-training objectives can be cast as one another\\nand how interpolating between different objectives can be\\nFig. 21: Sparrow pipeline relies on human participation to\\ncontinually expand a training set. Courtesy of [90].\\neffective. They proposed Mixture-of-Denoisers (MoD), a pre-\\ntraining objective that combines diverse pre-training paradigms\\ntogether. This framework is known as Unifying Language\\nLearning (UL2). An overview of UL2 pretraining paradigm\\nis shown in Fig 21.\\nFig. 22: An overview of UL2 pretraining paradigm. Courtesy\\nof [92].\\nBLOOM: In [93], Scao et al. presented BLOOM, a 176B-\\nparameter open-access language model designed and built\\nthanks to a collaboration of hundreds of researchers. BLOOM\\nis a decoder-only Transformer language model trained on the\\nROOTS corpus, a dataset comprising hundreds of sources in\\n46 natural and 13 programming languages (59 in total). An\\noverview of BLOOM architecture is shown in Fig 23.\\nFig. 23: An overview of BLOOM architecture. Courtesy of\\n[93].\\nGLM: In [94], Zeng et al. introduced GLM-130B, a'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 11}, page_content='bilingual (English and Chinese) pre-trained language model\\nwith 130 billion parameters. It was an attempt to open-source\\na 100B-scale model at least as good as GPT-3 (davinci) and\\nunveil how models of such a scale can be successfully pre-\\ntrained.\\nPythia: In [95], Biderman et al. introduced Pythia, a suite\\nof 16 LLMs all trained on public data seen in the exact same\\norder and ranging in size from 70M to 12B parameters. We\\nprovide public access to 154 checkpoints for each one of the\\n16 models, alongside tools to download and reconstruct their\\nexact training dataloaders for further study.\\nOrca: In [96], Mukherjee et al. develop Orca, a 13-billion\\nparameter model that learns to imitate the reasoning process\\nof large foundation models. Orca learns from rich signals\\nfrom GPT-4 including explanation traces; step-by-step thought\\nprocesses; and other complex instructions, guided by teacher\\nassistance from ChatGPT.\\nStarCoder: In [97], Li et al. introduced StarCoder and\\nStarCoderBase. They are 15.5B parameter models with 8K\\ncontext length, infilling capabilities and fast large-batch in-\\nference enabled by multi-query attention. StarCoderBase is\\ntrained on one trillion tokens sourced from The Stack, a\\nlarge collection of permissively licensed GitHub repositories\\nwith inspection tools and an opt-out process. They fine-tuned\\nStarCoderBase on 35B Python tokens, resulting in the creation\\nof StarCoder. They performed the most comprehensive evalu-\\nation of Code LLMs to date and showed that StarCoderBase\\noutperforms every open Code LLM that supports multiple pro-\\ngramming languages and matches or outperforms the OpenAI\\ncode-cushman-001 model.\\nKOSMOS: In [98], Huang et al. introduced KOSMOS-1,\\na Multimodal Large Language Model (MLLM) that can per-\\nceive general modalities, learn in context (i.e., few-shot), and\\nfollow instructions (i.e. zero-shot). Specifically, they trained\\nKOSMOS-1 from scratch on web-scale multi-modal corpora,\\nincluding arbitrarily interleaved text and images, image-caption\\npairs, and text data. Experimental results show that KOSMOS-\\n1 achieves impressive performance on (i) language understand-\\ning, generation, and even OCR-free NLP (directly fed with\\ndocument images), (ii) perception-language tasks, including\\nmultimodal dialogue, image captioning, visual question an-\\nswering, and (iii) vision tasks, such as image recognition with\\ndescriptions (specifying classification via text instructions).\\nGemini: In [99], Gemini team introduced a new family of\\nmultimodal models, that exhibit promising capabilities across\\nimage, audio, video, and text understanding. Gemini family\\nincludes three versions: Ultra for highly-complex tasks, Pro\\nfor enhanced performance and deployability at scale, and Nano\\nfor on-device applications. Gemini architecture is built on top\\nof Transformer decoders, and is trained to support 32k context\\nlength (via using efficient attention mechanisms).\\nSome of the other popular LLM frameworks (or techniques\\nused for efficient developments of LLMs) includes Inner-\\nMonologue [100], Megatron-Turing NLG [101], LongFormer\\n[102], OPT-IML [103], MeTaLM [104], Dromedary [105],\\nPalmyra [106], Camel [107], Yalm [108], MPT [109], ORCA-\\n2 [110], Gorilla [67], PAL [111], Claude [112], CodeGen 2\\n[113], Zephyr [114], Grok [115], Qwen [116], Mamba [30],\\nMixtral-8x7B [117], DocLLM [118], DeepSeek-Coder [119],FuseLLM-7B [120], TinyLlama-1.1B [121], LLaMA-Pro-8B\\n[122].\\nFig 24 provides an overview of some of the most repre-\\nsentative LLM frameworks, and the relevant works that have\\ncontributed to the success of LLMs and helped to push the\\nlimits of LLMs.\\nIII. H OWLLM SAREBUILT\\nIn this section, we first review the popular architectures\\nused for LLMs, and then discuss data and modeling techniques\\nranging from data preparation, tokenization, to pre-training,\\ninstruction tuning, and alignment.\\nOnce the model architecture is chosen, the major steps\\ninvolved in training an LLM includes: data preparation (col-\\nlection, cleaning, deduping, etc.), tokenization, model pre-\\ntraining (in a self-supervised learning fashion), instruction\\ntuning, and alignment. We will explain each of them in a\\nseparate subsection below. These steps are also illustrated in\\nFig 25.\\nA. Dominant LLM Architectures\\nThe most widely used LLM architectures are encoder-only,\\ndecoder-only, and encoder-decoder. Most of them are based on\\nTransformer (as the building block). Therefore we also review\\nthe Transformer architecture here.\\n1)Transformer :in a ground-breaking work [44], Vaswani\\net al. proposed the Transformer framework, which was orig-\\ninally designed for effective parallel computing using GPUs.\\nThe heart of Transformer is the (self-)attention mechanism,\\nwhich can capture long-term contextual information much\\nmore effectively using GPUs than the recurrence and convo-\\nlution mechanisms. Fig 26 provides a high-level overview of\\ntransformer work. In this section we provide an overview of the\\nmain elements and variants, see [44], [123] for more details.\\nThe Transformer language model architecture, originally\\nproposed for machine translation, consists of an encoder and\\na decoder. The encoder is composed of a stack of N = 6\\nidentical Transformer layers. Each layer has two sub-layers.\\nThe first one is a multi-head self-attention layer, and the other\\none is a simple position-wise fully connected feed-forward\\nnetwork. The decoder is composed of a stack of 6 identical\\nlayers. In addition to the two sub-layers in each encoder layer,\\nthe decoder has a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. The attention\\nfunction can be described as mapping a query and a set of key-\\nvalue pairs to an output, where the query, keys, values, and\\noutput are all vectors. The output is computed as a weighted\\nsum of the values, where the weight assigned to each value\\nis computed by a compatibility function of the query with the\\ncorresponding key. Instead of performing a single attention\\nfunction with dmodel dimensional keys, values and queries,\\nit is found to be beneficial to linearly project the queries,\\nkeys and values hwith different, learned linear projections to\\ndk,dkanddvdimensions, respectively. Positional encoding is\\nincorporated to fuse information about the relative or absolute\\nposition of the tokens in the sequence.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 12}, page_content='Fig. 24: Timeline of some of the most representative LLM frameworks (so far). In addition to large language models with our\\n#parameters threshold, we included a few representative works, which pushed the limits of language models, and paved the way\\nfor their success (e.g. vanilla Transformer, BERT, GPT-1), as well as some small language models. ‚ô£shows entities that serve\\nnot only as models but also as approaches. ‚ô¶shows only approaches.\\n2)Encoder-Only :For this family, at each stage, the atten-\\ntion layers can access all the words in the initial sentence.\\nThe pre-training of these models usually consist of some-\\nhow corrupting a given sentence (for instance, by masking\\nrandom words in it) and tasking the model with finding or\\nreconstructing the initial sentence. Encoder models are great\\nfor tasks requiring an understanding of the full sequence,\\nsuch as sentence classification, named entity recognition, and\\nextractive question answering. One prominent encoder only\\nmodel is BERT (Bidirectional Encoder Representations from\\nTransformers), proposed in [24].\\n3)Decoder-Only :For these models, at each stage, for any\\nword, the attention layers can only access the words positioned\\nbefore that in the sentence. These models are also sometimes\\ncalled auto-regressive models. The pretraining of these models\\nis usually formulated as predicting the next word (or token)\\nin the sequence. The decoder-only models are best suited for\\ntasks involving text generation. GPT models are prominent\\nexample of this model category.\\n4)Encoder-Decoder :These models use both encoder and\\ndecoder, and are sometimes called sequence-to-sequence mod-\\nels. At each stage, the attention layers of the encoder can access\\nall the words in the initial sentence, whereas the attention\\nlayers of the decoder only accesses the words positioned before\\na given word in the input. These models are usually pre-\\ntrained using the objectives of encoder or decoder models, but\\nusually involve something a bit more complex. For instance,\\nsome models are pretrained by replacing random spans of text\\n(that can contain several words) with a single mask special\\nword, and the objective is then to predict the text that thismask word replaces. Encoder-decoder models are best suited\\nfor tasks about generating new sentences conditioned on a\\ngiven input, such as summarization, translation, or generative\\nquestion answering.\\nB. Data Cleaning\\nData quality is crucial to the performance of language\\nmodels trained on them. Data cleaning techniques such as\\nfiltering, deduplication, are shown to have a big impact on\\nthe model performance.\\nAs an example, in Falcon40B [124], Penedo et al. showed\\nthat properly filtered and deduplicated web data alone can lead\\nto powerful models; even significantly outperforming models\\nfrom the state-of-the-art trained on The Pile. Despite extensive\\nfiltering, they were able to obtain five trillion tokens from\\nCommonCrawl. They also released an extract of 600 billion\\ntokens from our REFINEDWEB dataset, and 1.3/7.5B param-\\neters language models trained on it. 27 shows the Refinement\\nprocess of CommonCrawl data by this work.\\n1) Data Filtering: Data filtering aims to enhance the qual-\\nity of training data and the effectiveness of the trained LLMs.\\nCommon data filtering techniques include:\\nRemoving Noise: refers to eliminating irrelevant or noisy\\ndata that might impact the model‚Äôs ability to generalize well.\\nAs an example, one can think of removing false information\\nfrom the training data, to lower the chance of model generating\\nfalse responses. Two mainstream approaches for quality filter-\\ning includes: classifier-based, and heuristic-based frameworks.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 13}, page_content='How LLMs Are Built?\\nData Cleaning\\nTokenizationsBytePairEncoding\\nWordPieceEncoding\\nSentencePieceEncoding\\nPositional EncodingAbsolute Positional Embeddings\\nRelative Positional Embeddings\\nRotary Position Embeddings\\nRelative Positional Bias\\nModel Pre-trainingMasked Language Modeling\\nCausal Language Modeling\\nNext Sentence Prediction\\nMixture of Experts\\nFine-tuning and Instruction T uning\\nAlignmentSupervised learning\\nReinforcement Learning from Human Feedback\\nDirect Preference Optimization\\nKahneman-Tversky Optimization\\nDecoding StrategiesGreedy Search\\nBeam Search\\nTop-k Sampling\\nTop-p Sampling\\nCost-Effective T raining/Inference,\\nAdaptation & CompressionOptimized T raining\\nZero Redundancy Optimizer\\nReceptance W eighted Key V alue\\nLow-Rank Adaption\\nKnowledge Distillation\\nQuantizationData Filtering\\nRemoving Noise\\nHandling Outliers\\nAddressing Imbalances\\nText Preprocessing\\nDeduplication\\nLLM ArchitecturesEncoder-Only\\nDecoder-Only\\nEncoder-Decoder\\n...\\nSupervised Fine-tuning\\nGeneral Fine-tuning\\nMulti- turn I nstructions\\nInstruction FollowingFig. 25: This figure shows different components of LLMs.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 14}, page_content='Fig. 26: High-level overview of transformer work. Courtesy of\\n[44].\\nFig. 27: Subsequent stages of Macrodata Refinement remove\\nnearly 90% of the documents originally in CommonCrawl.\\nCourtesy of [124].\\nHandling Outliers: Identifying and handling outliers or\\nanomalies in the data to prevent them from disproportionately\\ninfluencing the model.\\nAddressing Imbalances: Balancing the distribution of\\nclasses or categories in the dataset to avoid biases and ensure\\nfair representation. This is specially useful for responsible\\nmodel training and evaluation.\\nText Preprocessing: Cleaning and standardizing text data\\nby removing stop words, punctuation, or other elements that\\nmay not contribute significantly to the model‚Äôs learning.\\nDealing with Ambiguities: Resolving or excluding am-\\nbiguous or contradictory data that might confuse the model\\nduring training. This can help the model to provide more\\ndefinite and reliable answers.\\n2) Deduplication: De-duplication refers to the process of\\nremoving duplicate instances or repeated occurrences of the\\nsame data in a dataset. Duplicate data points can introducebiases in the model training process and reduce the diversity, as\\nthe model may learn from the same examples multiple times,\\npotentially leading to overfitting on those particular instances.\\nSome works [125] have shown that de-duplication improves\\nmodels‚Äô ability to generalize to new, unseen data.\\nThe de-duplication process is particularly important when\\ndealing with large datasets, as duplicates can unintentionally\\ninflate the importance of certain patterns or characteristics.\\nThis is especially relevant in NLP tasks, where diverse and\\nrepresentative training data is crucial for building robust lan-\\nguage models.\\nThe specific de-duplication method can vary based on\\nthe nature of the data and the requirements of the particular\\nlanguage model being trained. It may involve comparing entire\\ndata points or specific features to identify and eliminate du-\\nplicates. At the document level, existing works mainly rely on\\nthe overlap ratio of high-level features (e.g. n-grams overlap)\\nbetween documents to detect duplicate samples.\\nC. Tokenizations\\nTokenization referes to the process of converting a se-\\nquence of text into smaller parts, known as tokens. While\\nthe simplest tokenization tool simply chops text into tokens\\nbased on white space, most tokenization tools rely on a word\\ndictionary. However, out-of-vocabulary (OOV) is a problem\\nin this case because the tokenizer only knows words in its\\ndictionary. To increase the coverage of dictionaries, popular\\ntokenizers used for LLMs are based on sub-words, which can\\nbe combined to form a large number of words, including the\\nwords unseen in training data or words in different languages.\\nIn what follows, we describe three popular tokenizers.\\n1)BytePairEncoding : BytePairEncoding is originally a\\ntype of data compression algorithm that uses frequent patterns\\nat byte level to compress the data. By definition, this algorithm\\nmainly tries to keep the frequent words in their original form\\nand break down ones that are not common. This simple\\nparadigm keeps the vocabulary not very large, but also good\\nenough to represent common words at the same time. Also\\nmorphological forms of the frequent words can be represented\\nvery well if suffix or prefix is also commonly presented in the\\ntraining data of the algorithm.\\n2)WordPieceEncoding :This algorithm is mainly used for\\nvery well-known models such as BERT and Electra. At the\\nbeginning of training, the algorithm takes all the alphabet from\\nthe training data to make sure that nothing will be left as UNK\\norunknown from the training dataset. This case happens when\\nthe model is given an input that can not be tokenized by the\\ntokenizer. It mostly happens in cases where some characters are\\nnot tokenizable by it. Similar to BytePairEncoding, it tries to\\nmaximize the likelihood of putting all the tokens in vocabulary\\nbased on their frequency.\\n3)SentencePieceEncoding :Although both tokenizers de-\\nscribed before are strong and have many advantages compared\\nto white-space tokenization, they still take assumption of\\nwords being always separated by white-space as granted. This\\nassumption is not always true, in fact in some languages, words\\ncan be corrupted by many noisy elements such as unwanted\\nspaces or even invented words. SentencePieceEncoding tries\\nto address this issue.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 15}, page_content='D.Positional Encoding\\n1)Absolute Positional Embeddings :(APE) [44] has been\\nused in the original Transformer model to preserve the infor-\\nmation of sequence order. Therefore, the positional information\\nof words is added to the input embeddings at the bottom of\\nboth the encoder and decoder stacks. There are various options\\nfor positional encodings, either learned or fixed. In the vanilla\\nTransformer, sine and cosine functions are employed for this\\npurpose. The main drawback of using APE in Transformers\\nis the restriction to a certain number of tokens. Additionally,\\nAPE fails to account for the relative distances between tokens.\\n2)Relative Positional Embeddings :(RPE) [126] involves\\nextending self-attention to take into account the pairwise links\\nbetween input elements. RPE is added to the model at two\\nlevels: first as an additional component to the keys, and\\nsubsequently as a sub-component of the values matrix. This\\napproach looks at the input as a fully-connected graph with\\nlabels and directed edges. In the case of linear sequences, edges\\ncan capture information about the relative position differences\\nbetween input elements. A clipping distance, represented as k\\n2‚â§k‚â§n‚àí4, specifies the maximum limit on relative lo-\\ncations. This allows the model to make reasonable predictions\\nfor sequence lengths that are not part of the training data.\\n3)Rotary Position Embeddings :Rotary Positional Em-\\nbedding (RoPE) [127] tackles problems with existing ap-\\nproaches. Learned absolute positional encodings can lack gen-\\neralizability and meaningfulness, particularly when sentences\\nare short. Moreover, current methods like T5‚Äôs positional\\nembedding face challenges with constructing a full attention\\nmatrix between positions. RoPE uses a rotation matrix to\\nencode the absolute position of words and simultaneously in-\\ncludes explicit relative position details in self-attention. RoPE\\nbrings useful features like flexibility with sentence lengths, a\\ndecrease in word dependency as relative distances increase,\\nand the ability to improve linear self-attention with relative\\nposition encoding. GPT-NeoX-20B, PaLM, CODEGEN, and\\nLLaMA are among models that take advantage of RoPE in\\ntheir architectures.\\n4)Relative Positional Bias :The concept behind this type\\nof positional embedding is to facilitate extrapolation during\\ninference for sequences longer than those encountered in train-\\ning. In [128] Press et al. proposed Attention with Linear Biases\\n(ALiBi). Instead of simply adding positional embeddings to\\nword embeddings, they introduced a bias to the attention scores\\nof query-key pairs, imposing a penalty proportional to their\\ndistance. In the BLOOM model, ALiBi is leveraged.\\nE. Model Pre-training\\nPre-training is the very first step in large language model\\ntraining pipeline, and it helps LLMs to acquire fundamental\\nlanguage understanding capabilities, which can be useful in a\\nwide range of language related tasks. During pre-training, the\\nLLM is trained on a massive amount of (usually) unlabeled\\ntexts, usually in a self-supervised manner. There are different\\napproaches used for pre-training like next sentence prediction\\n[24], two most common ones include, next token prediction\\n(autoregressive language modeling), and masked language\\nmodeling.InAutoregressive Language Modeling framework, given\\na sequence of ntokens x1, ...,xn, the model tries to predict\\nnext token xn+1(and sometimes next sequence of tokens) in\\nan auto-regressive fashion. One popular loss function in this\\ncase is the log-likelihood of predicted tokens as shown in Eq\\n2\\nLALM(x) =NX\\ni=1p(xi+n|xi, ..., x i+n‚àí1) (1)\\nGiven the auto-regressive nature of this framework, the\\ndecoder-only models are naturally better suited to learn how\\nto accomplish these task.\\nInMasked Language Modeling , some words are masked\\nin a sequence and the model is trained to predict the masked\\nwords based on the surrounding context. Sometimes people\\nrefer to this approach as denoising autoencoding, too. If we\\ndenote the masked/corrupted samples in the sequence x, asÀúx,\\nthen the training objective of this approach can be written as:\\nLMLM (x) =NX\\ni=1p(Àúx|x\\\\Àúx) (2)\\nAnd more recently, Mixture of Experts (MoE) [130],\\n[131] have become very popular in LLM space too. MoEs\\nenable models to be pre-trained with much less compute,\\nwhich means one can dramatically scale up the model or\\ndataset size with the same compute budget as a dense model.\\nMoE consists of two main elements: Sparse MoE layers ,\\nwhich are used instead of dense feed-forward network (FFN)\\nlayers, and have a certain number of ‚Äúexperts‚Äù (e.g. 8), in\\nwhich each expert is a neural network. In practice, the experts\\nare FFNs, but they can also be more complex networks. A gate\\nnetwork or router , that determines which tokens are sent to\\nwhich expert. It is worth noting that, one can send a token\\nto more than one expert. How to route a token to an expert\\nis one of the big decisions when working with MoEs - the\\nrouter is composed of learned parameters and is pretrained at\\nthe same time as the rest of the network. Fig 29 provides an\\nillustration of a Switch Transformer encoder block, which are\\nused in MoE.\\nF . Fine-tuning and Instruction Tuning\\nEarly language models such as BERT trained using self-\\nsupervision as explained in section III-E were not able to\\nperform specific tasks. In order for the foundation model to be\\nuseful it needed to be fine-tuned to a specific task with labeled\\ndata (so-called supervised fine-tuning or SFT for short). For\\nexample, in the original BERT paper [24], the model was fine-\\ntuned to 11 different tasks. While more recent LLMs no longer\\nrequire fine-tuning to be used, they can still benefit from task\\nor data-specific fine-tuning. For example, OpenAI reports that\\nthe much smaller GPT-3.5 Turbo model can outperform GPT-4\\nwhen fine-tuned with task specific data2.\\nFine-tuning does not need to be performed to a single\\ntask though, and there are different approaches to multi-task\\nfine-tuning (see e.g. Mahabi et al. [132]). Fine-tuning to one\\nor more tasks is known to improve results and reduce the\\ncomplexity of prompt engineering, and it can serve as an\\n2https://platform.openai.com/docs/guides/fine-tuning'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 16}, page_content='(a) Absolute Positional Embeddings [129]\\n (b) Relative Positional Embeddings\\n(c) Rotary Positional Embedding [127]\\n (d) Relative Positional Bias [128]\\nFig. 28: Various positional encodings are employed in LLMs.\\nFig. 29: : Illustration of a Switch Transformer encoder block.\\nThey replaced the dense feed forward network (FFN) layer\\npresent in the Transformer with a sparse Switch FFN layer\\n(light blue). . Courtesy of [131].\\nalternative to retrieval augmented generation. Furthermore,\\nthere are other reasons why it might be advisable to fine-tune.\\nFor example, one might want to fine-tune to expose the model\\nto new or proprietary data that it has not been exposed to\\nduring pre-training.\\nAn important reason to fine-tune LLMs is to align the\\nresponses to the expectations humans will have when providing\\ninstructions through prompts. This is the so-called instruction\\ntuning [133]. We dive into the details of how to design\\nand engineer prompts in section IV-B, but in the context\\nof instruction tuning, it is important to understand that the\\ninstruction is a prompt that specifies the task that the LLM\\nshould accomplish. Instruction tuning datasets such as NaturalInstructions [134] include not only the task definition but other\\ncomponents such as positive/negative examples or things to\\navoid.\\nThe specific approach and instruction datasets used to\\ninstruction-tune an LLM varies, but, generally speaking, in-\\nstruction tuned models outperform their original foundation\\nmodels they are based on. For example, InstructGPT [59]\\noutperforms GPT-3 on most benchmarks. The same is true\\nfor Alpaca [62] when compared to LLaMA.\\nSelf-Instruct [135], proposed by Wang et al. is also a\\npopular approach along this line, in which they introduced a\\nframework for improving the instruction-following capabilities\\nof pre-trained language models by bootstrapping their own\\ngenerations. Their pipeline generates instructions, input, and\\noutput samples from a language model, then filters invalid or\\nsimilar ones before using them to fine tune the original model.\\nG. Alignment\\nAI Alignment is the process of steering AI systems towards\\nhuman goals, preferences, and principles. LLMs, pre-trained\\nfor word prediction, often exhibit unintended behaviors. For\\nexample, they might generate contents that are toxic, harmful,\\nmisleading and biased.\\nInstruction tuning, discussed above, gets LLMs a step\\ncloser to being aligned. However, in many cases, it is important\\nto include further steps to improve the alignment of the model\\nand avoid unintended behaviors3. We review the most popular\\n3According to very recent research by Ethayarajh et al. [136], further\\nalignment besides SFT mainly improves models of at least 7B parameters.\\nFor smaller models, SFT is sufficient.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 17}, page_content='approaches to alignment in this subsection.\\nRLHF (reinforcement learning from human feedback) and\\nRLAIF (reinforcement learning from AI feedback) are two\\npopular approaches. RLHF uses a reward model to learn\\nalignment from human feedback. This reward model, after\\nbeing tuned, is able to rate different outputs and score them\\naccording to their alignment preferences given by humans. The\\nreward model gives feedback to the original LLM and this\\nfeedback is used to tune the LLM further [137]. Reinforcement\\nlearning from AI feedback on the other hand, directly connects\\na pretrained and well-aligned model to the LLM and helps it\\nto learn from larger and more aligned models [138].\\nIn another recent work (known as DPO ) [139], Rafailov\\net al. discussed that RLHF is a complex and often unstable\\nprocedure, and tried to address this with a new approach. They\\nleveraged a mapping between reward functions and optimal\\npolicies to show that this constrained reward maximization\\nproblem can be optimized exactly with a single stage of policy\\ntraining, essentially solving a classification problem on the\\nhuman preference data. The resulting algorithm, which they\\ncalled Direct Preference Optimization (DPO), is stable, per-\\nformant, and computationally lightweight, eliminating the need\\nfor fitting a reward model, sampling from the LM during fine-\\ntuning, or performing significant hyperparameter tuning. They\\nobserved that fine-tuning with DPO exceeds RLHF‚Äôs ability to\\ncontrol sentiment of generations and improves response quality\\nin summarization. Fig 30 shows the high-level comparison\\nbetween DPO vs RLHF.\\nFig. 30: DPO optimizes for human preferences while avoiding\\nreinforcement learning. Existing methods for fine-tuning lan-\\nguage models with human feedback first fit a reward model\\nto a dataset of prompts and human preferences over pairs of\\nresponses, and then use RL to find a policy that maximizes\\nthe learned reward. In contrast, DPO directly optimizes for\\nthe policy best satisfying the preferences with a simple classi-\\nfication objective, without an explicit reward function or RL.\\nCourtesy of [139].\\nEven more recently Ethayarajh et al. proposed a new align-\\nment approach called the Kahneman-Tversky Optimization\\n(KTO) [136]. Unlike existing state-of-the-art approaches, KTO\\ndoes not require paired preference data ( x,yw,yl), and it\\nonly needs (x,y) and knowledge of whether yis desirable or\\nundesirable. KTO-aligned models are shown to be good or\\nbetter than DPO-aligned models at scales from 1B to 30B,\\ndespite not using paired preferences. KTO is also far easier to\\nuse in the real world than preference optimization methods, as\\nthe kind of data it needs is far more abundant. As an example,\\nevery retail company has a lot of customer interaction data and\\nwhether that interaction was successful (e.g., purchase made)\\nor unsuccessful (e.g., no purchase made). However, They have\\nlittle to no counterfactual data (i.e., what would have made\\nan unsuccessful customer interaction ylinto a successful oneyw). Fig 31 shows a high-level comparison between KTO and\\nother alignment approaches discussed above.\\nFig. 31: LLM alignment involves supervised finetuning fol-\\nlowed by optimizing a human-centered loss (HALO). How-\\never, the paired preferences that existing approaches need are\\nhard-to-obtain. In contrast, KTO uses a far more abundant\\nkind of data, making it much easier to use in the real world.\\nCourtesy of [136].\\nH. Decoding Strategies\\nDecoding refers to the process of text generation using pre-\\ntrained LLMs. Given an input prompt, the tokenizer translates\\neach token in the input text into a corresponding token ID.\\nThen, the language model uses these token IDs as input and\\npredicts the next most likely token (or a sequence of tokens).\\nFinally, the model generates logits, which are converted to\\nprobabilities using a softmax function. Different decoding\\nstrategies have been proposed. Some of the most popular ones\\nare greedy search, beam search, as well as different sample\\ntechniques such as top-K, top-P (Nucleus sampling).\\n1)Greedy Search :Greedy search takes the most probable\\ntoken at each step as the next token in the sequence, discarding\\nall other potential options. As you can imagine, this is a simple\\napproach and can loose a lot of temporal consistency and\\ncoherency. It only considers the most probable token at each\\nstep, without considering the overall effect on the sequence.\\nThis property makes it fast, but it also means that it can miss\\nout on better sequences that might have appeared with slightly\\nless probable next tokens.\\n2)Beam Search :Unlike greedy search that only considers\\nthe next most probable token, beam search takes into account\\ntheNmost likely tokens, where Ndenotes the number of\\nbeams. This procedure is repeated until a predefined maxi-\\nmum sequence length is reached or an end-of-sequence token\\nappears. At this point, the sequence of tokens (AKA ‚Äúbeam‚Äù)\\nwith the highest overall score is chosen as the output. For\\nexample for beam size of 2 and maximum length of 5,\\nthe beam search needs to keep track of 25= 32 possible\\nsequences. So it is more computationally intensive than greedy\\nsearch.\\n3)Top-k Sampling :Top-k sampling is a technique that\\nuses the probability distribution generated by the language\\nmodel to select a token randomly from the k most likely\\noptions.\\nSuppose we have 6 tokens (A, B, C, D, E, F) and k=2,\\nand P(A)= 30%, and P(B)= 20%, P(C)= P(D)= P(E)= P(F)='),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 18}, page_content='12.5%. In top-k sampling, tokens C, D, E, F are disregarded,\\nand the model outputs A 60% of the time, and B, 40% of\\nthe time. This approach ensures that we prioritize the most\\nprobable tokens while introducing an element of randomness\\nin the selection process.\\nThe randomness is usually introduced via the concept of\\ntemperature. The temperature T is a parameter that ranges from\\n0 to 1, which affects the probabilities generated by the softmax\\nfunction, making the most likely tokens more influential. In\\npractice, it simply consists of dividing the input logits by\\ntemperature value:\\nsoftmax (xi) =exi/T\\nP\\njexj/T(3)\\nA low temperature setting significantly alters the proba-\\nbility distribution (and is commonly used in text generation\\nto control the level of ‚Äúcreativity‚Äù in the generated output),\\nwhile a large temperature prioritizes the tokens with higher\\nprobabilities. Top-k is a creative way of sampling, and can be\\nused along with beam search. The sequence chosen by top-\\nk sampling may not be the sequence with highest probability\\nin beam search. But it‚Äôs important to remember that highest\\nscores do not always lead to more realistic or meaningful\\nsequences.\\n4)Top-p Sampling :Top-p sampling, also known as Nu-\\ncleus sampling, takes a slightly different approach from top-k\\nsampling. Instead of selecting the top k most probable tokens,\\nnucleus sampling chooses a cutoff value p such that the sum of\\nthe probabilities of the selected tokens exceeds p. This forms\\na ‚Äúnucleus‚Äù of tokens from which to randomly choose the next\\ntoken. In other words, in top-p sampling the language model\\nexamines the most probable tokens in descending order and\\nkeeps adding them to the list until the sum of probabilities\\nsurpasses the threshold p. As you can imagine, this could be\\nbetter specially for scenarios in which top-k tokens do not have\\na large probability mass. Unlike top-k sampling, the number\\nof tokens included in the nucleus sampling is not fixed. This\\nvariability often results in a more diverse and creative output,\\nmaking nucleus sampling popular for text generation related\\ntasks.\\nI. Cost-Effective Training/Inference/Adaptation/Compression\\nIn this part, we review some of the popular approaches\\nused for more cost-friendly (and compute-friendly) training\\nand usage of LLMs.\\n1)Optimized Training :There are many frameworks de-\\nveloped for optimized training of LLMs, here we introduce\\nsome of the prominent ones.\\nZeRO: In [140], Rajbhandari et al. developed a novel\\nsolution, Zero Redundancy Optimizer (ZeRO), to optimize\\nmemory, vastly improving training speed of LLMs while\\nincreasing the model size that can be efficiently trained. ZeRO\\neliminates memory redundancies in data- and model-parallel\\ntraining while retaining low communication volume and high\\ncomputational granularity, allowing one to scale the model\\nsize proportional to the number of devices with sustained high\\nefficiency.RWKV: In [141], Peng et al. proposed a novel model\\narchitecture, Receptance Weighted Key Value (RWKV), that\\ncombines the efficient parallelizable training of Transformers\\nwith the efficient inference of RNNs. Their approach leverages\\na linear attention mechanism and allows them to formulate the\\nmodel as either a Transformer or an RNN, which parallelizes\\ncomputations during training and maintains constant compu-\\ntational and memory complexity during inference, leading to\\nthe first non-transformer architecture to be scaled to tens of\\nbillions of parameters. RWKV architecture is shown in Fig\\n32. The Time Complexity comparison of RWKV with different\\nFig. 32: RWKV architecture. Courtesy of [141].\\nTransformers are provided in Fig 33.\\nFig. 33: Time Complexity comparison of RWKV with different\\nTransformers. Here T denotes the sequence length, d the\\nfeature dimension, and c is MEGA‚Äôs chunk size of quadratic\\nattention. Courtesy of [141].\\n2)Low-Rank Adaption (LoRA) :Low-Rank Adaptation is\\na popular and lightweight training technique that significantly\\nreduces the number of trainable parameters, and is based\\non a crucial insight that the difference between the fine-\\ntuned weights for a specialized task and the initial pre-trained\\nweights often exhibits ‚Äúlow intrinsic rank‚Äù - meaning that\\nit can be approximated well by a low rank matrix [142].'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 19}, page_content='Fig. 34: An illustration of LoRA reparametrizan. Only Aand\\nBtrained during this process. Courtesy of [142].\\nTraining with LoRA is much faster, memory-efficient, and\\nproduces smaller model weights (a few hundred MBs), that are\\neasier to store and share. One property of low-rank matrices\\nis that they can be represented as the product of two smaller\\nmatrices. This realization leads to the hypothesis that this delta\\nbetween fine-tuned weights and initial pre-trained weights can\\nbe represented as the matrix product of two much smaller\\nmatrices. By focusing on updating these two smaller matrices\\nrather than the entire original weight matrix, computational\\nefficiency can be substantially improved.\\nSpecifically, for a pre-trained weight matrix W0‚ààRd√ók,\\nLoRA constrains its update by representing the latter with\\na low-rank decomposition W0+ ‚àÜW=W0+BA, where\\nB‚ààRd√ór,A‚ààRr√ók, and the rank r‚â™min(d, k). During\\ntraining, W0is frozen and does not receive gradient updates,\\nwhile AandBcontain trainable parameters. It is worth\\nmentioning that both W0and‚àÜW=BA are multiplied with\\nthe same input, and their respective output vectors are summed\\ncoordinate-wise. For h=W0x, their modified forward pass\\nyields: h=W0x+ ‚àÜWx=W0x+BAx . Usually a random\\nGaussian initialization is used for A, and zero initialization\\nforB, so‚àÜW=BA is zero at the beginning of training.\\nThey then scale ‚àÜWx byŒ±r, where Œ±is a constant in r. This\\nreparametrization is illustrated in Figure 34\\nIt is worth mentioning that LoRA can be applied to any a\\nsubset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architec-\\nture, there are four weight matrices in the self-attention module\\n(Wq,Wk,Wv,Wo), and two in the MLP module. Most of\\nthe time, LoRA is focused on adapting the attention weights\\nonly for downstream tasks, and freezes the MLP modules, so\\nthey are not trained in downstream tasks both for simplicity\\nand parameter-efficiency.\\n3)Knowledge Distillation :Knowledge distillation is the\\nprocess of learning from a larger model [143]. Earlier days of\\nbest-performing models release have proven that this approach\\nis very useful even if it is used in an API distillation approach.\\nIt is also referred to as an approach to distill the knowledge of\\nnot a single model but in fact multiple models into a smaller\\none. Creating smaller models by this approach yields smaller\\nmodel sizes that can be used even on edge devices. Knowledge\\ndistillation as shown in Fig 35, illustrates a general setup of\\nthis training scheme.\\nFig. 35: A generic knowledge distillation framework with\\nstudent and teacher (Courtesy of [144]).\\nKnowledge can be transferred by different forms of learn-\\ning: response distillation, feature distillation, and API distilla-\\ntion. Response distillation is concerned only with the outputs\\nof the teacher model and tries to teach the student model\\nhow to exactly or at least similarly perform (in the sense of\\nprediction) as the teacher. Feature distillation not only uses\\nthe last layer but also intermediate layers as well to create a\\nbetter inner representation for the student model. This helps the\\nsmaller model to have a similar representation as the teacher\\nmodel.\\nAPI distillation is the process of using an API (typically\\nfrom an LLM provider such as OpenAI) to train smaller\\nmodels. In the case of LLMs, it is used to train the model\\nfrom the direct output of the larger model which makes it very\\nsimilar to response distillation. Many concerns are raised by\\nthis type of distillation because in cases where the model itself\\nis not openly available, a (usually) paid API is exposed for end\\nusers. On the other hand, while users pay for each call, how to\\nuse the predictions is limited, for example, OpenAI prohibits\\nusage of its API to create LLMs that later will be used to\\ncompete with it. The main value in such case is training data.\\n4)Quantization :deep learning in its core, is a set of\\nmathematical functions applied to matrices, with a specific\\nprecision for model weights. Reducing the precision of the\\nweights can be used to reduce the size of the model and also\\nmake it faster. As an example, Float-32 operations compared\\nto Int-8 operations are slower. This process, which is called\\nquantization, can be applied in different phases. Main ap-\\nproaches for model quantization can be categorized as: post\\ntraining quantization and quantization-aware training. Post-\\ntraining quantization is concerned with quantized trained mod-\\nels in two well-known methods: dynamic and static. Dynamic\\npost-training quantization computes the range of quantization\\non the runtime and is slower compared to static. Quantization-\\naware training adds quantization criteria into training, and\\na quantized model is trained and optimized during training\\nprocess. This approach ensures that the end model will have\\ngood performance and also does not need to be quantized after\\ntraining.\\nIV. H OWLLM SAREUSED AND AUGMENTED\\nOnce the LLMs are trained, we can use them to generate\\ndesired outputs for a variety of tasks. LLMs can be used\\ndirectly through basic prompting. However, in order to exploit\\ntheir full potential or to address some of the shortcomings,'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 20}, page_content='we need to augment the models through some external means.\\nIn this section we first provide a brief overview of the main\\nshortcoming of LLMs, with a deeper look at the issue of\\nhallucination. We then describe how prompting and some aug-\\nmentation approaches can not only address those limitations\\nbut also be used to augment the capabilities of LLMs going\\nas far as turning an LLM into a full-blown AI agent with the\\nability to interface with the external world.\\nA. LLM limitations\\nIt is important to remember that LLMs are trained to predict\\na token. While fine-tuning and alignment improves their per-\\nformance and adds different dimensions to their abilities, there\\nare still some important limitations that come up, particularly\\nif they are used naively. Some of them include the following:\\n‚Ä¢ They don‚Äôt have state/memory. LLMs on their own\\ncannot remember even what was sent to them in the\\nprevious prompt. That is an important limitation for\\nmany of the uses cases that require some form of state.\\n‚Ä¢ They are stochastic/probabilistic. If you send the same\\nprompt to an LLM several times, you are likely to get\\ndifferent responses. While there are parameters, and\\nin particular the temperature, to limit the variability\\nin the response, this is an inherent property of their\\ntraining that can create issues.\\n‚Ä¢ They have stale information and, on their own, don‚Äôt\\nhave access to external data. An LLM on its own does\\nnot even know about the current time or day and does\\nnot have access to any information that was not present\\nin its training set.\\n‚Ä¢ They are generally very large. This means that many\\ncostly GPU machines are needed for training and\\nserving. In some cases, largest models have poor\\nSLAs, particularly in terms of latency.\\n‚Ä¢ They hallucinate. LLMs do not have a notion of\\n‚Äùtruth‚Äù and they have usually been trained on a mix\\nof good and bad content. They can produce very\\nplausible but untruthful answers.\\nWhile the previous limitations can all become important\\nfor some applications, it is worth for us to dive a bit into the\\nlast one, hallucinations, since it has gathered a lot of interest\\nover the past few months and it has also sparked many of the\\nprompt approaches and LLM augmentation methods we will\\nlater describe.\\nHallucination: In the realm of Large Language Models\\n(LLMs), the phenomenon of ‚Äùhallucinations‚Äù has garnered\\nsignificant attention. Defined in the literature, notably in the\\n‚ÄùSurvey of Hallucination in Natural Language Generation‚Äù\\npaper [145], hallucination in an LLM is characterized as\\n‚Äùthe generation of content that is nonsensical or unfaithful\\nto the provided source.‚Äù This terminology, although rooted in\\npsychological parlance, has been appropriated within the field\\nof artificial intelligence.\\nHallucinations in LLMs can be broadly categorized into\\ntwo types:1) Intrinsic Hallucinations : These directly conflict with\\nthe source material, introducing factual inaccuracies\\nor logical inconsistencies.\\n2) Extrinsic Hallucinations : These, while not contra-\\ndicting, are unverifiable against the source, encom-\\npassing speculative or unconfirmable elements.\\nThe definition of ‚Äôsource‚Äô in LLM contexts varies with the\\ntask. In dialogue-based tasks, it refers to ‚Äôworld knowledge‚Äô,\\nwhereas in text summarization, it pertains to the input text\\nitself. This distinction plays a crucial role in evaluating and\\ninterpreting hallucinations. The impact of hallucinations is also\\nhighly context-dependent. For instance, in creative endeavors\\nlike poem writing, hallucinations might be deemed acceptable\\nor even beneficial.\\nLLMs, trained on diverse datasets including the internet,\\nbooks, and Wikipedia, generate text based on probabilistic\\nmodels without an inherent understanding of truth or falsity.\\nRecent advancements like instruct tuning and Reinforcement\\nLearning from Human Feedback (RLHF) have attempted to\\nsteer LLMs towards more factual outputs, but the fundamental\\nprobabilistic nature and its inherent limitations remain. A\\nrecent study, ‚ÄúSources of Hallucination by Large Language\\nModels on Inference Tasks‚Äù [146], highlights two key aspects\\ncontributing to hallucinations in LLMs: the veracity prior and\\nthe relative frequency heuristic, underscoring the complexities\\ninherent in LLM training and output generation.\\nEffective automated measurement of hallucinations in\\nLLMs requires a combination of statistical and model-based\\nmetrics.\\nStatistical Metrics :\\n‚Ä¢ Metrics like ROUGE [147] and BLEU [148] are com-\\nmon for assessing text similarity, focusing on intrinsic\\nhallucinations.\\n‚Ä¢ Advanced metrics such as PARENT [149], PARENT-\\nT [150], and Knowledge F1 [151] are utilized when\\nstructured knowledge sources are available. These\\nmetrics, while effective, have limitations in capturing\\nsyntactic and semantic nuances.\\nModel-Based Metrics :\\n‚Ä¢ IE-Based Metrics : Utilize Information Extraction\\nmodels to simplify knowledge into relational tuples,\\nthen compare these with the source.\\n‚Ä¢ QA-Based Metrics : Assess the overlap between gen-\\nerated content and the source through a question-\\nanswering framework (see [152]).\\n‚Ä¢ NLI-Based Metrics : Use Natural Language Inference\\ndatasets to evaluate the truthfulness of a generated\\nhypothesis based on a given premise (see [153]).\\n‚Ä¢ Faithfulness Classification Metrics : Offer a refined\\nassessment by creating task-specific datasets for a\\nnuanced evaluation (see [154]).\\nDespite advances in automated metrics, human judgment\\nremains a vital piece. It typically involves two methodologies:'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 21}, page_content='B) Augmenting LLMs through\\nexternal knowledge - RAG\\nHow LLMs Are Used and Augmented\\nC) Using External T ools\\nD) LLM AgentsFunctionality of an LLM-based agent\\nTool Access and Utilization\\nDecision MakingPrompt engineering techniques for agents\\nReasoning without Observation\\nReason and Act\\nDialog-Enabled Resolving Agentsa) RAG-aware prompting techniques\\na) Tool-aware prompting techniques\\nA) LLM limitationsHallucination\\nHallucination QuantificationAutomated metrics\\nHuman judgmentStatistical Metrics\\nModel-Based Metrics\\nScoring\\nComparative AnalysisIE-Based Metrics\\nQA-Based Metrics\\nNLI-Based Metrics\\nB) Using LLMs Prompt Design and Engineering\\n1) Chain of Thought\\nZero-Shot CoT\\nManual CoT5) Expert Prompting\\n6) Chains2) Tree of Thought 7) Rails\\nTopical Rails\\nFact-Checking Rails\\nJailbreaking Rails8) Automatic Prompt Engineering\\nPrompt Generation\\nPrompt Scoring\\nRefinement and Iteration3) Self-Consistency\\n4) Reflection\\nComponents of a RAG\\nRetrieval \\nGeneration \\nAugmentationRAG T ools\\nLangChain \\nLlamaIndex\\nHayStackMeltano\\nCohere Coral\\nFlowise AIFig. 36: How LLMs Are Used and Augmented.\\n1) Scoring : Human evaluators rate the level of halluci-\\nnation within a predefined scale.\\n2) Comparative Analysis : Evaluators compare gener-\\nated content against baseline or ground-truth refer-\\nences, adding an essential layer of subjective assess-\\nment.\\nFactScore [155] is a recent example of a metric that can be\\nused both for human and model-based evaluation. The metric\\nbreaks an LLM generation into ‚Äúatomic facts‚Äù. The final score\\nis computed as the sum of the accuracy of each atomic fact,\\ngiving each of them equal weight. Accuracy is a binary number\\nthat simply states whether the atomic fact is supported by the\\nsource. The authors implement different automation strategies\\nthat use LLMs to estimate this metric.\\nFinally, mitigating hallucinations in LLMs is a multifaceted\\nchallenge, requiring tailored strategies to suit various applica-\\ntions. Those include:\\n‚Ä¢ Product Design and User Interaction Strategies such\\nas use case design, structuring the input/output, or\\nproviding mechanisms for user feedback.\\n‚Ä¢ Data Management and Continuous Improvement.Maintaining and analyzing a tracking set of hallucina-\\ntions is essential for ongoing model improvement.\\n‚Ä¢ Prompt Engineering and Metaprompt Design. Many\\nof the advanced prompt techniques described in IV-B\\nsuch as Retrieval Augmented Generation directly ad-\\ndress hallucination risks.\\n‚Ä¢ Model Selection and Configuration for Hallucination\\nMitigation. For exemple, larger models with lower\\ntemperature settings usually perform better. Also,\\ntechniques such as RLHF or domain-sepcific fine-\\ntuning can mitigate hallucination risks.\\nB. Using LLMs: Prompt Design and Engineering\\nA prompt in generative AI models is the textual input\\nprovided by users to guide the model‚Äôs output. This could\\nrange from simple questions to detailed descriptions or specific\\ntasks. Prompts generally consist of instructions, questions,\\ninput data, and examples. In practice, to elicit a desired\\nresponse from an AI model, a prompt must contain either\\ninstructions or questions, with other elements being optional.\\nAdvanced prompts involve more complex structures, such as'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 22}, page_content='‚Äùchain of thought‚Äù prompting, where the model is guided to\\nfollow a logical reasoning process to arrive at an answer.\\nPrompt engineering is a rapidly evolving discipline that\\nshapes the interactions and outputs of LLMs and other gen-\\nerative AI models. The essence of prompt engineering lies in\\ncrafting the optimal prompt to achieve a specific goal with\\na generative model. This process is not only about instructing\\nthe model but also involves some understanding of the model‚Äôs\\ncapabilities and limitations, and the context within which it\\noperates.\\nPrompt engineering transcends the mere construction of\\nprompts; it requires a blend of domain knowledge, understand-\\ning of the AI model, and a methodical approach to tailor\\nprompts for different contexts. This might involve creating\\ntemplates that can be programmatically modified based on a\\ngiven dataset or context. For example, generating personalized\\nresponses based on user data might use a template that is\\ndynamically filled with relevant user information.\\nFurthermore, prompt engineering is an iterative and ex-\\nploratory process, akin to traditional machine learning prac-\\ntices such as model evaluation or hyperparameter tuning. The\\nrapid growth of this field suggests its potential to revolutionize\\ncertain aspects of machine learning, moving beyond traditional\\nmethods like feature or architecture engineering. On the other\\nhand, traditional engineering practices such as version con-\\ntrol and regression testing need to be adapted to this new\\nparadigm just like they were adapted to other machine learning\\napproaches [156].\\nIn the following paragraphs we detail some of the most\\ninteresting and popular prompt engineering approaches.\\n1) Chain of Thought (CoT): The Chain of Thought (CoT)\\ntechnique, initially described in the paper ‚ÄúChain-of-Thought\\nPrompting Elicits Reasoning in Large Language Models‚Äù[34]\\nby Google researchers, represents a pivotal advancement in\\nprompt engineering for Large Language Models (LLMs).\\nThis approach hinges on the understanding that LLMs, while\\nproficient in token prediction, are not inherently designed for\\nexplicit reasoning. CoT addresses this by guiding the model\\nthrough essential reasoning steps.\\nCoT is based on making the implicit reasoning process of\\nLLMs explicit. By outlining the steps required for reasoning,\\nthe model is directed closer to a logical and reasoned output,\\nespecially in scenarios demanding more than simple informa-\\ntion retrieval or pattern recognition.\\nCoT prompting manifests in two primary forms:\\n1) Zero-Shot CoT: This form involves instructing the\\nLLM to ‚Äúthink step by step‚Äù, prompting it to de-\\nconstruct the problem and articulate each stage of\\nreasoning.\\n2) Manual CoT: A more complex variant, it requires\\nproviding step-by-step reasoning examples as tem-\\nplates for the model. While yielding more effective\\nresults, it poses challenges in scalability and mainte-\\nnance.\\nManual CoT is more effective than zero-shot. However,\\nthe effectiveness of this example-based CoT depends on the\\nchoice of diverse examples, and constructing prompts withsuch examples of step by step reasoning by hand is hard and\\nerror prone. That is where automatic CoT [157] comes into\\nplay.\\n2) Tree of Thought (ToT): The Tree of Thought (ToT)\\n[158] prompting technique is inspired by the concept of\\nconsidering various alternative solutions or thought processes\\nbefore converging on the most plausible one. ToT is based\\non the idea of branching out into multiple ‚Äùthought trees‚Äù\\nwhere each branch represents a different line of reasoning.\\nThis method allows the LLM to explore various possibilities\\nand hypotheses, much like human cognitive processes where\\nmultiple scenarios are considered before determining the most\\nlikely one.\\nA critical aspect of ToT is the evaluation of these reasoning\\npaths. As the LLM generates different branches of thought,\\neach is assessed for its validity and relevance to the query.\\nThis process involves real-time analysis and comparison of\\nthe branches, leading to a selection of the most coherent and\\nlogical outcome.\\nToT is particularly useful in complex problem-solving\\nscenarios where a single line of reasoning might not suffice.\\nIt allows LLMs to mimic a more human-like problem-solving\\napproach, considering a range of possibilities before arriving\\nat a conclusion. This technique enhances the model‚Äôs ability\\nto handle ambiguity, complexity, and nuanced tasks, making it\\na valuable tool in advanced AI applications.\\n3) Self-Consistency: Self-Consistency [159] utilizes an\\nensemble-based method, where the LLM is prompted to gen-\\nerate multiple responses to the same query. The consistency\\namong these responses serves as an indicator of their accuracy\\nand reliability.\\nThe Self-Consistency approach is grounded in the principle\\nthat if an LLM generates multiple, similar responses to the\\nsame prompt, it is more likely that the response is accurate.\\nThis method involves asking the LLM to tackle a query mul-\\ntiple times, each time analyzing the response for consistency.\\nThis technique is especially useful in scenarios where factual\\naccuracy and precision are paramount.\\nThe consistency of responses can be measured using vari-\\nous methods. One common approach is to analyze the overlap\\nin the content of the responses. Other methods may include\\ncomparing the semantic similarity of responses or employing\\nmore sophisticated techniques like BERT-scores or n-gram\\noverlaps. These measures help in quantifying the level of\\nagreement among the responses generated by the LLM.\\nSelf-Consistency has significant applications in fields\\nwhere the veracity of information is critical. It is particularly\\nrelevant in scenarios like fact-checking, where ensuring the\\naccuracy of information provided by AI models is essential.\\nBy employing this technique, prompt engineers can enhance\\nthe trustworthiness of LLMs, making them more reliable for\\ntasks that require high levels of factual accuracy.\\n4) Reflection: Reflection [160] involves prompting LLMs\\nto assess and potentially revise their own outputs based on\\nreasoning about the correctness and coherence of their re-\\nsponses. The concept of Reflection centers on the ability of\\nLLMs to engage in a form of self-evaluation. After generating'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 23}, page_content='an initial response, the model is prompted to reflect on its\\nown output, considering factors like factual accuracy, logical\\nconsistency, and relevance. This introspective process can lead\\nto the generation of revised or improved responses.\\nA key aspect of Reflection is the LLM‚Äôs capacity for\\nself-editing. By evaluating its initial response, the model can\\nidentify potential errors or areas of improvement. This iterative\\nprocess of generation, reflection, and revision enables the LLM\\nto refine its output, enhancing the overall quality and reliability\\nof its responses.\\n5) Expert Prompting: Expert Prompting [161] enhances the\\ncapabilities of Large Language Models (LLMs) by simulating\\nthe responses of experts in various fields. This method involves\\nprompting the LLMs to assume the role of an expert and re-\\nspond accordingly, providing high-quality, informed answers.\\nA key strategy within Expert Prompting is the multi-expert\\napproach. The LLM is prompted to consider responses from\\nmultiple expert perspectives, which are then synthesized to\\nform a comprehensive and well-rounded answer. This tech-\\nnique not only enhances the depth of the response but also\\nincorporates a range of viewpoints, reflecting a more holistic\\nunderstanding of the subject matter.\\n6) Chains: Chains refer to the method of linking multiple\\ncomponents in a sequence to handle complex tasks with Large\\nLanguage Models (LLMs). This approach involves creating a\\nseries of interconnected steps or processes, each contributing\\nto the final outcome. The concept of Chains is based on\\nthe idea of constructing a workflow where different stages\\nor components are sequentially arranged. Each component in\\na Chain performs a specific function, and the output of one\\nserves as the input for the next. This end-to-end arrangement\\nallows for more complex and nuanced processing, as each\\nstage can be tailored to handle a specific aspect of the task.\\nChains can vary in complexity and structure, depending on\\nthe requirements. In ‚ÄúPromptChainer: Chaining Large Lan-\\nguage Model Prompts through Visual Programming‚Äù [162],\\nthe authors not only describe the main challenges in designing\\nchains, but also describe a visual tool to support those tasks.\\n7) Rails: Rails in advanced prompt engineering refer to\\na method of guiding and controlling the output of Large\\nLanguage Models (LLMs) through predefined rules or tem-\\nplates. This approach is designed to ensure that the model‚Äôs\\nresponses adhere to certain standards or criteria, enhancing the\\nrelevance, safety, and accuracy of the output. The concept of\\nRails involves setting up a framework or a set of guidelines\\nthat the LLM must follow while generating responses. These\\nguidelines are typically defined using a modeling language or\\ntemplates known as Canonical Forms, which standardize the\\nway natural language sentences are structured and delivered.\\nRails can be designed for various purposes, depending on\\nthe specific needs of the application:\\n‚Ä¢ Topical Rails: Ensure that the LLM sticks to a\\nparticular topic or domain.\\n‚Ä¢ Fact-Checking Rails: Aimed at minimizing the gen-\\neration of false or misleading information.\\n‚Ä¢ Jailbreaking Rails: Prevent the LLM from generating\\nresponses that attempt to bypass its own operational\\nconstraints or guidelines.8) Automatic Prompt Engineering (APE): Automatic\\nPrompt Engineering (APE) [163] focuses on automating the\\nprocess of prompt creation for Large Language Models\\n(LLMs). APE seeks to streamline and optimize the prompt\\ndesign process, leveraging the capabilities of LLMs themselves\\nto generate and evaluate prompts. APE involves using LLMs\\nin a self-referential manner where the model is employed\\nto generate, score, and refine prompts. This recursive use of\\nLLMs enables the creation of high-quality prompts that are\\nmore likely to elicit the desired response or outcome.\\nThe methodology of APE can be broken down into several\\nkey steps:\\n‚Ä¢ Prompt Generation: The LLM generates a range of\\npotential prompts based on a given task or objective.\\n‚Ä¢ Prompt Scoring: Each generated prompt is then\\nevaluated for its effectiveness, often using criteria\\nlike clarity, specificity, and likelihood of eliciting the\\ndesired response.\\n‚Ä¢ Refinement and Iteration: Based on these evalua-\\ntions, prompts can be refined and iterated upon, further\\nenhancing their quality and effectiveness.\\nC.Augmenting LLMs through external knowledge - RAG\\nOne of the main limitations of pre-trained LLMs is their\\nlack of up-to-date knowledge or access to private or use-\\ncase-specific information. This is where retrieval augmented\\ngeneration (RAG) comes into the picture [164]. RAG, illus-\\ntrated in figure 37, involves extracting a query from the input\\nprompt and using that query to retrieve relevant information\\nfrom an external knowledge source (e.g. a search engine or a\\nknowledge graph, see figure 38 ). The relevant information is\\nthen added to the original prompt and fed to the LLM in order\\nfor the model to generate the final response. A RAG system\\nincludes three important components: Retrieval, Generation,\\nAugmentation [165].\\na) RAG-aware prompting techniques: Because of the\\nimportance of RAG to build advanced LLM systems, several\\nRAG-aware prompting techniques have been developed re-\\ncently. One such technique is Forward-looking Active Retrieval\\nAugmented Generation (FLARE)\\nForward-looking Active Retrieval Augmented Generation\\n(FLARE) [168] enhances the capabilities of Large Language\\nModels (LLMs) by iteratively combining prediction and in-\\nformation retrieval. FLARE represents an evolution in the\\nuse of retrieval-augmented generation, aimed at improving the\\naccuracy and relevance of LLM responses.\\nFLARE involves an iterative process where the LLM\\nactively predicts upcoming content and uses these predictions\\nas queries to retrieve relevant information. This method con-\\ntrasts with traditional retrieval-augmented models that typically\\nretrieve information once and then proceed with generation. In\\nFLARE, this process is dynamic and ongoing throughout the\\ngeneration phase. In FLARE, each sentence or segment gener-\\nated by the LLM is evaluated for confidence. If the confidence\\nlevel is below a certain threshold, the model uses the generated\\ncontent as a query to retrieve relevant information, which is\\nthen used to regenerate or refine the sentence. This iterative'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 24}, page_content='Fig. 37: An example of synthesizing RAG with LLMs for question answering application [166].\\nFig. 38: This is one example of synthesizing the KG as a\\nretriever with LLMs [167].\\nprocess ensures that each part of the response is informed by\\nthe most relevant and current information available.\\nFor more details on RAG framework and its relevant works,\\nwe refer the readers to this survey of retrieval augmented\\ngenerations [165].\\nD.Using External Tools\\nRetrieving information from an external knowledge source\\nas described above is only one of the potential ways to augment\\nan LLM. More generally, an LLM can access any number\\nof external tools (e.g. an API to a service) to augment its\\nfunctionality. In that regards, RAG can be seen as a specific\\ninstance of the broader category of the so called ‚Äùtools‚Äù.\\nTools in this context are external functions or services that\\nLLMs can utilize. These tools extend the range of tasks an\\nLLM can perform, from basic information retrieval to complex\\ninteractions with external databases or APIs.\\nIn the paper ‚ÄùToolformer: Language Models Can Teach\\nThemselves to Use Tools‚Äù [169], the authors go beyond simple\\ntool usage by training an LLM to decide what tool to use\\nwhen, and even what parameters the API needs. Tools include\\ntwo different search engines, or a calculator. In the followingexamples, the LLM decides to call an external Q&A tool,\\na calculator, and a Wikipedia Search Engine More recently,\\nresearchers at Berkeley have trained a new LLM called Gorilla\\n[67] that beats GPT-4 at the use of APIs, a specific but quite\\ngeneral tool.\\na) Tool-aware prompting techniques: Similarly to what\\nwas described with RAG, several tool-aware prompting ap-\\nproaches have been developed to make usage of tools more\\nscalable. A popular technique is the so called Automatic Multi-\\nstep Reasoning and Tool-use (ART).\\nAutomatic Multi-step Reasoning and Tool-use (ART) [170]\\nis a prompt engineering technique that combines automated\\nchain of thought prompting with the use of external tools.\\nART represents a convergence of multiple prompt engineering\\nstrategies, enhancing the ability of Large Language Models\\n(LLMs) to handle complex tasks that require both reasoning\\nand interaction with external data sources or tools.\\nART involves a systematic approach where, given a task\\nand input, the system first identifies similar tasks from a task\\nlibrary. These tasks are then used as examples in the prompt,\\nguiding the LLM on how to approach and execute the current\\ntask. This method is particularly effective when tasks require a\\ncombination of internal reasoning and external data processing\\nor retrieval.\\nE.LLM Agents\\nThe idea of AI agents has been well-explored in the history\\nof AI. An agent is typically an autonomous entity that can\\nperceive the environment using its sensors, make a judgment\\nbased on the state it currently is, and accordingly act based on\\nthe actions that are available to it.\\nIn the context of LLMs, an agent refers to a system based\\non a specialized instantiation of an (augmented) LLM that\\nis capable of performing specific tasks autonomously. These\\nagents are designed to interact with users and environment to\\nmake decisions based on the input and the intended goal of\\nthe interaction. Agents are based on LLMs equipped with the'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 25}, page_content='ability to access and use tools, and to make decisions based on\\nthe given input. They are designed to handle tasks that require\\na degree of autonomy and decision-making, typically beyond\\nsimple response generation.\\nThe functionalities of a generic LLM-based agent include:\\n‚Ä¢ Tool Access and Utilization: Agents have the capabil-\\nity to access external tools and services, and to utilize\\nthese resources effectively to accomplish tasks.\\n‚Ä¢ Decision Making: They can make decisions based on\\nthe input, context, and the tools available to them,\\noften employing complex reasoning processes.\\nAs an example, an LLM that has access to a function (or\\nan API) such as weather API, can answer any question related\\nto the weather of the specific place. In other words, it can use\\nAPIs to solve problems. Furthermore, if that LLM has access\\nto an API that allows to make purchases, a purchasing agent\\ncan be built to not only have capabilities to read information\\nfrom the external world, but also act on it [171].\\nFig. 40 shows another example of LLM-based agents for\\nconversational information seeking [36], where an LLM is\\naugmented with a set of plug-and-play modules, including\\naworking memory that tracks the dialog state, a policy that\\nmakes an execution plan for the task and selects next system\\naction, an action executor that performs an action selected by\\nthe policy (consolidating evidence from external knowledge,\\nor prompting the LLM to generate responses), and a utility\\nthat accesses the alignment of the LLM‚Äôs responses with user\\nexpectations or specific business requirements, and generate\\nfeedback to improve agent performance.\\nFor more details on LLM-based AI agents see recent survey\\n[172], [173], [174].\\na) Prompt engineering techniques for agents: Like\\nRAG and Tools, prompt engineering techniques that specif-\\nically address the needs of LLM-based agents have been\\ndeveloped. Three such examples are Reasoning without Ob-\\nservation (ReWOO), Reason and Act (ReAct), and Dialog-\\nEnabled Resolving Agents (DERA).\\nReasoning without Observation (ReWOO) [175] aims to\\ndecouple reasoning from direct observations. ReWOO operates\\nby enabling LLMs to formulate comprehensive reasoning plans\\nor meta-plans without immediate reliance on external data\\nor tools. This approach allows the agent to create a struc-\\ntured framework for reasoning that can be executed once the\\nnecessary data or observations are available. In ReWOO, the\\nLLM initially develops a plan (a series of steps) that outlines\\nhow to approach and solve a given problem. This meta-\\nplanning phase is crucial as it sets the stage for the agent to\\nprocess information once it becomes available. The execution\\nphase then involves integrating actual data or observations into\\nthe pre-specified plan, leading to coherent and contextually\\nrelevant responses. ReWOO offers significant advantages in\\nterms of token efficiency and robustness to tool failure. It\\nenables LLMs to handle tasks where immediate access to\\nexternal data is not available, relying instead on a well-\\nstructured reasoning framework. This method is particularly\\nadvantageous in scenarios where data retrieval is costly, slow,or uncertain, allowing the LLM-based agent to maintain a high\\nlevel of performance and reliability.\\nReason and Act (ReAct)[176] prompts LLMs to generate\\nnot only verbal reasoning but also actionable steps, thus\\nenhancing the model‚Äôs dynamic problem-solving capabilities.\\nReAct is grounded in the principle of integrating reasoning\\nwith action. In this approach, the LLM is prompted to alternate\\nbetween generating reasoning traces (explanations) and taking\\nactions (steps or commands) in an interleaved manner. This\\napproach allows the model to dynamically reason about a prob-\\nlem, and propose and take concrete actions simultaneously.\\nDialog-Enabled Resolving Agents (DERA) [177] are spe-\\ncialized AI agents that can engage in dialogue, resolve queries,\\nand make decisions based on interactive exchanges. DERA\\nis developed based on the idea of utilizing multiple agents\\nwithin a dialog context, each with specific roles and functions.\\nThese agents can include Researchers, who gather and analyze\\ninformation, and Deciders, who make final judgments based\\non the information provided. This division of roles allows for\\na well-organized and efficient approach to problem-solving\\nand decision-making. DERA is particularly advantageous in\\nscenarios requiring complex decision-making and problem-\\nsolving, such as those in medical diagnostics or customer ser-\\nvice. The collaborative and interactive nature of DERA agents\\nallows them to handle intricate queries with a level of depth\\nand nuance that single-agent systems might struggle with.\\nMoreover, this approach aligns well with human decision-\\nmaking processes, making AI reasoning more relatable and\\ntrustworthy.\\nV. P OPULAR DATASETS FOR LLM S\\nLarge language models exhibit promising accomplish-\\nments, but the main question that arises is how effectively\\nthey function and how their performance can be assessed in\\nspecific tasks or applications.\\nThe evaluation of LLMs poses particular challenges due\\nto the evolving landscape of their applications. The original\\nintent behind developing LLMs was to boost the performance\\nof NLP tasks such as translation, summarization, question-\\nanswering, and so on [178]. However, it is evident today\\nthat these models are finding utility across diverse domains\\nincluding code generation and finance. Moreover, the eval-\\nuation of LLMs encompasses several critical considerations\\nsuch as fairness and bias, fact-checking, and reasoning. In\\nthis section, we outline the commonly used benchmarks for\\nassessing LLMs. These benchmarks are categorized based on\\ntraining or evaluating the LLM Capabilities.\\nA. Datasets for Basic Tasks: language model-\\ning/understanding/generation\\nThis section provides an overview of the benchmarks and\\ndatasets suited to evaluate the basic abilities of LLMs.\\n‚Ä¢ Natural Questions [179] is a QA dataset that consists\\nof real anonymized, aggregated queries submitted to\\nthe Google search engine as questions. An annotator\\nis presented with a question along with a Wikipedia\\npage from the top 5search results, and annotates a\\nlong answer (typically a paragraph) and a short answer'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 26}, page_content='Fig. 39: HuggingGPT: An agent-based approach to use tools and planning [image courtesy of [171]]\\nFig. 40: A LLM-based agent for conversational information\\nseeking. Courtesy of [36].\\n(one or more entities) if present on the page, or marks\\nnull if no long/short answer is present.\\n‚Ä¢ MMLU [180] is intended to evaluate the knowl-\\nedge gained in zero-shot and few-shot scenarios. That\\nmeans that MMLU assesses both the general knowl-\\nedge and problem-solving ability of a model. It covers\\n57 subjects in STEM, humanities, social sciences,\\nand other areas. The benchmark varies in complexity,\\nranging from elementary to advanced professional.\\nIt is worth mentioning that the main contribution of\\nthis dataset is for multi-task language understanding,\\nquestion answering, and arithmetic reasoning.\\n‚Ä¢ MBPP [181] stands for ‚ÄúMostly Basic Python Prob-\\nlems‚Äù and provides a benchmark for evaluating the\\nperformance of models designed for code generation.\\nThe benchmark encompasses 974 short Python pro-\\ngrams including a wide range of topics, including\\nfundamental programming concepts and standard li-\\nbrary usage, and more. Each challenge comprises atask description, a code solution, and three automated\\ntest cases.\\n‚Ä¢ HumanEval [182] is a dataset for code generation\\ntask. This dataset consists of 164 hand-crafted pro-\\ngramming challenges. Each challenge is accompanied\\nby a function signature, docstring, code body, and mul-\\ntiple unit tests. The main intuition behind developing\\nthis dataset is to guarantee the exclusion of its contents\\nfrom training datasets for code generation models.\\n‚Ä¢ APPS [183] is designed for code generation task\\nfocusing on the Python programming language. The\\nAPPS dataset contains a collection of 232,444Python\\nprograms. Each program in the dataset has an average\\nof18lines of Python code. Additionally, APPS offers\\naccess to a repository of 10,000unique programming\\nexercises, each with text-based problem descriptions.\\nThe final aspect to highlight is that the it includes test\\ncases.\\n‚Ä¢ WikiSQL [184] is crafted for code generation task and\\nit has 87,726 carefully labeled pairs of SQL queries\\nand corresponding natural language questions from\\nWikipedia tables. The SQL queries comprise three\\nsubsets: test sets ( 17,284 examples), development\\n(9,145examples), and training ( 61,297examples).\\n‚Ä¢ TriviaQA [185] is designed for QA task. This\\ndataset comprises more than 650,000 question-\\nanswer-evidence triples. There are 95,000 question-\\nanswer pairs in this dataset, each authored by trivia en-\\nthusiasts and supported by an average of six indepen-\\ndently sourced evidence documents. These documents\\nare automatically acquired from Wikipedia or broader\\nweb search results. The dataset is categorized into\\ntwo segments, including those with authentic answers\\nfrom Wikipedia and web domains, and verified sets\\nembody the accurately answered questions along with\\ntheir associated documents from both Wikipedia and\\nonline.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 27}, page_content='Fig. 41: Dataset applications.\\n‚Ä¢ RACE [186] suits for reading comprehension task.\\nThis dataset is based on English tests completed by\\nChinese students from middle school and high school,\\naged 12to18, and it contains roughly 28,000 texts\\nand100,000questions rigorously prepared by human\\nspecialists, primarily English instructors. This dataset\\ncontains a wide range of subjects that were purpose-\\nfully chosen to assess students‚Äô comprehension and\\nreasoning abilities. This dataset is available in three\\nsubgroups: RACE-M, RACE-H, and RACE. RACE-\\nM refers to the middle school examinations, whereas\\nRACE-H denotes the high school tests. Finally, RACEis the synthesis of RACE-M and RACE-H.\\n‚Ä¢ SQuAD [187] stands for ‚ÄúStanford Question Answer-\\ning Dataset‚Äù and is a crowdsourced reading compre-\\nhension dataset based on Wikipedia articles. It has\\napproximately 100,000 question-answer pairs con-\\nnected to more than 500 articles. The answers to\\nthese questions are typically text fragments or spans\\ntaken from the corresponding reading passages. The\\nquestions may be unanswerable in some cases. The\\ndataset is divided into three sets: an 80% training set,\\na10% development set, and a 10% hidden test set.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 28}, page_content='Fig. 42: Datasets licensed under different licenses.\\n‚Ä¢ BoolQ [188] is a yes/no question-answering dataset\\nwhere the goal is reading comprehension task. BoolQ\\nincludes 15,942 examples. Each example is a triplet\\nthat includes a question, a relevant paragraph, and\\nthe solution. Although the main intuition behind\\nthis dataset is for reading comprehension, it can be\\nused for reasoning, natural language inference, and\\nquestion-answering tasks.\\n‚Ä¢ MultiRC [189] is another dataset that fits reading\\ncomprehension task. MultiRC contains brief para-\\ngraphs as well as multi-sentence questions that can\\nbe answered using the information in the paragraph.\\nThe paragraphs in this dataset come from a variety\\nof sources, including news, fiction, historical texts,\\nWikipedia articles, discussions on society and law,\\nelementary school science textbooks, and 9/11 re-\\nports. Each question has many response choices, with\\none or more of them being correct. Answering the\\nquestions requires reasoning across several sentences.\\nMultiRC dataset encompasses around 6,000 multi-\\nsentence questions gathered from over 800 paragraphs.\\nOn average, each question offers about two valid\\nanswer alternatives out of a total of five.\\nB. Datasets for Emergent: ICL, reasoning (CoT), instruction\\nfollowing\\nThis section centers on the benchmarks and datasets em-\\nployed to evaluate the emergent abilities of LLMs.‚Ä¢ GSM8K [190] is designed to evaluate the model‚Äôs\\nability for multi-step mathematical reasoning. GSM8K\\nincludes 8.5K linguistically diverse grade school math\\nword problems written by humans. The dataset is split\\ninto two sets: a training set with 7.5Kproblems,\\nand a test set with 1K problems. These problems\\nneed 2to8steps to be solved. Solutions mainly\\nare a series of elementary calculations using basic\\narithmetic operations.\\n‚Ä¢ MATH [191] enables to assess how well models can\\nsolve math problems. MATH dataset hast 12,500\\nproblems from high school math competitions. Each\\nproblem in the dataset has a step-by-step solution and\\na final answer enclosed in a box. The problems cover\\na wide range of topics and have different levels of\\ncomplexity. There are seven subjects in total. Further-\\nmore, the difficulty of each problem is rated based\\non the AoPS standards on a scale from‚Ä≤1‚Ä≤to‚Ä≤5‚Ä≤. A\\n‚Ä≤1‚Ä≤shows the easiest problems in a subject, while‚Ä≤5‚Ä≤\\nrepresents the most difficult. In terms of formatting,\\nall problems and solutions are presented using LATEX\\nand the Asymptote vector graphics language.\\n‚Ä¢ HellaSwag [192] is designed to assess commonsense\\nreasoning in LLMs. This benchmark includes 70,000\\nmultiple-choice questions. Each question is derived\\nfrom one of two domains: ActivityNet or WikiHow,\\nand presents four answer choices regarding what\\nmight happen in the following situation. The correct\\nanswer provides an actual statement describing the'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 29}, page_content='upcoming event, but the three wrong answers are\\ncreated to confuse machines.\\n‚Ä¢ AI2 Reasoning Challenge (ARC) [193] is used\\nfor commonsense reasoning. This benchmark encom-\\npasses 7,787 science examination questions. These\\nquestions are in English, and most of them are set\\nup in a multiple-choice format. The questions have\\nbeen divided into two groups: a Challenge Set with\\n2,590difficult questions and an Easy Set with 5,197\\nquestions. Each collection has also been pre-divided\\ninto Train, Development, and Test subsets.\\n‚Ä¢ PIQA [194] is intended to evaluate the language\\nrepresentations on their knowledge of physical com-\\nmonsense. In this dataset, the focus is on everyday\\nsituations with a preference for uncommon solutions.\\nThe central task is a multiple-choice question answer-\\ning, where a question (q)is provided along with two\\npotential solutions (s1, s2). Then, the best solution is\\nchosen by whether a model or a human. For each\\nquestion, only one of the solutions is the correct\\nanswer.\\n‚Ä¢ SIQA [195] provides a framework for evaluating mod-\\nels‚Äô ability for commonsense reasoning about social\\nsituations. SIQA dataset has 38,000 multiple-choice\\nquestions designed to assess emotional and social\\nintelligence in everyday circumstances. This dataset\\ncovers a wide variety of social scenarios. In SIQA,\\nthe potential answers is a mixture of human-selected\\nresponses and machine-generated ones that have been\\nfiltered through adversarial processes.\\n‚Ä¢ OpenBookQA (OBQA) [196] is a new kind of\\nquestion-answering dataset where answering its ques-\\ntions requires additional common and commonsense\\nknowledge not contained in the book and rich text\\ncomprehension. This dataset includes around 6,000\\nmultiple-choice questions. Each question is linked to\\none core fact, as well as an additional collection\\nof over 6000 facts. The questions were developed\\nusing a multi-stage crowdsourcing and expert filter-\\ning procedure. OpenBookQA questions are difficult\\nbecause they need multi-hop reasoning with limited\\nbackground.\\n‚Ä¢ TruthfulQA [197] is designed specifically to eval-\\nuate the truthfulness of language models in gen-\\nerating answers to questions. This dataset includes\\n817 questions, written by authors, from 38different\\ncategories, including health, law, finance, and politics.\\nThese questions are purposefully designed to chal-\\nlenge human responders, as they may contain common\\nmisunderstandings that lead to incorrect answers.\\n‚Ä¢ OPT-IML Bench [103] is a comprehensive bench-\\nmark for Instruction Meta-Learning. It covers 2000\\nNLP tasks from 8 existing benchmarks. The OPT-IML\\nBench consists of a training set with 17.9 M examples,\\na dev set with 145K samples, and a test set with 321K\\nsamples.C. Datasets for Augmented: using external knowledge/tools\\nThis section focuses on datasets designed for the aug-\\nmented abilities of LLMs.\\n‚Ä¢ HotpotQA [198] is designed to cover a diverse and\\nexplainable question-answering dataset that necessi-\\ntates multi-hop reasoning. This dataset is derived from\\nthe English Wikipedia. It consists of roughly 113,000\\nquestions. Each question in the dataset comes with\\ntwo paragraphs, called gold paragraphs, from two\\nWikipedia articles. Also, there is a list of sentences\\nin those paragraphs that crowdworkers have picked as\\nimportant for answering the question.\\n‚Ä¢ ToolQA [199] is a question answering benchmark\\nto evaluate LLMs‚Äô ability to use external tools for\\nanswering questions.\\n‚Ä¢ GPT4Tools serves as an instructional dataset, gener-\\nated by instructing advanced teachers (such as Chat-\\nGPT), with instructions conditioned on visual content\\nand tool descriptions. This process results in the\\ngeneration of instructions related to the use of tools.\\nThere are three versions of this dataset. The first\\nversion comprises 71,000 instruction-following data\\npoints utilized to fine-tune the GPT4Tools model. The\\nnext version consists of manually cleaned instruction\\ndata used for validation, covering instructions related\\nto the tools from the first version. The last version is\\ncleaned instruction data used for testing and includes\\ninstructions related to some tools that are not present\\nin the first version.\\nVI. P ROMINENT LLM S‚Äô PERFORMANCE ON\\nBENCHMARKS\\nIn this section we first provide an overview of some of\\npopular metrics used for evaluating the performance of LLMs\\nunder different scenarios. We then look at the performance\\nof prominent large language models on some of the popular\\ndatasets and benchmarks.\\nA. Popular Metrics for Evaluating LLMs\\nEvaluating the performance of generative language models\\ndepends on the underlying task they are going to be used for.\\nTasks that are mostly about selecting a choice out of given\\nones (such as sentiment analysis), can be seen as simple as\\nclassification and their performance can be evaluated using\\nclassification metrics. Metrics such as accuracy, precision,\\nrecall, F1, etc are applicable in this case. It is also important to\\nnote that the answers generated by the model for specific tasks\\nsuch as multi-choice question answering are always either True\\nor False. If the answer is not in a set of options, it can be seen\\nas False as well.\\nHowever, some tasks that are purely open-ended text gener-\\nation cannot be evaluated in the same way as for categorization.\\nDifferent metrics are required for the specific purpose of the\\nevaluation. Code generation is a very different case in open-\\nended generative evaluations. The generated code must pass\\nthe test suite but on the other hand, it is also important\\nto understand if a model is capable of generating different'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 30}, page_content='TABLE II: LLM Datasets Overview.\\nBenchmark Name Evaluation Metric Leaderboard Source paperswithcode\\nHumanEval PASS@k Link Link Link\\nMBPP PASS@k, Accuracy - Link Link\\nAPPS PASS@k, Accuracy - Link Link\\nWikiSQL Accuracy - Link Link\\nCoNaLa BLEU Link Link\\nCodeParrot PASS@k - Link -\\nHellaSwag Accuracy Link Link Link\\nAI2 Reasoning\\nChallenge (ARC)Accuracy Link Link Link\\nBoolQ Accuracy - Link Link\\nMultiRC F1-score, Accuracy - Link Link\\nCNN/Daily Mail [200] Accuracy - Link -\\nSQuAD F1-score, EM Link Link Link\\nRACE Accuracy - Link Link\\nCNN/Daily Mail [201] ROUGE - Link Link\\nDrop F1-score, EM Link Link Link\\nQuAC F1-score, HEQ-Q, HEQ-D Link Link Link\\nTriviaQA EM, F1-score, Accuracy Link Link Link\\nNatural Questions EM, F1-score, Accuracy Link Link Link\\nStrategyQA Accuracy, Recall@10, SARI Link Link Link\\nCoQA F1-score Link Link Link\\nXSum ROUGE - Link Link\\nSAMSum ROUGE - - Link\\nWikiSum ROUGE - Link -\\nDialogSum ROUGE - Link Link\\nTruthfulQA MC1 , MC2, % true, % info, BLEURT Link Link Link\\nMMLU Accuracy Link Link Link\\nGSM8K Accuracy Link Link Link\\nPIQA Accuracy Link Link Link\\nSIQA Accuracy Link Link Link\\nOpenBookQA (OBQA) Accuracy Link Link Link\\nHotpotQA EM, F1-score, Joint EM, Joint F1-score, Link Link Link\\nMATH Accuracy - Link Link\\nCommonsenseQA Accuracy Link Link Link\\nNatural Instructions ROUGE-L, Human Link Link Link\\nBIG-bench Accuracy, Average - Link Link\\nToolTalkSuccess rate, Precision, Recall, Incorrect\\naction rate, Percent of failing error types- Link Link\\nMetaTool Accuracy, Precision, Recall, F1-score - Link Link\\nGPT4ToolsSuccessful Rate of Thought, Successful\\nRate of Action, Successful Rate of Ar-\\nguments, Success Rate- Link Link\\nAPI-BankCorrectness, ROUGE, Error(API Hallu-\\ncination, Has Exception, Invalid Input\\nParameters, False API Call Format, API\\nCall, Miss Input Parameters)- Link Link\\nAlpaca-CoT - - Link Link\\nsolutions as a code, what is the probability of selecting the\\ncorrect one among them. Pass@k is a very good metric in this\\ncase. It works in this manner that given a problem, different\\nsolutions as code are generated. They are tested for correctness\\nusing different functionality tests. Afterward, from generated\\nn solutions, and the respective c number of them being correct\\nequation 4 provides the final value.\\npass@ k:=E\\nProblems\"\\n1‚àí\\x00n‚àíc\\nk\\x01\\n\\x00n\\nk\\x01#\\n(4)\\nExact match (EM) is another metric that is mostly con-\\ncerned with exact matches from (pre-defined) answers. It\\ncounts a prediction as correct if it exactly matches one of\\nmore than one desired reference text token by token. In some\\ncases, it can be the same as accuracy and the equation 5 shows\\nthe mathematical definition. Here M is total number of correct\\nanswers and N is the total number of questions [202].EM =M\\nN(5)\\nHuman equivalence score (HEQ) on the other hand, is an\\nalternative to F1 score [203]. HEQ-Q represents the precision\\nof individual questions, wherein an answer is deemed correct\\nif the model‚Äôs F1 score surpasses the average human F1 score.\\nLikewise, HEQ-D denotes the precision of each dialogue; it is\\ndeemed accurate when all questions within the dialogue meet\\nthe criteria of HEQ [182].\\nEvaluation of other generative tasks such as machine trans-\\nlation are based on metrics such as Rouge and BLEU. These\\nscores work well when there is a reference text as ground\\ntruth (such as translation) and a hypothesis that is generated\\nby the generative model, in our case the LLM. These scores\\nare mostly used for cases where the goal is to detect the\\nsimilarity of the answer and ground truth in a computation\\nmanner. In a computation manner, it meant that nothing more\\nthan N-Grams would be used. However, metrics such as BERT-\\nScore are also good for these cases but they are also heavily'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 31}, page_content='TABLE III: LLM categories and respective definitions.\\nClassification Category Description\\nSizeSmall Number of parameters ‚â§1B\\nMedium 1B<Number of parameters ‚â§10B\\nLarge 10B<Number of parameters ‚â§100B\\nVery Large 100B<Number of parameters\\nTypeFoundation model Pretrained language model\\nInstruction model Pretrained and instruction fine-tuned language model\\nChat model Pretrained, instruction fine-tuned, and chat fine-tuned language model\\nOriginOriginal model An original model released with either Foundation, Instruction, or Chat model\\nTuned model Fine-tuned version of an original model\\nAvailabilityPublicly available Model and weights are available due to request to without request\\nPublicly unavailable Model and weights are not publicly available\\nTABLE IV: Different LLM categorization.\\nModel Size #Params (B) Type Availability Origin\\nDavinci-002 Very Large 175 Instruction Unavailable Tuned\\nDavinci-003 Very Large 175 Instruction Unavailable Tuned\\nGPT 3.5-turbo Large 20 Chat Unavailable Tuned\\nFalcon 7B Medium 7 Foundation Public Original\\nAlpaca Large 13 Chat Public Tuned\\nPythia 7B Medium 7 Foundation Public Original\\nPythia 12B Large 12 Foundation Public Original\\nLLAMA 7B Medium 7 Chat Public Original\\nLLAMA 2 7B Medium 7 Chat Public Tuned\\nLLAMA 2 7B Medium 7 Foundation Public Original\\nVicuna 13B Large 13 Foundation Public Tuned\\nVicuna 7B Medium 7 Foundation Public Tuned\\nClaude Large 93 Chat Unavailable Original\\nClaude 2 Very Large 137 Chat Unavailable Original\\nerroneous because another model is used to judge. Still, even\\ntoday, evaluating purely generated content is very hard and\\nno completely fitting metric is not found, metrics are either\\nlooking for simplistic features such as N-Gram, SkipGram,\\netc, or they are models with unknown accuracy and preciseness\\n[204].\\nGenerative evaluation metrics are also another type of eval-\\nuation metric for LLMs that use another LLM for evaluating\\nthe answer. However, depending on the task itself, evaluation\\ncan be possible in this way or not. Another dependency\\nthat makes generative evaluation error-prone is reliance on\\nthe prompt itself. RAGAS is one of the good examples that\\nincorporate the usage of generative evaluation.\\nVarious benchmarks and leaderboards have been proposed\\nto address the most challenging question in the world of\\nlarge language models: Which one is better? However not\\na simple answer can address this question. The answer de-\\npends on various aspects of large language models. Section V\\nshows the categorical presentation of different tasks and the\\nmost important datasets in each category. We will follow the\\nsame categorization and provide a comparison based on each\\ncategory. After providing comparison for each category, we\\nwill provide a broad overview of aggregated performance by\\naveraging the reported performance metric on different tasks.\\nEvaluating different LLMs can be seen also from different\\nperspectives. For example, a LLM with a drastically fewer\\nnumber of parameters is not completely comparable to one\\nwith a larger number of parameters. From this perspective, we\\nwill categorize LLMs in four categories as well: small (less\\nthan or equal to 1 billion parameters), medium (between 1 and\\n10 billion), large (between 10 and 100 billion), and very large\\n(more than 100 billion). Another classification for the LLMswe use is their primary use case. We consider each LLM to\\nbe either: Foundation model (pretrained language model with\\nno instruction fine-tuning and chat fine-tuning), Instruction\\nmodel (pretrained language model with only instruction fine-\\ntuning), and Chat model (pretrained language model with\\ninstruction and chat fine-tuning). Apart from all the catego-\\nrization described, another category is required to distinguish\\nbetween original models and tuned ones. Original models are\\nthose that have been released as a foundation model or a fine-\\ntuned one. Tuned models are those that grasped the original\\nmodel and tuned it with different datasets or even different\\ntraining approaches. It is also good to note that original models\\nare usually foundation models that have been fine-tuned on\\nspecific datasets or even different approaches. Availability of\\nthe model weights regardless of the license is another category\\nin our classification. Models that have their weights publicly\\navailable (even through request) are noted as Public models\\nwhile others are noted as Private . Table III shows all of these\\ndefinitions and abbreviations used in the rest of the article.\\nFigure 43 illustrate these visually.\\nAccording to the provided categorizations, we can catego-\\nrize and label each notable LLM as shown in table IV. As can\\nbe seen from this table, models categorized as very large are\\nalso unavailable as well.\\nB. LLMs‚Äô Performance on Different Tasks\\nCommonsense reasoning is one of the important capabili-\\nties each model can obtain. This capability denotes the ability\\nof the model to use prior knowledge in combination with\\nreasoning skills. In the case of HellaSwag for example, finding\\nthe continuation of text is challenging because the given text\\ncontains a partial part of the story while the given choices\\nas continuation are tricky to select, and without having prior'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 32}, page_content='Large\\nLanguage\\nModelsParameters\\nAvailabilityOriginalityTypeSmall LM\\n# of params <1BMedium LM\\n1B < # of params <10BLarge LM\\n10B < # of params <100B\\nVery Large LM\\n100B < # of params\\nTuned\\nFine tuning\\nOriginal\\nPublic PrivateFoundation\\nInstruction\\nChatFine tuned models that are originally\\nbased on original models.\\nExample: Alpaca  (based on LLaMA)\\nOriginal models that are not fine\\ntuned or based on any other\\npretrained model.\\nExample: LLaMA\\nModel weights are publicly released\\nand is available.\\nExample: LLaMAModel weights are NOT  publicly\\nreleased and is NOT  available.\\nExample: GPT -4Pretrained model with no instruction\\nor chat fine-tuning.\\nExample: MPT -7B\\nPretrained model that is\\nalso fine-tuned on\\ninstruction following.\\nExample: MPT -7B-instruct\\nPretrained model that is\\nalso fine-tuned on chat.\\nExample: MPT -7B-chatFig. 43: LLM categorizations.\\nknowledge about the world it is not possible. This specific kind\\nof reasoning deserves high attention because it is related to\\nutilizing previous knowledge with open text-described scenes\\nor facts. As can be seen from table V not just Unavailable\\nmodels but also Public ones can achieve good results on\\nvarious tests.\\nTABLE V: Commonsense reasoning comparison.\\nModel OBQA HellaSwag\\nDavinci-003 51 83.4\\nFalcon 7B 44.4 76.3\\nAlpaca 43.4 73.9\\nPythia 7B 37.2 64\\nPythia 12B 43.2 68.1\\nLLAMA 7B 42.4 73\\nDolly 6B 41.2 67.6\\nDolly 12B 40.4 71\\nAlpaca 7B 43.4 73.9\\nAlpaca Lora 7B 42.6 74\\nGPT-J 6.7B 38.2 66.2\\nLLama 7B 42.4 73\\nLLama 13B 42.2 76.2\\nPythia 6.7B 37.2 64\\nPythia 12B 38 67.3\\nStableLM Tuned 33.4 53.6\\nKoala 13B 42.8 72.6\\nMosaic mpt-7B 42.6 76.3\\nLLAMA 2 70B - 87.33\\nLLAMA 65B - 86.09\\nFalcon 40B - 85.3\\nFalcon 180B - 88.86\\nMPT Instruct 30B - 84.31\\nMPT Instruct 7B - 77.91\\nYi 6B - 76.42\\nYi 34B - 85.69\\nGPT-4 - 95.3\\nGemini Ultra - 87.8From the results presented in Table V it is clear that GPT-4\\nachieves best results for HellaSwag while Davinci-003 is best\\nmodel for OBQA. It is also good to note that results for OBQA\\nare not reported for all of the models and possibly davinci-003\\nis not the best model achieving highest results on OBQA.\\nNot all models report their performance on all datasets, and\\nbecause of that, the number of models for which performance\\nis reported in different tables varies.\\nTABLE VI: Symbolic reasoning comparison.\\nModel Cobjects Penguins\\nGPT-NeoX 26 33.56\\nOPT 66B 31.2 28.08\\nBloomberg GPT 34.8 37.67\\nBLOOM 176B 36.8 40.41\\nPaLM 540B 38 44.5\\nGopher-280B 49.2 40.6\\nChinchilla-70B 59.7 48.7\\nPaLM 2 61.2 65.8\\nWorld knowledge is mostly about general knowledge ques-\\ntions, for example, in Wikifact dataset questions such as ‚ÄùWho\\nis the author of a specific well-known book‚Äù can be found and\\nreferences are also provided. Table VII shows the results.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 33}, page_content='TABLE VII: World knowledge comparison.\\nModel TriviaQA NaturalQ WebQ ARC\\nBLOOM - - - 32.9\\nBLOOM 176B - - - 50.85\\nBloomberg GPT - - - 48.63\\nChinchilla - 35.5 - -\\nCodex + REPLUG 76.8 44.7 - -\\nGAL 120B - - - 67.9\\nGLaM 62B/64E 75.8 32.5 15.5 50.3\\nGopher - 28.2 - -\\nGPT-3 175B 71.2 29.9 41.5 85.2\\nGPT-4 - - - 96.4\\nGPT-NeoX - - - 45.39\\nLLaMA 13B - - - 52.7\\nLLaMA 2 70B 85 33 - -\\nLLaMA 33B - 24.9 - 57.8\\nLLaMA 65B 72.6 39.9 - -\\nLLaMA 7B - - - 47.6\\nMistral 7B 69.9 28.8 - 55.5\\nNeo-6B - 13.7 - -\\nOPT - - - 31.1\\nOPT 66B - - - 44.54\\nOPT-175B - - - 43.94\\nOPT-175B - - - 25.6\\nPaLM 2-L 86.1 37.5 28.2 95.1\\nPaLM 2-M 81.7 32 26.9 64.9\\nPaLM 2-S 75.2 25.3 21.8 59.6\\nPaLM-540B 81.4 39.6 43.5 87.1\\nphi-1.5-web 1.3B - - - 44.9\\nSparseGPT - - - 38.99\\nSparseGPT - - - 39.85\\nSparseGPT - - - 41.3\\nFor some specific use-case models, it is highly demanded to\\nhave coding and code-generation capability. Table VIII shows\\nthe results of different models on coding capability.\\nTABLE VIII: Coding capability comparison.\\nModel HumanEval\\nGemini Ultra 74.4\\nGemini Pro 67.7\\nGPT-4 67\\nWizardCoder 15B 57.3\\nphi-1 1.3B 50.6\\nCode Llama 48.8\\nGPT-3.5 48.1\\nOctoCoder 46.2\\nphi-1-small 45\\nPaLM 2-S 37.6\\nInstructCodeT5+ 16B 35\\nMistral 7B 30.5\\nLLaMA 2 29.9\\nphi-1-base 29\\nCodex-12B 28.81\\nPaLM 540B 26.2\\nCodeT5+ 2B 24.2\\nLLaMA 65B 23.7\\nLLaMA 33B 21.7\\nPaLM 62B 15.9\\nLLaMA 13B 15.8\\nLaMDA 137B 14\\nMIM-350M 13.7\\nLLaMA 7B 10.5\\nPaLM 8B 3.6\\nArithmetic reasoning is another challenging reasoning ca-\\npability to achieve. GSM8K for example contains grade school\\nmathematical questions with respect to their answers. Table IX\\nprovides an insight for different model comparisons.TABLE IX: Arithmetic reasoning comparison.\\nModel GSM8k MATH\\nGemini Ultra 94.4 53.2\\nGPT-4 87.1 42.5\\nGemini Pro 86.5 32.6\\nToRA 70B 84.3 49.7\\nMathCoder-L-70B 83.9 -\\nMetaMath 70B 82.3 26\\nMuggleMATH 70B 82.3 -\\nMathCoder-CL-34B 81.7 45.2\\nToRA-Code 34B 80.7 50.8\\nMetaMath-Mistral-7B 77.7 -\\nArithmo2-Mistral-7B 76.4 -\\nToRA-Code 13B 75.8 48.1\\nArithmo-Mistral-7B 74.7 -\\nMathCoder-CL-13B 74.1 35.9\\nMuggleMATH 13B 74 -\\nCodeT5+ 73.8 -\\nKwaiYiiMath 13B 73.3 -\\nToRA-Code 7B 72.6 44.6\\nMathCoder-L-13B 72.6 29.9\\nMetaMath 13B 71 22.5\\nLLaMA 65B 69.7 10.6\\nMuggleMATH 7B 68.4 -\\nMathCoder-CL-7B 67.8 23.3\\nMetaMath 7B 66.4 19.4\\nRFT 70B 64.8 -\\nMathCoder-L-7B 64.2 -\\nOrca 2-13B 59.14 -\\nU-PaLM 58.5 -\\nPaLM-540B 58.1 8.8\\nLLaMA 2 70B 56.8 -\\nRFT 13B 55.3 -\\nLLaMA 33B 53.1 7.1\\nMistral 7B 52.2 13.1\\nRFT 7B 51.2 -\\nLLaMA 65B 50.9 20.5\\nOrca 2-7B 47.23 -\\nText-davinci-002 40.7 19.1\\nLLaMA 33B 35.6 3.9\\nGPT-Neo-2.7B 19.5 -\\nLLaMA 7B 18.1 2.9\\nPaLM 540B 17.9 8.8\\nLLaMA 13B 17.8 3.9\\nLLaMA 7B 11 2.9\\nGPT-Neo-125M 7.5 -\\nPaLM 8B 4.1 1.5\\nGPT-2 - 5.4\\nGPT-3 175B - 5.2\\nPaLM 62B - 4.4\\nGPT-3-13B - 3\\nLLaMA 7B 11 2.9\\nPaLM 8B - 1.5\\nLarge language models in some cases are hallucinating an-\\nswers simply because they are next-token prediction machines.\\nHallucination is one of the important factors in measuring\\nhow much a large language model is trustworthy and reliable.\\nMeasuring hallucination on the other hand is also not easy as it\\nseems because each fact can be written in different styles and\\neven the smallest changes in writing make it hard to detect.\\nIt is fair to assume if any particular LLM is more capable\\nto detect hallucination of false information in text, it is also\\nmore trustworthy. HaluEval is one of the datasets that aims to\\nmeasure hallucination in this field [205]. Evaluation can also be\\nperformed by another model judging the response with regard\\nto the actual answer [206]. Table X shows the evaluation of\\ndifferent models based on these datasets.\\nVII. C HALLENGES AND FUTURE DIRECTIONS\\nAs we have seen in the previous sections, large language\\nmodels have achieved impressive results in the past 1-2 years.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 34}, page_content='TABLE X: Hallucination evaluation\\nModel HHEM HaluEval QA HaluEval Dialogue HaluEval Sum. HaluEval General\\nGPT 4 97 - - - -\\nGPT 4 Turbo 97 - - - -\\nGPT 3.5 Turbo 96.5 62.59 72.4 58.53 79.44\\nDavinci002 - 60.05 60.81 47.77 80.42\\nDavinci003 - 49.65 68.37 48.07 80.4\\nGPT-3 - 49.21 50.02 51.23 72.72\\nGoogle Gemini Pro 95.2 - - - -\\nLlama 2 70B 94.9 - - - -\\nLlama 2 7B 94.4 49.6 43.99 49.55 20.46\\nLlama 2 13B 94.1 - - - -\\nCohere-Chat 92.5 - - - -\\nCohere 91.5 - - - -\\nClaude 2 91.5 69.78 64.73 57.75 75\\nClaude 1 67.6 64.83 53.76 73.88\\nMicrosoft Phi 2 91.5 - - - -\\nGoogle Palm 2 (beta) 91.4 - - - -\\nMixtral 8x7B 90.7 - - - -\\nAmazon Titan Express 90.6 - - - -\\nMistral 7B 90.6 - - - -\\nGoogle Palm 2 Chat (beta) 90 - - - -\\nGoogle Palm 2 87.9 - - - -\\nGoogle Palm 2 Chat 72.8 - - - -\\nChatGLM - 47.93 44.41 48.57 30.92\\nFalcon - 39.66 29.08 42.71 18.98\\nVicuna - 60.34 46.35 45.62 19.48\\nAlpaca - 6.68 17.55 20.63 9.54\\nAt the same time this is still a new and extremely active\\nresearch area where the pace of innovation is increasing rather\\nthan slowing down. As in any other evolving area though, there\\nare still numerous challenges ahead. Here we briefly mention\\nsome of the challenges and main active areas which are known\\nso far. It is worth noting that LLM challenges are discussed\\nin details in a work by Kaddour et al. [207].\\nA. Smaller and more efficient Language Models\\nThis is a survey on large language models, and there\\nhas been an initial push towards ‚Äùlarger is better‚Äù that has\\nclearly been rewarded with ever larger models like GPT-\\n4 getting better accuracy and performance in benchmarks.\\nHowever, those large models are costly and inefficient in\\nseveral dimensions (e.g. high latency). In response to all of\\nthis, there is a current research trend to come up with Small\\nLanguage Models (SLMs) as a cost-effective alternative to\\nLLMs, particularly when used on specific tasks that might not\\nrequire the full generality of larger models. Prominent works\\nin this direction include Phi-1 [208], Phi-1.5 [209], and Phi-2\\nfrom Microsoft.\\nMore generally, we should expect many research efforts in\\nthis area of how to train smaller and more efficient models.\\nTechniques such as parameter-efficient fine-tuning (PEFT),\\nteacher/student, and other forms of distillation ‚Äì see section\\nIII-I ‚Äì will continue to be used to build a smaller model out\\nof larger ones.\\nB. New Post-attention Architectural Paradigms\\nTransformer blocks have been a crucial and constant part of\\nmost of current LLM frameworks, and it‚Äôs a big question mark\\nhow much longer this architecture will be in vogue, and what\\nwill be the next big architectural break-through in the field of\\ndeep learning (and NLP). Since AlexNet in 2012, we have seen\\nmany architectures go in and out of fashion, including LSTM,GRU, seq2seq, but Transformers have been the dominant\\napproach since its inception. As described earlier, attention is\\nthe main mechanism driving transformers. More recently, there\\nhas been promising research in alternative approaches that are\\nbeing labelled as post-attention.\\nAn important class of such class of post-attention models\\nare the so called State Space Models (SSMs). While the notion\\nof State Space Models has a long history in machine learning,\\nit should be noted that in the context of language models, SSM\\nis usually used in reference to the newer Structure State Space\\nModel architecture or S4 for short (see Gu et al. [29]). Some\\nrecent models in this category are Mamba [30], Hyena [210],\\nand Striped Hyena [211].\\nWhile all of those models are very competitive in terms of\\nperformance in leaderboards and efficiency, they also address\\nan important challenge in more traditional attention-based\\narchitectures: the lack of support for larger context windows .\\nHaving a good answer to many prompts requires context.\\nFor example, the response to ‚ÄùRecommend some good movies\\nfor me‚Äù requires a lot of context about ‚Äùme‚Äù as well as what\\nmovies are available and which ones I have not watched.\\nContext length is especially important for RAG, where large\\nportions of text might be retrieved and injected into the prompt\\nfor generation (see section IV-C.\\nThe longer the context length, the more tokens we can\\nsqueeze into the context. The more information the model has\\naccess to, the better its response will be. But on the other\\nhand, with very long context, it would be hard for the model\\nto remember everything and efficiently process all the informa-\\ntion. Attention-based models are highly inefficient for longer\\ncontexts and that is why we should expect more research in\\ndifferent mechanisms that enable processing longer contexts\\nand generally come up with more efficient architectures.\\nThat being said, new architectures might not only propose'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 35}, page_content='alternatives for the attention mechanism but rather rethink the\\nwhole Transformer architecture. As an early example of this,\\nMonarch Mixer [212] proposes a new architecture that uses\\nthe same sub-quadratic primitive that achieves high hardware\\nefficiency on GPUs ‚Äì Monarch matrices ‚Äì along both sequence\\nlength and model dimension.\\nOn the other end of the spectrum, it is worth mentioning\\nthat there are some attention-compatible architectural mecha-\\nnisms that have been recently gaining steam and proving their\\nvalue in creating better and more powerful LLMs. Probably\\nthe best example of such mechanism is Mixture of Experts\\n(MoE). MoEs have been around in machine learning for years,\\neven before the Deep Learning Era [213], but they have been\\ngaining popularity since then, and particularly in the context\\nof Transformer models and LLMs.\\nIn LLMs, MoEs allow to train an extremely large model\\nthan is then only partially instantiated during inference\\nwhen some of the experts are turned off wherever the gat-\\ning/weighting function has a low weight assigned to them. As\\nan example, the GLaM model has 1.2 trillion parameters, but\\nduring inference only 2 out of the 64 experts are used [84].\\nMoEs are nowadays an important component of the so-\\ncalled frontier LLMs (i.e. the most advanced and capable\\nmodels). GPT-4 itself is rumored to be based on a MoE\\narchitecture, and some of the best performing LLMs such as\\nMixtral [117], are basically an MoE version of pre-existing\\nLLMs.\\nFinally, it is important to note that MoEs can be used as a\\ncomponent of any architecture regardless of whether it is based\\non attention or not. In fact, MoEs have also been applied to\\nSSM-based LLMs like Mamba citepioro2024moemamba. We\\nshould continue to see MoE-driven improvements in the future\\nregardless of the underlying architecture.\\nC. Multi-modal Models\\nFuture LLMs are expected to be multi-modal and handle\\na variety of data types, such as text, images, and videos,\\naudio, in a unified manner. This opens up possibilities for\\nmore diverse applications in fields like question answering,\\ncontent generation, creative arts, and healthcare, robotics, and\\nbeyond. There are already several prominent multi-modal\\nLLMs out there, including: LLA V A [214], LLA V A-Plus [215],\\nGPT-4 [33], Qwen-vl [116], Next-GPT [216], but the trend is\\nexpected to be continued. Evaluation of these models also is a\\nnew research topic, especially conversational generative vision\\nmodels [217]. Multi-modal LLMs can unlock huge potentials\\nin a variety of tasks, and there has already been a descent\\nprogress in this direction, which needs a dedicated paper to\\ndiscuss all its details.\\nD. Improved LLM Usage and Augmentation techniques\\nAs we described in sectionIV, many of the shortcomings\\nand limitations of LLMs such as hallucination can be ad-\\ndressed through advanced prompt engineering, use of tools,\\nor other augmentation techniques. We should expect not only\\ncontinued, but accelerated research in this area. It is worth\\nmentioning that, in the specific case of software engineering,\\nsome works ([218]) tried to automatically eliminate this issue\\nfrom the overall software engineering workflowLLM-based systems are already starting to replace ma-\\nchine learning systems that were until recently using other\\napproaches. As a clear example of this, LLMs are now being\\ndeployed to better understand people preference and interests,\\nand provide more personalized interactions, whether in cus-\\ntomer service, content recommendation, or other applications.\\nThis involves better understanding of user preferences, and\\nanalyzing their past interactions and using them as the context.\\nWe will continue to see research in the application and usage\\nof LLMs for not only personalization and recommendations ,\\nbut many other application areas using other machine learning\\ntechniques.\\nFinally, another important area of research we expect to\\ngather increased attention is that of LLM-based agents and\\nmulti-agent systems [172], [173], [174]. The development of\\nLLM systems with access to external tools and decision-\\nmaking capabilities is both exciting and challenging. We will\\nsee continued research and progress in this important area that\\nsome argue could lead to Artificial General Intelligence (AGI).\\nE. Security and Ethical/Responsible AI\\nEnsuring the robustness and security of LLMs against\\nadversarial attacks and other vulnerabilities is a critical area\\nof research [219]. As LLMs are increasingly deployed in real-\\nworld applications, they need to be protected from potential\\nthreats, to prevent them being used to manipulate people or\\nspread mis-information.\\nAddressing ethical concerns and biases in LLMs is another\\nactive area of research. Efforts are being made to ensure that\\nLLMs are fair, unbiased, and capable of handling sensitive\\ninformation responsibly. As LLMs are being used more and\\nmore by a large number of people on a daily basis, making\\nsure they are unbiased and behave responsibly is crucial.\\nVIII. C ONCLUSION\\nThis paper present a survey of LLMs developed in the\\npast few years. We first provide an overview of early pre-\\ntrained language models (e.g., as BERT), then review three\\npopular LLM families (GPT, LLaMA, PaLM), and other\\nrepresentative LLMs. We then survey methods and techniques\\nof building, augmenting, and using LLMs. We review popular\\nLLM datasets and benchmarks, and compare performance of\\na set of prominent models on public benchmarks. Finally, we\\npresent open challenges and future research directions.\\nREFERENCES\\n[1] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, ‚ÄúScaling laws\\nfor neural language models,‚Äù arXiv preprint arXiv:2001.08361 , 2020.\\n[2] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark\\net al. , ‚ÄúTraining compute-optimal large language models,‚Äù arXiv\\npreprint arXiv:2203.15556 , 2022.\\n[3] C. E. Shannon, ‚ÄúPrediction and entropy of printed english,‚Äù Bell system\\ntechnical journal , vol. 30, no. 1, pp. 50‚Äì64, 1951.\\n[4] F. Jelinek, Statistical methods for speech recognition . MIT press,\\n1998.\\n[5] C. Manning and H. Schutze, Foundations of statistical natural lan-\\nguage processing . MIT press, 1999.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 36}, page_content='[6] C. D. Manning, An introduction to information retrieval . Cambridge\\nuniversity press, 2009.\\n[7] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min,\\nB. Zhang, J. Zhang, Z. Dong et al. , ‚ÄúA survey of large language\\nmodels,‚Äù arXiv preprint arXiv:2303.18223 , 2023.\\n[8] C. Zhou, Q. Li, C. Li, J. Yu, Y . Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\\nL. He et al. , ‚ÄúA comprehensive survey on pretrained foundation mod-\\nels: A history from bert to chatgpt,‚Äù arXiv preprint arXiv:2302.09419 ,\\n2023.\\n[9] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, ‚ÄúPre-\\ntrain, prompt, and predict: A systematic survey of prompting methods\\nin natural language processing,‚Äù ACM Computing Surveys , vol. 55,\\nno. 9, pp. 1‚Äì35, 2023.\\n[10] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\\nJ. Xu, and Z. Sui, ‚ÄúA survey for in-context learning,‚Äù arXiv preprint\\narXiv:2301.00234 , 2022.\\n[11] J. Huang and K. C.-C. Chang, ‚ÄúTowards reasoning in large language\\nmodels: A survey,‚Äù arXiv preprint arXiv:2212.10403 , 2022.\\n[12] S. F. Chen and J. Goodman, ‚ÄúAn empirical study of smoothing\\ntechniques for language modeling,‚Äù Computer Speech & Language ,\\nvol. 13, no. 4, pp. 359‚Äì394, 1999.\\n[13] Y . Bengio, R. Ducharme, and P. Vincent, ‚ÄúA neural probabilistic\\nlanguage model,‚Äù Advances in neural information processing systems ,\\nvol. 13, 2000.\\n[14] H. Schwenk, D. D ¬¥echelotte, and J.-L. Gauvain, ‚ÄúContinuous space\\nlanguage models for statistical machine translation,‚Äù in Proceedings\\nof the COLING/ACL 2006 Main Conference Poster Sessions , 2006,\\npp. 723‚Äì730.\\n[15] T. Mikolov, M. Karafi ¬¥at, L. Burget, J. Cernock `y, and S. Khudanpur,\\n‚ÄúRecurrent neural network based language model.‚Äù in Interspeech ,\\nvol. 2, no. 3. Makuhari, 2010, pp. 1045‚Äì1048.\\n[16] A. Graves, ‚ÄúGenerating sequences with recurrent neural networks,‚Äù\\narXiv preprint arXiv:1308.0850 , 2013.\\n[17] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, ‚ÄúLearning\\ndeep structured semantic models for web search using clickthrough\\ndata,‚Äù in Proceedings of the 22nd ACM international conference on\\nInformation & Knowledge Management , 2013, pp. 2333‚Äì2338.\\n[18] J. Gao, C. Xiong, P. Bennett, and N. Craswell, Neural Approaches to\\nConversational Information Retrieval . Springer Nature, 2023, vol. 44.\\n[19] I. Sutskever, O. Vinyals, and Q. V . Le, ‚ÄúSequence to sequence learning\\nwith neural networks,‚Äù Advances in neural information processing\\nsystems , vol. 27, 2014.\\n[20] K. Cho, B. Van Merri ¬®enboer, D. Bahdanau, and Y . Bengio, ‚ÄúOn\\nthe properties of neural machine translation: Encoder-decoder ap-\\nproaches,‚Äù arXiv preprint arXiv:1409.1259 , 2014.\\n[21] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Doll ¬¥ar,\\nJ. Gao, X. He, M. Mitchell, J. C. Platt et al. , ‚ÄúFrom captions to\\nvisual concepts and back,‚Äù in Proceedings of the IEEE conference\\non computer vision and pattern recognition , 2015, pp. 1473‚Äì1482.\\n[22] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, ‚ÄúShow and tell:\\nA neural image caption generator,‚Äù in Proceedings of the IEEE\\nconference on computer vision and pattern recognition , 2015, pp.\\n3156‚Äì3164.\\n[23] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nand L. Zettlemoyer, ‚ÄúDeep contextualized word representations. corr\\nabs/1802.05365 (2018),‚Äù arXiv preprint arXiv:1802.05365 , 2018.\\n[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-training\\nof deep bidirectional transformers for language understanding,‚Äù arXiv\\npreprint arXiv:1810.04805 , 2018.\\n[25] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, and V . Stoyanov, ‚ÄúRoberta: A robustly optimized bert\\npretraining approach,‚Äù arXiv preprint arXiv:1907.11692 , 2019.\\n[26] P. He, X. Liu, J. Gao, and W. Chen, ‚ÄúDeberta: Decoding-enhanced bert\\nwith disentangled attention,‚Äù arXiv preprint arXiv:2006.03654 , 2020.\\n[27] X. Han, Z. Zhang, N. Ding, Y . Gu, X. Liu, Y . Huo, J. Qiu, Y . Yao,\\nA. Zhang, L. Zhang et al. , ‚ÄúPre-trained models: Past, present and\\nfuture,‚Äù AI Open , vol. 2, pp. 225‚Äì250, 2021.\\n[28] X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang, ‚ÄúPre-trainedmodels for natural language processing: A survey,‚Äù Science China\\nTechnological Sciences , vol. 63, no. 10, pp. 1872‚Äì1897, 2020.\\n[29] A. Gu, K. Goel, and C. R ¬¥e, ‚ÄúEfficiently modeling long sequences with\\nstructured state spaces,‚Äù 2022.\\n[30] A. Gu and T. Dao, ‚ÄúMamba: Linear-time sequence modeling with\\nselective state spaces,‚Äù arXiv preprint arXiv:2312.00752 , 2023.\\n[31] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\\nA. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al. ,\\n‚ÄúPalm: Scaling language modeling with pathways,‚Äù arXiv preprint\\narXiv:2204.02311 , 2022.\\n[32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al. , ‚ÄúLlama:\\nOpen and efficient foundation language models,‚Äù arXiv preprint\\narXiv:2302.13971 , 2023.\\n[33] OpenAI, ‚ÄúGPT-4 Technical Report,‚Äù https://arxiv.org/pdf/2303.\\n08774v3.pdf, 2023.\\n[34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter,\\nF. Xia, E. Chi, Q. V . Le, and D. Zhou, ‚ÄúChain-of-thought\\nprompting elicits reasoning in large language models,‚Äù in\\nAdvances in Neural Information Processing Systems , S. Koyejo,\\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\\nEds., vol. 35. Curran Associates, Inc., 2022, pp. 24 824‚Äì24 837.\\n[Online]. Available: https://proceedings.neurips.cc/paper files/paper/\\n2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\\n[35] G. Mialon, R. Dess `ƒ±, M. Lomeli, C. Nalmpantis, R. Pasunuru,\\nR. Raileanu, B. Rozi `ere, T. Schick, J. Dwivedi-Yu, A. Celikyil-\\nmaz et al. , ‚ÄúAugmented language models: a survey,‚Äù arXiv preprint\\narXiv:2302.07842 , 2023.\\n[36] B. Peng, M. Galley, P. He, H. Cheng, Y . Xie, Y . Hu, Q. Huang,\\nL. Liden, Z. Yu, W. Chen, and J. Gao, ‚ÄúCheck your facts and try\\nagain: Improving large language models with external knowledge and\\nautomated feedback,‚Äù arXiv preprint arXiv:2302.12813 , 2023.\\n[37] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao,\\n‚ÄúReact: Synergizing reasoning and acting in language models,‚Äù arXiv\\npreprint arXiv:2210.03629 , 2022.\\n[38] D. E. Rumelhart, G. E. Hinton, R. J. Williams et al. , ‚ÄúLearning internal\\nrepresentations by error propagation,‚Äù 1985.\\n[39] J. L. Elman, ‚ÄúFinding structure in time,‚Äù Cognitive science , vol. 14,\\nno. 2, pp. 179‚Äì211, 1990.\\n[40] M. V . Mahoney, ‚ÄúFast text compression with neural networks.‚Äù in\\nFLAIRS conference , 2000, pp. 230‚Äì234.\\n[41] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. ÀáCernock `y, ‚ÄúStrate-\\ngies for training large scale neural network language models,‚Äù in 2011\\nIEEE Workshop on Automatic Speech Recognition & Understanding .\\nIEEE, 2011, pp. 196‚Äì201.\\n[42] tmikolov. rnnlm. [Online]. Available: https://www.fit.vutbr.cz/\\n‚àºimikolov/rnnlm/\\n[43] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu,\\nand J. Gao, ‚ÄúDeep learning‚Äìbased text classification: a comprehensive\\nreview,‚Äù ACM computing surveys (CSUR) , vol. 54, no. 3, pp. 1‚Äì40,\\n2021.\\n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\\nAdvances in neural information processing systems , vol. 30, 2017.\\n[45] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\\n‚ÄúAlbert: A lite bert for self-supervised learning of language represen-\\ntations,‚Äù arXiv preprint arXiv:1909.11942 , 2019.\\n[46] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning, ‚ÄúElectra: Pre-\\ntraining text encoders as discriminators rather than generators,‚Äù arXiv\\npreprint arXiv:2003.10555 , 2020.\\n[47] G. Lample and A. Conneau, ‚ÄúCross-lingual language model pretrain-\\ning,‚Äù arXiv preprint arXiv:1901.07291 , 2019.\\n[48] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and\\nQ. V . Le, ‚ÄúXlnet: Generalized autoregressive pretraining for language\\nunderstanding,‚Äù Advances in neural information processing systems ,\\nvol. 32, 2019.\\n[49] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao,\\nM. Zhou, and H.-W. Hon, ‚ÄúUnified language model pre-training for'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 37}, page_content='natural language understanding and generation,‚Äù Advances in neural\\ninformation processing systems , vol. 32, 2019.\\n[50] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al. , ‚ÄúImprov-\\ning language understanding by generative pre-training,‚Äù 2018.\\n[51] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. ,\\n‚ÄúLanguage models are unsupervised multitask learners,‚Äù OpenAI blog ,\\nvol. 1, no. 8, p. 9, 2019.\\n[52] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY . Zhou, W. Li, and P. J. Liu, ‚ÄúExploring the limits of transfer learning\\nwith a unified text-to-text transformer,‚Äù The Journal of Machine\\nLearning Research , vol. 21, no. 1, pp. 5485‚Äì5551, 2020.\\n[53] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\\nA. Barua, and C. Raffel, ‚Äúmt5: A massively multilingual pre-trained\\ntext-to-text transformer,‚Äù arXiv preprint arXiv:2010.11934 , 2020.\\n[54] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y . Liu, ‚ÄúMass: Masked\\nsequence to sequence pre-training for language generation,‚Äù arXiv\\npreprint arXiv:1905.02450 , 2019.\\n[55] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\\nV . Stoyanov, and L. Zettlemoyer, ‚ÄúBart: Denoising sequence-to-\\nsequence pre-training for natural language generation, translation, and\\ncomprehension,‚Äù arXiv preprint arXiv:1910.13461 , 2019.\\n[56] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , ‚ÄúLanguage mod-\\nels are few-shot learners,‚Äù Advances in neural information processing\\nsystems , vol. 33, pp. 1877‚Äì1901, 2020.\\n[57] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Ka-\\nplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman et al. ,\\n‚ÄúEvaluating large language models trained on code,‚Äù arXiv preprint\\narXiv:2107.03374 , 2021.\\n[58] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V . Kosaraju, W. Saunders et al. , ‚ÄúWebgpt: Browser-\\nassisted question-answering with human feedback,‚Äù arXiv preprint\\narXiv:2112.09332 , 2021.\\n[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , ‚ÄúTraining language\\nmodels to follow instructions with human feedback,‚Äù Advances in\\nNeural Information Processing Systems , vol. 35, pp. 27 730‚Äì27 744,\\n2022.\\n[60] OpenAI. (2022) Introducing chatgpt. [Online]. Available: https:\\n//openai.com/blog/chatgpt\\n[61] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al. , ‚ÄúLlama\\n2: Open foundation and fine-tuned chat models,‚Äù arXiv preprint\\narXiv:2307.09288 , 2023.\\n[62] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang,\\nand T. B. Hashimoto, ‚ÄúAlpaca: A strong, replicable instruction-\\nfollowing model,‚Äù Stanford Center for Research on Foundation Mod-\\nels. https://crfm. stanford. edu/2023/03/13/alpaca. html , vol. 3, no. 6,\\np. 7, 2023.\\n[63] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, ‚ÄúQlora: Ef-\\nficient finetuning of quantized llms,‚Äù arXiv preprint arXiv:2305.14314 ,\\n2023.\\n[64] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\\nand D. Song, ‚ÄúKoala: A dialogue model for academic research,‚Äù Blog\\npost, April , vol. 1, 2023.\\n[65] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,\\nD. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al. ,\\n‚ÄúMistral 7b,‚Äù arXiv preprint arXiv:2310.06825 , 2023.\\n[66] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y . Adi,\\nJ. Liu, T. Remez, J. Rapin et al. , ‚ÄúCode llama: Open foundation models\\nfor code,‚Äù arXiv preprint arXiv:2308.12950 , 2023.\\n[67] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, ‚ÄúGorilla: Large\\nlanguage model connected with massive apis,‚Äù 2023.\\n[68] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, and\\nS. Naidu, ‚ÄúGiraffe: Adventures in expanding context lengths in llms,‚Äù\\narXiv preprint arXiv:2308.10882 , 2023.\\n[69] B. Huang, ‚ÄúVigogne: French instruction-following and chat models,‚Äù\\nhttps://github.com/bofenghuang/vigogne, 2023.[70] Y . Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu,\\nD. Wadden, K. MacMillan, N. A. Smith, I. Beltagy et al. , ‚ÄúHow far can\\ncamels go? exploring the state of instruction tuning on open resources,‚Äù\\narXiv preprint arXiv:2306.04751 , 2023.\\n[71] S. Tworkowski, K. Staniszewski, M. Pacek, Y . Wu, H. Michalewski,\\nand P. Mi≈Ço ¬¥s, ‚ÄúFocused transformer: Contrastive training for context\\nscaling,‚Äù arXiv preprint arXiv:2307.03170 , 2023.\\n[72] D. Mahan, R. Carlow, L. Castricato, N. Cooper,\\nand C. Laforte, ‚ÄúStable beluga models.‚Äù [Online].\\nAvailable: [https://huggingface.co/stabilityai/StableBeluga2](https://\\nhuggingface.co/stabilityai/StableBeluga2)\\n[73] Y . Tay, J. Wei, H. W. Chung, V . Q. Tran, D. R. So, S. Shakeri, X. Gar-\\ncia, H. S. Zheng, J. Rao, A. Chowdhery et al. , ‚ÄúTranscending scaling\\nlaws with 0.1% extra compute,‚Äù arXiv preprint arXiv:2210.11399 ,\\n2022.\\n[74] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus,\\nY . Li, X. Wang, M. Dehghani, S. Brahma et al. , ‚ÄúScaling instruction-\\nfinetuned language models,‚Äù arXiv preprint arXiv:2210.11416 , 2022.\\n[75] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen et al. , ‚ÄúPalm 2 technical\\nreport,‚Äù arXiv preprint arXiv:2305.10403 , 2023.\\n[76] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al. , ‚ÄúLarge language\\nmodels encode clinical knowledge,‚Äù arXiv preprint arXiv:2212.13138 ,\\n2022.\\n[77] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou,\\nK. Clark, S. Pfohl, H. Cole-Lewis, D. Neal et al. , ‚ÄúTowards expert-\\nlevel medical question answering with large language models,‚Äù arXiv\\npreprint arXiv:2305.09617 , 2023.\\n[78] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,\\nA. M. Dai, and Q. V . Le, ‚ÄúFinetuned language models are zero-shot\\nlearners,‚Äù arXiv preprint arXiv:2109.01652 , 2021.\\n[79] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\\nJ. Aslanides, S. Henderson, R. Ring, S. Young et al. , ‚ÄúScaling language\\nmodels: Methods, analysis & insights from training gopher,‚Äù arXiv\\npreprint arXiv:2112.11446 , 2021.\\n[80] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al. , ‚ÄúMulti-\\ntask prompted training enables zero-shot task generalization,‚Äù arXiv\\npreprint arXiv:2110.08207 , 2021.\\n[81] Y . Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\\nY . Zhao, Y . Lu et al. , ‚ÄúErnie 3.0: Large-scale knowledge enhanced pre-\\ntraining for language understanding and generation,‚Äù arXiv preprint\\narXiv:2107.02137 , 2021.\\n[82] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Mil-\\nlican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark\\net al. , ‚ÄúImproving language models by retrieving from trillions of\\ntokens,‚Äù in International conference on machine learning . PMLR,\\n2022, pp. 2206‚Äì2240.\\n[83] O. Lieber, O. Sharir, B. Lenz, and Y . Shoham, ‚ÄúJurassic-1: Technical\\ndetails and evaluation,‚Äù White Paper. AI21 Labs , vol. 1, p. 9, 2021.\\n[84] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun,\\nY . Zhou, A. W. Yu, O. Firat et al. , ‚ÄúGlam: Efficient scaling of\\nlanguage models with mixture-of-experts,‚Äù in International Conference\\non Machine Learning . PMLR, 2022, pp. 5547‚Äì5569.\\n[85] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-\\nT. Cheng, A. Jin, T. Bos, L. Baker, Y . Du et al. , ‚ÄúLamda: Language\\nmodels for dialog applications,‚Äù arXiv preprint arXiv:2201.08239 ,\\n2022.\\n[86] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,\\nC. Dewan, M. Diab, X. Li, X. V . Lin et al. , ‚ÄúOpt: Open pre-trained\\ntransformer language models,‚Äù arXiv preprint arXiv:2205.01068 , 2022.\\n[87] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Sar-\\navia, A. Poulton, V . Kerkez, and R. Stojnic, ‚ÄúGalactica: A large\\nlanguage model for science,‚Äù arXiv preprint arXiv:2211.09085 , 2022.\\n[88] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou,\\nS. Savarese, and C. Xiong, ‚ÄúCodegen: An open large language\\nmodel for code with multi-turn program synthesis,‚Äù arXiv preprint\\narXiv:2203.13474 , 2022.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 38}, page_content='[89] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky et al. ,\\n‚ÄúAlexatm 20b: Few-shot learning using a large-scale multilingual\\nseq2seq model,‚Äù arXiv preprint arXiv:2208.01448 , 2022.\\n[90] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V . Firoiu,\\nT. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al. ,\\n‚ÄúImproving alignment of dialogue agents via targeted human judge-\\nments,‚Äù arXiv preprint arXiv:2209.14375 , 2022.\\n[91] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski,\\nV . Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo et al. ,\\n‚ÄúSolving quantitative reasoning problems with language models,‚Äù\\nAdvances in Neural Information Processing Systems , vol. 35, pp.\\n3843‚Äì3857, 2022.\\n[92] Y . Tay, M. Dehghani, V . Q. Tran, X. Garcia, D. Bahri, T. Schuster,\\nH. S. Zheng, N. Houlsby, and D. Metzler, ‚ÄúUnifying language learning\\nparadigms,‚Äù arXiv preprint arXiv:2205.05131 , 2022.\\n[93] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ¬¥c, D. Hesslow,\\nR. Castagn ¬¥e, A. S. Luccioni, F. Yvon, M. Gall ¬¥eet al. , ‚ÄúBloom: A 176b-\\nparameter open-access multilingual language model,‚Äù arXiv preprint\\narXiv:2211.05100 , 2022.\\n[94] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu,\\nW. Zheng, X. Xia et al. , ‚ÄúGlm-130b: An open bilingual pre-trained\\nmodel,‚Äù arXiv preprint arXiv:2210.02414 , 2022.\\n[95] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O‚ÄôBrien,\\nE. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff et al. ,\\n‚ÄúPythia: A suite for analyzing large language models across train-\\ning and scaling,‚Äù in International Conference on Machine Learning .\\nPMLR, 2023, pp. 2397‚Äì2430.\\n[96] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and\\nA. Awadallah, ‚ÄúOrca: Progressive learning from complex explanation\\ntraces of gpt-4,‚Äù arXiv preprint arXiv:2306.02707 , 2023.\\n[97] R. Li, L. B. Allal, Y . Zi, N. Muennighoff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim et al. , ‚ÄúStarcoder: may the source\\nbe with you!‚Äù arXiv preprint arXiv:2305.06161 , 2023.\\n[98] S. Huang, L. Dong, W. Wang, Y . Hao, S. Singhal, S. Ma, T. Lv,\\nL. Cui, O. K. Mohammed, Q. Liu et al. , ‚ÄúLanguage is not all you\\nneed: Aligning perception with language models,‚Äù arXiv preprint\\narXiv:2302.14045 , 2023.\\n[99] G. Team, R. Anil, S. Borgeaud, Y . Wu, J.-B. Alayrac, J. Yu, R. Soricut,\\nJ. Schalkwyk, A. M. Dai, A. Hauth et al. , ‚ÄúGemini: a family of highly\\ncapable multimodal models,‚Äù arXiv preprint arXiv:2312.11805 , 2023.\\n[100] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\\nJ. Tompson, I. Mordatch, Y . Chebotar et al. , ‚ÄúInner monologue:\\nEmbodied reasoning through planning with language models,‚Äù arXiv\\npreprint arXiv:2207.05608 , 2022.\\n[101] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti\\net al. , ‚ÄúUsing deepspeed and megatron to train megatron-turing\\nnlg 530b, a large-scale generative language model,‚Äù arXiv preprint\\narXiv:2201.11990 , 2022.\\n[102] I. Beltagy, M. E. Peters, and A. Cohan, ‚ÄúLongformer: The long-\\ndocument transformer,‚Äù arXiv preprint arXiv:2004.05150 , 2020.\\n[103] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus-\\nter, T. Wang, Q. Liu, P. S. Koura et al. , ‚ÄúOpt-iml: Scaling language\\nmodel instruction meta learning through the lens of generalization,‚Äù\\narXiv preprint arXiv:2212.12017 , 2022.\\n[104] Y . Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma,\\nand F. Wei, ‚ÄúLanguage models are general-purpose interfaces,‚Äù arXiv\\npreprint arXiv:2206.06336 , 2022.\\n[105] Z. Sun, Y . Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y . Yang,\\nand C. Gan, ‚ÄúPrinciple-driven self-alignment of language mod-\\nels from scratch with minimal human supervision,‚Äù arXiv preprint\\narXiv:2305.03047 , 2023.\\n[106] W. E. team, ‚ÄúPalmyra-base Parameter Autoregressive Language\\nModel,‚Äù https://dev.writer.com, 2023.\\n[107] ‚Äî‚Äî, ‚ÄúCamel-5b instructgpt,‚Äù https://dev.writer.com, 2023.\\n[108] Yandex. Yalm. [Online]. Available: https://github.com/yandex/\\nYaLM-100B\\n[109] M. Team et al. , ‚ÄúIntroducing mpt-7b: a new standard for open-source,\\ncommercially usable llms,‚Äù 2023.[110] A. Mitra, L. D. Corro, S. Mahajan, A. Codas, C. Simoes, S. Agarwal,\\nX. Chen, A. Razdaibiedina, E. Jones, K. Aggarwal, H. Palangi,\\nG. Zheng, C. Rosset, H. Khanpour, and A. Awadallah, ‚ÄúOrca 2:\\nTeaching small language models how to reason,‚Äù 2023.\\n[111] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y . Yang, J. Callan, and\\nG. Neubig, ‚ÄúPal: Program-aided language models,‚Äù in International\\nConference on Machine Learning . PMLR, 2023, pp. 10 764‚Äì10 799.\\n[112] Anthropic. claude. [Online]. Available: https://www.anthropic.com/\\nnews/introducing-claude\\n[113] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y . Zhou,\\n‚ÄúCodegen2: Lessons for training llms on programming and natural\\nlanguages,‚Äù arXiv preprint arXiv:2305.02309 , 2023.\\n[114] L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y . Belkada,\\nS. Huang, L. von Werra, C. Fourrier, N. Habib et al. , ‚ÄúZephyr: Direct\\ndistillation of lm alignment,‚Äù arXiv preprint arXiv:2310.16944 , 2023.\\n[115] X. team. Grok. [Online]. Available: https://grok.x.ai/\\n[116] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou,\\nand J. Zhou, ‚ÄúQwen-vl: A frontier large vision-language model with\\nversatile abilities,‚Äù arXiv preprint arXiv:2308.12966 , 2023.\\n[117] mixtral. mixtral. [Online]. Available: https://mistral.ai/news/\\nmixtral-of-experts/\\n[118] D. Wang, N. Raman, M. Sibue, Z. Ma, P. Babkin, S. Kaur, Y . Pei,\\nA. Nourbakhsh, and X. Liu, ‚ÄúDocllm: A layout-aware generative\\nlanguage model for multimodal document understanding,‚Äù 2023.\\n[119] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi,\\nY . Wu, Y . K. Li, F. Luo, Y . Xiong, and W. Liang, ‚ÄúDeepseek-coder:\\nWhen the large language model meets programming ‚Äì the rise of code\\nintelligence,‚Äù 2024.\\n[120] F. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi, ‚ÄúKnowledge\\nfusion of large language models,‚Äù 2024.\\n[121] P. Zhang, G. Zeng, T. Wang, and W. Lu, ‚ÄúTinyllama: An open-source\\nsmall language model,‚Äù 2024.\\n[122] C. Wu, Y . Gan, Y . Ge, Z. Lu, J. Wang, Y . Feng, P. Luo, and Y . Shan,\\n‚ÄúLlama pro: Progressive llama with block expansion,‚Äù 2024.\\n[123] X. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and\\nM. Kazi, ‚ÄúTransformer models: an introduction and catalog,‚Äù 2023.\\n[124] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli,\\nH. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, ‚ÄúThe refined-\\nweb dataset for falcon llm: outperforming curated corpora with web\\ndata, and web data only,‚Äù arXiv preprint arXiv:2306.01116 , 2023.\\n[125] D. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-\\nShowk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume et al. ,\\n‚ÄúScaling laws and interpretability of learning from repeated data,‚Äù\\narXiv preprint arXiv:2205.10487 , 2022.\\n[126] P. Shaw, J. Uszkoreit, and A. Vaswani, ‚ÄúSelf-attention with relative\\nposition representations,‚Äù arXiv preprint arXiv:1803.02155 , 2018.\\n[127] J. Su, Y . Lu, S. Pan, B. Wen, and Y . Liu, ‚ÄúRoformer: En-\\nhanced transformer with rotary position embedding,‚Äù arXiv preprint\\narXiv:2104.09864 , 2021.\\n[128] O. Press, N. A. Smith, and M. Lewis, ‚ÄúTrain short, test long: Attention\\nwith linear biases enables input length extrapolation,‚Äù arXiv preprint\\narXiv:2108.12409 , 2021.\\n[129] G. Ke, D. He, and T.-Y . Liu, ‚ÄúRethinking positional encoding in\\nlanguage pre-training,‚Äù arXiv preprint arXiv:2006.15595 , 2020.\\n[130] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\\nand J. Dean, ‚ÄúOutrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer,‚Äù arXiv preprint arXiv:1701.06538 , 2017.\\n[131] W. Fedus, B. Zoph, and N. Shazeer, ‚ÄúSwitch transformers: Scaling\\nto trillion parameter models with simple and efficient sparsity,‚Äù The\\nJournal of Machine Learning Research , vol. 23, no. 1, pp. 5232‚Äì5270,\\n2022.\\n[132] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson,\\n‚ÄúParameter-efficient multi-task fine-tuning for transformers via shared\\nhypernetworks,‚Äù 2021.\\n[133] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,\\nT. Zhang, F. Wu, and G. Wang, ‚ÄúInstruction tuning for large language\\nmodels: A survey,‚Äù 2023.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 39}, page_content='[134] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, ‚ÄúCross-task\\ngeneralization via natural language crowdsourcing instructions,‚Äù arXiv\\npreprint arXiv:2104.08773 , 2021.\\n[135] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,\\nand H. Hajishirzi, ‚ÄúSelf-instruct: Aligning language model with self\\ngenerated instructions,‚Äù arXiv preprint arXiv:2212.10560 , 2022.\\n[136] K. Ethayarajh, W. Xu, D. Jurafsky, and D. Kiela. Kto. [Online].\\nAvailable: https://github.com/ContextualAI/HALOs/blob/main/assets/\\nreport.pdf\\n[137] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and\\nD. Amodei, ‚ÄúDeep reinforcement learning from human preferences,‚Äù\\nAdvances in neural information processing systems , vol. 30, 2017.\\n[138] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V . Car-\\nbune, and A. Rastogi, ‚ÄúRlaif: Scaling reinforcement learning from\\nhuman feedback with ai feedback,‚Äù arXiv preprint arXiv:2309.00267 ,\\n2023.\\n[139] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and\\nC. Finn, ‚ÄúDirect preference optimization: Your language model is\\nsecretly a reward model,‚Äù arXiv preprint arXiv:2305.18290 , 2023.\\n[140] S. Rajbhandari, J. Rasley, O. Ruwase, and Y . He, ‚ÄúZero: Memory\\noptimizations toward training trillion parameter models,‚Äù in SC20: In-\\nternational Conference for High Performance Computing, Networking,\\nStorage and Analysis . IEEE, 2020, pp. 1‚Äì16.\\n[141] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao,\\nX. Cheng, M. Chung, M. Grella, K. K. GV et al. , ‚ÄúRwkv: Reinventing\\nrnns for the transformer era,‚Äù arXiv preprint arXiv:2305.13048 , 2023.\\n[142] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,\\nand W. Chen, ‚ÄúLora: Low-rank adaptation of large language models,‚Äù\\narXiv preprint arXiv:2106.09685 , 2021.\\n[143] G. Hinton, O. Vinyals, and J. Dean, ‚ÄúDistilling the knowledge in a\\nneural network,‚Äù arXiv preprint arXiv:1503.02531 , 2015.\\n[144] J. Gou, B. Yu, S. J. Maybank, and D. Tao, ‚ÄúKnowledge distillation:\\nA survey,‚Äù International Journal of Computer Vision , vol. 129, pp.\\n1789‚Äì1819, 2021.\\n[145] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y . Xu, E. Ishii, Y . J.\\nBang, A. Madotto, and P. Fung, ‚ÄúSurvey of hallucination in natural\\nlanguage generation,‚Äù ACM Comput. Surv. , vol. 55, no. 12, mar 2023.\\n[Online]. Available: https://doi.org/10.1145/3571730\\n[146] N. McKenna, T. Li, L. Cheng, M. J. Hosseini, M. Johnson, and\\nM. Steedman, ‚ÄúSources of hallucination by large language models on\\ninference tasks,‚Äù 2023.\\n[147] C.-Y . Lin, ‚ÄúROUGE: A package for automatic evaluation of\\nsummaries,‚Äù in Text Summarization Branches Out . Barcelona, Spain:\\nAssociation for Computational Linguistics, Jul. 2004, pp. 74‚Äì81.\\n[Online]. Available: https://aclanthology.org/W04-1013\\n[148] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ‚ÄúBleu: a method for\\nautomatic evaluation of machine translation,‚Äù in Proceedings of the\\n40th Annual Meeting of the Association for Computational Linguistics ,\\nP. Isabelle, E. Charniak, and D. Lin, Eds. Philadelphia, Pennsylvania,\\nUSA: Association for Computational Linguistics, Jul. 2002, pp. 311‚Äì\\n318. [Online]. Available: https://aclanthology.org/P02-1040\\n[149] B. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, and\\nW. Cohen, ‚ÄúHandling divergent reference texts when evaluating\\ntable-to-text generation,‚Äù in Proceedings of the 57th Annual Meeting\\nof the Association for Computational Linguistics , A. Korhonen,\\nD. Traum, and L. M `arquez, Eds. Florence, Italy: Association\\nfor Computational Linguistics, Jul. 2019, pp. 4884‚Äì4895. [Online].\\nAvailable: https://aclanthology.org/P19-1483\\n[150] Z. Wang, X. Wang, B. An, D. Yu, and C. Chen, ‚ÄúTowards faithful\\nneural table-to-text generation with content-matching constraints,‚Äù\\ninProceedings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics , D. Jurafsky, J. Chai, N. Schluter,\\nand J. Tetreault, Eds. Online: Association for Computational\\nLinguistics, Jul. 2020, pp. 1072‚Äì1086. [Online]. Available: https:\\n//aclanthology.org/2020.acl-main.101\\n[151] H. Song, W.-N. Zhang, J. Hu, and T. Liu, ‚ÄúGenerating persona consis-\\ntent dialogues by exploiting natural language inference,‚Äù Proceedings\\nof the AAAI Conference on Artificial Intelligence , vol. 34, no. 05, pp.\\n8878‚Äì8885, Apr. 2020.\\n[152] O. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor,and O. Abend, ‚Äú q2: Evaluating factual consistency in knowledge-\\ngrounded dialogues via question generation and question answering,‚Äù\\ninProceedings of the 2021 Conference on Empirical Methods in\\nNatural Language Processing , M.-F. Moens, X. Huang, L. Specia,\\nand S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic:\\nAssociation for Computational Linguistics, Nov. 2021, pp. 7856‚Äì7870.\\n[Online]. Available: https://aclanthology.org/2021.emnlp-main.619\\n[153] N. Dziri, H. Rashkin, T. Linzen, and D. Reitter, ‚ÄúEvaluating attribution\\nin dialogue systems: The BEGIN benchmark,‚Äù Transactions of the\\nAssociation for Computational Linguistics , vol. 10, pp. 1066‚Äì1083,\\n2022. [Online]. Available: https://aclanthology.org/2022.tacl-1.62\\n[154] S. Santhanam, B. Hedayatnia, S. Gella, A. Padmakumar, S. Kim,\\nY . Liu, and D. Z. Hakkani-T ¬®ur, ‚ÄúRome was built in 1776: A case study\\non factual correctness in knowledge-grounded response generation,‚Äù\\nArXiv , vol. abs/2110.05456, 2021.\\n[155] S. Min, K. Krishna, X. Lyu, M. Lewis, W. tau Yih, P. W. Koh, M. Iyyer,\\nL. Zettlemoyer, and H. Hajishirzi, ‚ÄúFactscore: Fine-grained atomic\\nevaluation of factual precision in long form text generation,‚Äù 2023.\\n[156] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner,\\nV . Chaudhary, and M. Young, ‚ÄúMachine learning: The high interest\\ncredit card of technical debt,‚Äù in SE4ML: Software Engineering for\\nMachine Learning (NIPS 2014 Workshop) , 2014.\\n[157] Z. Zhang, A. Zhang, M. Li, and A. Smola, ‚ÄúAutomatic chain of thought\\nprompting in large language models,‚Äù 2022.\\n[158] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, and\\nK. Narasimhan, ‚ÄúTree of thoughts: Deliberate problem solving with\\nlarge language models,‚Äù 2023.\\n[159] P. Manakul, A. Liusie, and M. J. F. Gales, ‚ÄúSelfcheckgpt: Zero-\\nresource black-box hallucination detection for generative large lan-\\nguage models,‚Äù 2023.\\n[160] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan,\\nand S. Yao, ‚ÄúReflexion: Language agents with verbal reinforcement\\nlearning,‚Äù 2023.\\n[161] S. J. Zhang, S. Florin, A. N. Lee, E. Niknafs, A. Marginean, A. Wang,\\nK. Tyser, Z. Chin, Y . Hicke, N. Singh, M. Udell, Y . Kim, T. Buonassisi,\\nA. Solar-Lezama, and I. Drori, ‚ÄúExploring the mit mathematics and\\neecs curriculum using large language models,‚Äù 2023.\\n[162] T. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J.\\nCai, ‚ÄúPromptchainer: Chaining large language model prompts through\\nvisual programming,‚Äù 2022.\\n[163] Y . Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and\\nJ. Ba, ‚ÄúLarge language models are human-level prompt engineers,‚Äù\\n2023.\\n[164] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin,\\nN. Goyal, H. K ¬®uttler, M. Lewis, W. Yih, T. Rockt ¬®aschel, S. Riedel, and\\nD. Kiela, ‚ÄúRetrieval-augmented generation for knowledge-intensive\\nNLP tasks,‚Äù CoRR , vol. abs/2005.11401, 2020. [Online]. Available:\\nhttps://arxiv.org/abs/2005.11401\\n[165] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, and\\nH. Wang, ‚ÄúRetrieval-augmented generation for large language models:\\nA survey,‚Äù arXiv preprint arXiv:2312.10997 , 2023.\\n[166] A. W. Services. (Year of publication, e.g., 2023) Question answering\\nusing retrieval augmented generation with foundation models in\\namazon sagemaker jumpstart. Accessed: Date of access, e.g.,\\nDecember 5, 2023. [Online]. Available: https://shorturl.at/dSV47\\n[167] S. Pan, L. Luo, Y . Wang, C. Chen, J. Wang, and X. Wu, ‚ÄúUnifying large\\nlanguage models and knowledge graphs: A roadmap,‚Äù arXiv preprint\\narXiv:2306.08302 , 2023.\\n[168] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\\nJ. Callan, and G. Neubig, ‚ÄúActive retrieval augmented generation,‚Äù\\n2023.\\n[169] T. Schick, J. Dwivedi-Yu, R. Dess `ƒ±, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, ‚ÄúToolformer: Language models\\ncan teach themselves to use tools,‚Äù 2023.\\n[170] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer,\\nand M. T. Ribeiro, ‚ÄúArt: Automatic multi-step reasoning and tool-use\\nfor large language models,‚Äù 2023.\\n[171] Y . Shen, K. Song, X. Tan, D. Li, W. Lu, and Y . Zhuang, ‚ÄúHugginggpt:\\nSolving ai tasks with chatgpt and its friends in huggingface,‚Äù arXiv\\npreprint arXiv:2303.17580 , 2023.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 40}, page_content='[172] Z. Xi, W. Chen, X. Guo, W. He, Y . Ding, B. Hong, M. Zhang, J. Wang,\\nS. Jin, E. Zhou et al. , ‚ÄúThe rise and potential of large language model\\nbased agents: A survey,‚Äù arXiv preprint arXiv:2309.07864 , 2023.\\n[173] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\\nJ. Tang, X. Chen, Y . Lin et al. , ‚ÄúA survey on large language model\\nbased autonomous agents,‚Äù arXiv preprint arXiv:2308.11432 , 2023.\\n[174] Z. Durante, Q. Huang, N. Wake, R. Gong, J. S. Park, B. Sarkar,\\nR. Taori, Y . Noda, D. Terzopoulos, Y . Choi, K. Ikeuchi, H. V o, L. Fei-\\nFei, and J. Gao, ‚ÄúAgent ai: Surveying the horizons of multimodal\\ninteraction,‚Äù arXiv preprint arXiv:2401.03568 , 2024.\\n[175] B. Xu, Z. Peng, B. Lei, S. Mukherjee, Y . Liu, and D. Xu, ‚ÄúRewoo:\\nDecoupling reasoning from observations for efficient augmented lan-\\nguage models,‚Äù 2023.\\n[176] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao,\\n‚ÄúReact: Synergizing reasoning and acting in language models,‚Äù 2023.\\n[177] V . Nair, E. Schumacher, G. Tso, and A. Kannan, ‚ÄúDera: Enhanc-\\ning large language model completions with dialog-enabled resolving\\nagents,‚Äù 2023.\\n[178] Y . Chang, X. Wang, J. Wang, Y . Wu, L. Yang, K. Zhu, H. Chen, X. Yi,\\nC. Wang, Y . Wang, W. Ye, Y . Zhang, Y . Chang, P. S. Yu, Q. Yang,\\nand X. Xie, ‚ÄúA survey on evaluation of large language models,‚Äù 2023.\\n[179] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova,\\nL. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit,\\nQ. Le, and S. Petrov, ‚ÄúNatural questions: A benchmark for\\nquestion answering research,‚Äù Transactions of the Association for\\nComputational Linguistics , vol. 7, pp. 452‚Äì466, 2019. [Online].\\nAvailable: https://aclanthology.org/Q19-1026\\n[180] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, ‚ÄúMeasuring massive multitask language understanding,‚Äù\\n2021.\\n[181] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. Cai, M. Terry, Q. Le et al. , ‚ÄúProgram synthesis with large\\nlanguage models,‚Äù arXiv preprint arXiv:2108.07732 , 2021.\\n[182] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y . Choi, P. Liang,\\nand L. Zettlemoyer, ‚ÄúQuAC: Question answering in context,‚Äù in\\nProceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing , E. Riloff, D. Chiang, J. Hockenmaier, and\\nJ. Tsujii, Eds. Brussels, Belgium: Association for Computational\\nLinguistics, Oct.-Nov. 2018, pp. 2174‚Äì2184. [Online]. Available:\\nhttps://aclanthology.org/D18-1241\\n[183] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\\nC. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, ‚ÄúMeasuring\\ncoding challenge competence with apps,‚Äù NeurIPS , 2021.\\n[184] V . Zhong, C. Xiong, and R. Socher, ‚ÄúSeq2sql: Generating structured\\nqueries from natural language using reinforcement learning,‚Äù arXiv\\npreprint arXiv:1709.00103 , 2017.\\n[185] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, ‚ÄúTriviaQA:\\nA large scale distantly supervised challenge dataset for reading\\ncomprehension,‚Äù in Proceedings of the 55th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\\nR. Barzilay and M.-Y . Kan, Eds. Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601‚Äì1611. [Online].\\nAvailable: https://aclanthology.org/P17-1147\\n[186] G. Lai, Q. Xie, H. Liu, Y . Yang, and E. Hovy, ‚ÄúRACE: Large-scale\\nReAding comprehension dataset from examinations,‚Äù in Proceedings\\nof the 2017 Conference on Empirical Methods in Natural Language\\nProcessing , M. Palmer, R. Hwa, and S. Riedel, Eds. Copenhagen,\\nDenmark: Association for Computational Linguistics, Sep. 2017, pp.\\n785‚Äì794. [Online]. Available: https://aclanthology.org/D17-1082\\n[187] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, ‚ÄúSQuAD: 100,000+\\nquestions for machine comprehension of text,‚Äù in Proceedings of\\nthe 2016 Conference on Empirical Methods in Natural Language\\nProcessing , J. Su, K. Duh, and X. Carreras, Eds. Austin, Texas:\\nAssociation for Computational Linguistics, Nov. 2016, pp. 2383‚Äì2392.\\n[Online]. Available: https://aclanthology.org/D16-1264\\n[188] C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and\\nK. Toutanova, ‚ÄúBoolq: Exploring the surprising difficulty of natural\\nyes/no questions,‚Äù CoRR , vol. abs/1905.10044, 2019. [Online].\\nAvailable: http://arxiv.org/abs/1905.10044[189] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth,\\n‚ÄúLooking beyond the surface:a challenge set for reading compre-\\nhension over multiple sentences,‚Äù in Proceedings of North American\\nChapter of the Association for Computational Linguistics (NAACL) ,\\n2018.\\n[190] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and\\nJ. Schulman, ‚ÄúTraining verifiers to solve math word problems,‚Äù\\nCoRR , vol. abs/2110.14168, 2021. [Online]. Available: https:\\n//arxiv.org/abs/2110.14168\\n[191] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,\\nD. Song, and J. Steinhardt, ‚ÄúMeasuring mathematical problem solving\\nwith the MATH dataset,‚Äù CoRR , vol. abs/2103.03874, 2021. [Online].\\nAvailable: https://arxiv.org/abs/2103.03874\\n[192] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, ‚ÄúHellaswag:\\nCan a machine really finish your sentence?‚Äù 2019.\\n[193] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, ‚ÄúThink you have solved question answering? try\\narc, the AI2 reasoning challenge,‚Äù CoRR , vol. abs/1803.05457, 2018.\\n[Online]. Available: http://arxiv.org/abs/1803.05457\\n[194] Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi, ‚ÄúPIQA:\\nreasoning about physical commonsense in natural language,‚Äù CoRR ,\\nvol. abs/1911.11641, 2019. [Online]. Available: http://arxiv.org/abs/\\n1911.11641\\n[195] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y . Choi, ‚ÄúSocialiqa:\\nCommonsense reasoning about social interactions,‚Äù CoRR , vol.\\nabs/1904.09728, 2019. [Online]. Available: http://arxiv.org/abs/1904.\\n09728\\n[196] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, ‚ÄúCan a suit of\\narmor conduct electricity? A new dataset for open book question\\nanswering,‚Äù CoRR , vol. abs/1809.02789, 2018. [Online]. Available:\\nhttp://arxiv.org/abs/1809.02789\\n[197] S. Lin, J. Hilton, and O. Evans, ‚ÄúTruthfulqa: Measuring how models\\nmimic human falsehoods,‚Äù arXiv preprint arXiv:2109.07958 , 2021.\\n[198] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. W. Cohen, R. Salakhutdinov,\\nand C. D. Manning, ‚ÄúHotpotqa: A dataset for diverse, explainable\\nmulti-hop question answering,‚Äù CoRR , vol. abs/1809.09600, 2018.\\n[Online]. Available: http://arxiv.org/abs/1809.09600\\n[199] Y . Zhuang, Y . Yu, K. Wang, H. Sun, and C. Zhang, ‚ÄúToolqa: A\\ndataset for llm question answering with external tools,‚Äù arXiv preprint\\narXiv:2306.13304 , 2023.\\n[200] D. Chen, J. Bolton, and C. D. Manning, ‚ÄúA thorough examination\\nof the cnn/daily mail reading comprehension task,‚Äù in Association for\\nComputational Linguistics (ACL) , 2016.\\n[201] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang et al. , ‚ÄúAbstractive text\\nsummarization using sequence-to-sequence rnns and beyond,‚Äù arXiv\\npreprint arXiv:1602.06023 , 2016.\\n[202] Y . Bai and D. Z. Wang, ‚ÄúMore than reading comprehension: A survey\\non datasets and metrics of textual question answering,‚Äù arXiv preprint\\narXiv:2109.12264 , 2021.\\n[203] H.-Y . Huang, E. Choi, and W.-t. Yih, ‚ÄúFlowqa: Grasping flow in\\nhistory for conversational machine comprehension,‚Äù arXiv preprint\\narXiv:1810.06683 , 2018.\\n[204] S. Lee, J. Lee, H. Moon, C. Park, J. Seo, S. Eo, S. Koo, and H. Lim, ‚ÄúA\\nsurvey on evaluation metrics for machine translation,‚Äù Mathematics ,\\nvol. 11, no. 4, p. 1006, 2023.\\n[205] J. Li, X. Cheng, W. X. Zhao, J.-Y . Nie, and J.-R. Wen, ‚ÄúHalueval:\\nA large-scale hallucination evaluation benchmark for large language\\nmodels,‚Äù in Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing , 2023, pp. 6449‚Äì6464.\\n[206] Simon Mark Hughes, ‚ÄúHughes hallucination evaluation model\\n(hhem) leaderboard,‚Äù 2024, https://huggingface.co/spaces/vectara/\\nHallucination-evaluation-leaderboard, Last accessed on 2024-01-21.\\n[207] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and\\nR. McHardy, ‚ÄúChallenges and applications of large language models,‚Äù\\narXiv preprint arXiv:2307.10169 , 2023.\\n[208] S. Gunasekar, Y . Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno,\\nS. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi et al. ,\\n‚ÄúTextbooks are all you need,‚Äù arXiv preprint arXiv:2306.11644 , 2023.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 41}, page_content='[209] Y . Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y . T.\\nLee, ‚ÄúTextbooks are all you need ii: phi-1.5 technical report,‚Äù arXiv\\npreprint arXiv:2309.05463 , 2023.\\n[210] M. Poli, S. Massaroli, E. Nguyen, D. Y . Fu, T. Dao, S. Baccus,\\nY . Bengio, S. Ermon, and C. R ¬¥e, ‚ÄúHyena hierarchy: Towards larger\\nconvolutional language models,‚Äù 2023.\\n[211] M. Poli, J. Wang, S. Massaroli, J. Quesnelle, E. Nguyen, and\\nA. Thomas, ‚ÄúStripedHyena: Moving Beyond Transformers with\\nHybrid Signal Processing Models,‚Äù 12 2023. [Online]. Available:\\nhttps://github.com/togethercomputer/stripedhyena\\n[212] D. Y . Fu, S. Arora, J. Grogan, I. Johnson, S. Eyuboglu, A. W. Thomas,\\nB. Spector, M. Poli, A. Rudra, and C. R ¬¥e, ‚ÄúMonarch mixer: A simple\\nsub-quadratic gemm-based architecture,‚Äù 2023.\\n[213] G. J. McLachlan, S. X. Lee, and S. I. Rathnayake, ‚ÄúFinite mixture\\nmodels,‚Äù Annual review of statistics and its application , vol. 6, pp.\\n355‚Äì378, 2019.\\n[214] H. Liu, C. Li, Q. Wu, and Y . J. Lee, ‚ÄúVisual instruction tuning,‚Äù arXiv\\npreprint arXiv:2304.08485 , 2023.\\n[215] S. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou,\\nJ. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, ‚ÄúLlava-plus:\\nLearning to use tools for creating multimodal agents,‚Äù arXiv preprint\\narXiv:2311.05437 , 2023.\\n[216] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, ‚ÄúNext-gpt: Any-to-any\\nmultimodal llm,‚Äù arXiv preprint arXiv:2309.05519 , 2023.\\n[217] N. N. Khasmakhi, M. Asgari-Chenaghlu, N. Asghar, P. Schaer, and\\nD. Z ¬®uhlke, ‚ÄúConvgenvismo: Evaluation of conversational generative\\nvision models,‚Äù 2023.\\n[218] N. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya, M. Harman,\\nI. Harper, A. Marginean, S. Sengupta, and E. Wang, ‚ÄúAutomated unit\\ntest improvement using large language models at meta,‚Äù arXiv preprint\\narXiv:2402.09171 , 2024.\\n[219] L. Sun, Y . Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y . Huang,\\nW. Lyu, Y . Zhang, X. Li et al. , ‚ÄúTrustllm: Trustworthiness in large\\nlanguage models,‚Äù arXiv preprint arXiv:2401.05561 , 2024.\\n[220] Microsoft. Deepspeed. [Online]. Available: https://github.com/\\nmicrosoft/DeepSpeed\\n[221] HuggingFace. Transformers. [Online]. Available: https://github.com/\\nhuggingface/transformers\\n[222] Nvidia. Megatron. [Online]. Available: https://github.com/NVIDIA/\\nMegatron-LM\\n[223] BMTrain. Bmtrain. [Online]. Available: https://github.com/OpenBMB/\\nBMTrain\\n[224] EleutherAI. gpt-neox. [Online]. Available: https://github.com/\\nEleutherAI/gpt-neox\\n[225] microsoft. Lora. [Online]. Available: https://github.com/microsoft/\\nLoRA\\n[226] ColossalAI. Colossalai. [Online]. Available: https://github.com/\\nhpcaitech/ColossalAI\\n[227] FastChat. Fastchat. [Online]. Available: https://github.com/lm-sys/\\nFastChat\\n[228] skypilot. skypilot. [Online]. Available: https://github.com/skypilot-org/\\nskypilot\\n[229] vllm. vllm. [Online]. Available: https://github.com/vllm-project/vllm\\n[230] huggingface. text-generation-inference. [Online]. Available: https:\\n//github.com/huggingface/text-generation-inference\\n[231] langchain. langchain. [Online]. Available: https://github.com/\\nlangchain-ai/langchain\\n[232] bentoml. Openllm. [Online]. Available: https://github.com/bentoml/\\nOpenLLM\\n[233] embedchain. embedchain. [Online]. Available: https://github.com/\\nembedchain/embedchain\\n[234] microsoft. autogen. [Online]. Available: https://github.com/microsoft/\\nautogen\\n[235] babyagi. babyagi. [Online]. Available: https://github.com/\\nyoheinakajima/babyagi\\n[236] guidance. guidance. [Online]. Available: https://github.com/\\nguidance-ai/guidance[237] prompttools. prompttools. [Online]. Available: https://github.com/\\nhegelai/prompttools\\n[238] promptfoo. promptfoo. [Online]. Available: https://github.com/\\npromptfoo/promptfoo\\n[239] facebook. faiss. [Online]. Available: https://github.com/\\nfacebookresearch/faiss\\n[240] milvus. milvus. [Online]. Available: https://github.com/milvus-io/\\nmilvus\\n[241] qdrant. qdrant. [Online]. Available: https://github.com/qdrant/qdrant\\n[242] weaviate. weaviate. [Online]. Available: https://github.com/weaviate/\\nweaviate\\n[243] llama index. llama-index. [Online]. Available: https://github.com/\\nrun-llama/llama index\\nAPPENDIX\\n1. Open Source Toolkits For LLM Development and\\nDeployment\\nThere are various frameworks and libraries developed for\\nLLM training, evaluation, and deployment, and covering every\\nsingle framework is out of this paper‚Äôs scope. But we try to\\nprovide a brief introduction of some of the most popular ones,\\ngrouped into different categories.\\nA. LLM Training/Inference Frameworks\\nSome of the popular frameworks which are useful for LLM\\ntraining includes (note that some of them can be used beyond\\nLLM training too):\\nDeepSpeed [220] is a deep learning optimization library\\nthat makes distributed training and inference easy, efficient,\\nand effective. DeepSpeed enables world‚Äôs most powerful lan-\\nguage models like MT-530B and BLOOM. It is an easy-\\nto-use deep learning optimization software suite that powers\\nunprecedented scale and speed for both training and inference.\\nWith DeepSpeed you can:\\nTransformers [221] is library by HuggingFace which\\nprovides thousands of pretrained models to perform tasks on\\ndifferent modalities such as text, vision, and audio. Using\\npretrained models one can reduce compute costs, carbon\\nfootprint, and save the time and resources required to train\\na model from scratch.\\nMegatron-LM [222] is a large, powerful transformer\\ndeveloped by the Applied Deep Learning Research team\\nat NVIDIA. It contains efficient, model-parallel (tensor, se-\\nquence, and pipeline), and multi-node pre-training of trans-\\nformer based models such as GPT, BERT, and T5 using mixed\\nprecision.\\nBMTrain [223] is an efficient large model training toolkit\\nthat can be used to train large models with tens of billions of\\nparameters. It can train models in a distributed manner while\\nkeeping the code as simple as stand-alone training.\\nGPT-NeoX [224] leverages many of the same features and\\ntechnologies as the popular Megatron-DeepSpeed library but\\nwith substantially increased usability and novel optimizations.\\nLoRA [225] library provides the support for Low-Rank\\nAdaptation of Large Language Models. It reduces the number\\nof trainable parameters by learning pairs of rank-decompostion\\nmatrices while freezing the original weights. This vastly'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Survey_Large_LanguageModel.pdf', 'page': 42}, page_content='reduces the storage requirement for large language models\\nadapted to specific tasks and enables efficient task-switching\\nduring deployment all without introducing inference latency.\\nLoRA also outperforms several other adaptation methods in-\\ncluding adapter, prefix-tuning, and fine-tuning.\\nColossalAI library [226] provides a collection of parallel\\ncomponents. It aims to support developers to write their\\ndistributed deep learning models just like how they write their\\nmodel on their laptop. They provide user-friendly tools to\\nkickstart distributed training and inference in a few lines. In\\nterms of Parallelism strategies, they support: Data Parallelism,\\nPipeline Parallelism, Sequence Parallelism, Zero Redundancy\\nOptimizer (ZeRO) [140], and Auto-Parallelism.\\nB. Deployment Tools\\nWe provide an overview of some of the most popular LLM\\ndeployment tools here.\\nFastChat [227] is an open platform for training, serv-\\ning, and evaluating large language model based chatbots.\\nFastChat‚Äôs core features include: The training and evaluation\\ncode for state-of-the-art models (e.g., Vicuna, MT-Bench), and\\na distributed multi-model serving system with web UI and\\nOpenAI-compatible RESTful APIs.\\nSkypilot [228] is a framework for running LLMs, AI,\\nand batch jobs on any cloud, offering maximum cost savings,\\nhighest GPU availability, and managed execution.\\nvLLM [229] is a fast and easy-to-use library for LLM in-\\nference and serving. vLLM seamlessly supports many Hugging\\nFace models, including the following architectures: Aquila,\\nBaichuan, BLOOM, ChatGLM, DeciLM, Falcon, GPT Big-\\nCode, LLaMA, LLaMA 2, Mistral, Mixtral, MPT, OPT, Qwen,\\nYi, and many more.\\ntext-generation-inference [230] is a toolkit for deploying\\nand serving Large Language Models (LLMs). TGI enables\\nhigh-performance text generation for the most popular open-\\nsource LLMs, including Llama, Falcon, StarCoder, BLOOM,\\nGPT-NeoX, and more.\\nLangChain [231] is a framework for developing applica-\\ntions powered by language models. It enables applications that:\\n‚Ä¢ Are context-aware: connect a language model to\\nsources of context (prompt instructions, few shot ex-\\namples, content to ground its response in, etc.)\\n‚Ä¢ Reason: rely on a language model to reason (about\\nhow to answer based on provided context, what ac-\\ntions to take, etc.)\\nOpenLLM [232] is an open-source platform designed to\\nfacilitate the deployment and operation of large language mod-\\nels (LLMs) in real-world applications. With OpenLLM, you\\ncan run inference on any open-source LLM, deploy them on\\nthe cloud or on-premises, and build powerful AI applications.\\nEmbedchain [233] is an Open Source RAG Framework\\nthat makes it easy to create and deploy AI apps. Embedchain\\nstreamlines the creation of RAG applications, offering a seam-\\nless process for managing various types of unstructured data.\\nIt efficiently segments data into manageable chunks, generatesrelevant embeddings, and stores them in a vector database for\\noptimized retrieval.\\nAutogen [234] is a framework that enables the devel-\\nopment of LLM applications using multiple agents that can\\nconverse with each other to solve tasks. AutoGen agents\\nare customizable, conversable, and seamlessly allow human\\nparticipation. They can operate in various modes that employ\\ncombinations of LLMs, human inputs, and tools.\\nBabyAGI [235] is an autonomous Artificial Intelligence\\nagent, that is designed to generate and execute tasks based on\\ngiven objectives. It harnesses cutting-edge technologies from\\nOpenAI, Pinecone, LangChain, and Chroma to automate tasks\\nand achieve specific goals. In this blog post, we will dive\\ninto the unique features of BabyAGI and explore how it can\\nstreamline task automation.\\nC. Prompting Libraries\\nGuidance [236] is a programming paradigm that offers\\nsuperior control and efficiency compared to conventional\\nprompting and chaining. It allows users to constrain generation\\n(e.g. with regex and CFGs) as well as to interleave control\\n(conditional, loops) and generation seamlessly.\\nPromptTools [237] offers a set of open-source, self-\\nhostable tools for experimenting with, testing, and evaluating\\nLLMs, vector databases, and prompts. The core idea is to\\nenable developers to evaluate using familiar interfaces like\\ncode, notebooks, and a local playground.\\nPromptBench [?] is a Pytorch-based Python package for\\nEvaluation of Large Language Models (LLMs). It provides\\nuser-friendly APIs for researchers to conduct evaluation on\\nLLMs.\\nPromptfoo [238] is a tool for testing and evaluating LLM\\noutput quality. It systematically test prompts, models, and\\nRAGs with predefined test cases.\\nD. VectorDB\\nFaiss [239] is a library developed by Facebook AI Re-\\nsearch that provides efficient similarity search and clustering\\nof dense vectors. It is designed for use with large-scale,\\nhigh-dimensional data and supports several index types and\\nalgorithms for various use cases.\\nMilvus [240] is an open-source vector database built to\\npower embedding similarity search and AI applications. Mil-\\nvus makes unstructured data search more accessible, and pro-\\nvides a consistent user experience regardless of the deployment\\nenvironment.\\nQdrant [241] is a vector similarity search engine and\\nvector database. It provides a production-ready service with a\\nconvenient API to store, search, and manage points‚Äîvectors\\nwith an additional payload Qdrant is tailored to extended\\nfiltering support. environment.\\nWeaviate [242] is an open-source, GraphQL-based vec-\\ntor search engine that enables similarity search on high-\\ndimensional data. While it is open-source, the commercial ver-\\nsion offers additional features, support, and managed services.\\nSome of the other popular options includes LlamaIndex\\n[243] and Pinecone .'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/1703.06870v3.pdf', 'page': 0}, page_content='Mask R-CNN\\nKaiming He Georgia Gkioxari Piotr Doll ¬¥ar Ross Girshick\\nFacebook AI Research (FAIR)\\nAbstract\\nWe present a conceptually simple, Ô¨Çexible, and general\\nframework for object instance segmentation. Our approach\\nefÔ¨Åciently detects objects in an image while simultaneously\\ngenerating a high-quality segmentation mask for each in-\\nstance. The method, called Mask R-CNN, extends Faster\\nR-CNN by adding a branch for predicting an object mask in\\nparallel with the existing branch for bounding box recogni-\\ntion. Mask R-CNN is simple to train and adds only a small\\noverhead to Faster R-CNN, running at 5 fps. Moreover,\\nMask R-CNN is easy to generalize to other tasks, e.g., al-\\nlowing us to estimate human poses in the same framework.\\nWe show top results in all three tracks of the COCO suite\\nof challenges, including instance segmentation, bounding-\\nbox object detection, and person keypoint detection. With-\\nout bells and whistles, Mask R-CNN outperforms all ex-\\nisting, single-model entries on every task, including the\\nCOCO 2016 challenge winners. We hope our simple and\\neffective approach will serve as a solid baseline and help\\nease future research in instance-level recognition. Code\\nhas been made available at: https://github.com/\\nfacebookresearch/Detectron .\\n1. Introduction\\nThe vision community has rapidly improved object de-\\ntection and semantic segmentation results over a short pe-\\nriod of time. In large part, these advances have been driven\\nby powerful baseline systems, such as the Fast/Faster R-\\nCNN [12, 36] and Fully Convolutional Network (FCN) [30]\\nframeworks for object detection and semantic segmenta-\\ntion, respectively. These methods are conceptually intuitive\\nand offer Ô¨Çexibility and robustness, together with fast train-\\ning and inference time. Our goal in this work is to develop a\\ncomparably enabling framework for instance segmentation .\\nInstance segmentation is challenging because it requires\\nthe correct detection of all objects in an image while also\\nprecisely segmenting each instance. It therefore combines\\nelements from the classical computer vision tasks of ob-\\nject detection , where the goal is to classify individual ob-\\njects and localize each using a bounding box, and semantic\\nRoIAlignRoIAlignclass\\nbox\\nconvconv\\n convconvFigure 1. The Mask R-CNN framework for instance segmentation.\\nsegmentation , where the goal is to classify each pixel into\\na Ô¨Åxed set of categories without differentiating object in-\\nstances.1Given this, one might expect a complex method\\nis required to achieve good results. However, we show that\\na surprisingly simple, Ô¨Çexible, and fast system can surpass\\nprior state-of-the-art instance segmentation results.\\nOur method, called Mask R-CNN , extends Faster R-CNN\\n[36] by adding a branch for predicting segmentation masks\\non each Region of Interest (RoI), in parallel with the ex-\\nisting branch for classiÔ¨Åcation and bounding box regres-\\nsion (Figure 1). The mask branch is a small FCN applied\\nto each RoI, predicting a segmentation mask in a pixel-to-\\npixel manner. Mask R-CNN is simple to implement and\\ntrain given the Faster R-CNN framework, which facilitates\\na wide range of Ô¨Çexible architecture designs. Additionally,\\nthe mask branch only adds a small computational overhead,\\nenabling a fast system and rapid experimentation.\\nIn principle Mask R-CNN is an intuitive extension of\\nFaster R-CNN, yet constructing the mask branch properly\\nis critical for good results. Most importantly, Faster R-\\nCNN was not designed for pixel-to-pixel alignment be-\\ntween network inputs and outputs. This is most evident in\\nhow RoIPool [18, 12], the de facto core operation for at-\\ntending to instances, performs coarse spatial quantization\\nfor feature extraction. To Ô¨Åx the misalignment, we pro-\\npose a simple, quantization-free layer, called RoIAlign , that\\nfaithfully preserves exact spatial locations. Despite being\\n1Following common terminology, we use object detection to denote\\ndetection via bounding boxes , not masks, and semantic segmentation to\\ndenote per-pixel classiÔ¨Åcation without differentiating instances. Yet we\\nnote that instance segmentation is both semantic and a form of detection.\\n1arXiv:1703.06870v3  [cs.CV]  24 Jan 2018'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/1703.06870v3.pdf', 'page': 1}, page_content='dining table.96person1.00person1.00 person1.00 person1.00 person1.00person1.00\\nperson1.00person.94\\nbottle.99\\nbottle.99\\nbottle.99\\nmotorcycle1.00motorcycle1.00person1.00\\nperson1.00\\nperson.96person1.00person.83person.96\\nperson.98 person.90person.92person.99person.91\\nbus.99\\nperson1.00\\nperson1.00person1.00\\nbackpack.93person1.00person.99\\nperson1.00\\nbackpack.99\\nperson.99person.98person.89person.95\\nperson1.00\\nperson1.00\\ncar1.00traffic light .96\\nperson.96truck1.00person.99\\ncar.99person.85\\nmotorcycle.95car.99car.92person.99person1.00traffic light .92 traffic light .84traffic light .95\\ncar.93person.87\\nperson1.00\\nperson1.00umbrella.98\\numbrella.98\\nbackpack1.00\\nhandba g.96\\nelephant1.00person1.00person1.00 person.99\\nsheep1. 00person1.00\\nsheep.99sheep.91 sheep1. 00\\nsheep.99sheep.99sheep.95person.99\\nsheep1. 00sheep.96sheep.99sheep.99\\nsheep.96\\nsheep.96sheep.96sheep.86\\nsheep.82sheep.93\\ndining table.99\\nchair.99chair.90\\nchair.99chair.98\\nchair.96chair.86chair.99\\nbowl.81chair.96tv.99\\nbottle.99\\nwine glass.99wine glass1.00bowl.85\\nknife.83wine glass1.00 wine glass.93wine glass.97\\nfork.95Figure 2. Mask R-CNN results on the COCO test set. These results are based on ResNet-101 [19], achieving a mask AP of 35.7 and\\nrunning at 5 fps. Masks are shown in color, and bounding box, category, and conÔ¨Ådences are also shown.\\na seemingly minor change, RoIAlign has a large impact: it\\nimproves mask accuracy by relative 10% to 50%, showing\\nbigger gains under stricter localization metrics. Second, we\\nfound it essential to decouple mask and class prediction: we\\npredict a binary mask for each class independently, without\\ncompetition among classes, and rely on the network‚Äôs RoI\\nclassiÔ¨Åcation branch to predict the category. In contrast,\\nFCNs usually perform per-pixel multi-class categorization,\\nwhich couples segmentation and classiÔ¨Åcation, and based\\non our experiments works poorly for instance segmentation.\\nWithout bells and whistles, Mask R-CNN surpasses all\\nprevious state-of-the-art single-model results on the COCO\\ninstance segmentation task [28], including the heavily-\\nengineered entries from the 2016 competition winner. As\\na by-product, our method also excels on the COCO object\\ndetection task. In ablation experiments, we evaluate multi-\\nple basic instantiations, which allows us to demonstrate its\\nrobustness and analyze the effects of core factors.\\nOur models can run at about 200ms per frame on a GPU,\\nand training on COCO takes one to two days on a single\\n8-GPU machine. We believe the fast train and test speeds,\\ntogether with the framework‚Äôs Ô¨Çexibility and accuracy, will\\nbeneÔ¨Åt and ease future research on instance segmentation.\\nFinally, we showcase the generality of our framework\\nvia the task of human pose estimation on the COCO key-\\npoint dataset [28]. By viewing each keypoint as a one-hot\\nbinary mask, with minimal modiÔ¨Åcation Mask R-CNN can\\nbe applied to detect instance-speciÔ¨Åc poses. Mask R-CNN\\nsurpasses the winner of the 2016 COCO keypoint compe-\\ntition, and at the same time runs at 5 fps. Mask R-CNN,\\ntherefore, can be seen more broadly as a Ô¨Çexible framework\\nforinstance-level recognition and can be readily extended\\nto more complex tasks.\\nWe have released code to facilitate future research.2. Related Work\\nR-CNN: The Region-based CNN (R-CNN) approach [13]\\nto bounding-box object detection is to attend to a manage-\\nable number of candidate object regions [42, 20] and evalu-\\nate convolutional networks [25, 24] independently on each\\nRoI. R-CNN was extended [18, 12] to allow attending to\\nRoIs on feature maps using RoIPool, leading to fast speed\\nand better accuracy. Faster R-CNN [36] advanced this\\nstream by learning the attention mechanism with a Region\\nProposal Network (RPN). Faster R-CNN is Ô¨Çexible and ro-\\nbust to many follow-up improvements ( e.g., [38, 27, 21]),\\nand is the current leading framework in several benchmarks.\\nInstance Segmentation: Driven by the effectiveness of R-\\nCNN, many approaches to instance segmentation are based\\nonsegment proposals . Earlier methods [13, 15, 16, 9] re-\\nsorted to bottom-up segments [42, 2]. DeepMask [33] and\\nfollowing works [34, 8] learn to propose segment candi-\\ndates, which are then classiÔ¨Åed by Fast R-CNN. In these\\nmethods, segmentation precedes recognition, which is slow\\nand less accurate. Likewise, Dai et al. [10] proposed a com-\\nplex multiple-stage cascade that predicts segment proposals\\nfrom bounding-box proposals, followed by classiÔ¨Åcation.\\nInstead, our method is based on parallel prediction of masks\\nand class labels, which is simpler and more Ô¨Çexible.\\nMost recently, Li et al. [26] combined the segment pro-\\nposal system in [8] and object detection system in [11] for\\n‚Äúfully convolutional instance segmentation‚Äù (FCIS). The\\ncommon idea in [8, 11, 26] is to predict a set of position-\\nsensitive output channels fully convolutionally. These\\nchannels simultaneously address object classes, boxes, and\\nmasks, making the system fast. But FCIS exhibits system-\\natic errors on overlapping instances and creates spurious\\nedges (Figure 6), showing that it is challenged by the fun-\\ndamental difÔ¨Åculties of segmenting instances.\\n2'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/1703.06870v3.pdf', 'page': 2}, page_content='Another family of solutions [23, 4, 3, 29] to instance seg-\\nmentation are driven by the success of semantic segmen-\\ntation. Starting from per-pixel classiÔ¨Åcation results ( e.g.,\\nFCN outputs), these methods attempt to cut the pixels of\\nthe same category into different instances. In contrast to the\\nsegmentation-Ô¨Årst strategy of these methods, Mask R-CNN\\nis based on an instance-Ô¨Årst strategy. We expect a deeper in-\\ncorporation of both strategies will be studied in the future.\\n3. Mask R-CNN\\nMask R-CNN is conceptually simple: Faster R-CNN has\\ntwo outputs for each candidate object, a class label and a\\nbounding-box offset; to this we add a third branch that out-\\nputs the object mask. Mask R-CNN is thus a natural and in-\\ntuitive idea. But the additional mask output is distinct from\\nthe class and box outputs, requiring extraction of much Ô¨Åner\\nspatial layout of an object. Next, we introduce the key ele-\\nments of Mask R-CNN, including pixel-to-pixel alignment,\\nwhich is the main missing piece of Fast/Faster R-CNN.\\nFaster R-CNN: We begin by brieÔ¨Çy reviewing the Faster\\nR-CNN detector [36]. Faster R-CNN consists of two stages.\\nThe Ô¨Årst stage, called a Region Proposal Network (RPN),\\nproposes candidate object bounding boxes. The second\\nstage, which is in essence Fast R-CNN [12], extracts fea-\\ntures using RoIPool from each candidate box and performs\\nclassiÔ¨Åcation and bounding-box regression. The features\\nused by both stages can be shared for faster inference. We\\nrefer readers to [21] for latest, comprehensive comparisons\\nbetween Faster R-CNN and other frameworks.\\nMask R-CNN: Mask R-CNN adopts the same two-stage\\nprocedure, with an identical Ô¨Årst stage (which is RPN). In\\nthe second stage, in parallel to predicting the class and box\\noffset, Mask R-CNN also outputs a binary mask for each\\nRoI. This is in contrast to most recent systems, where clas-\\nsiÔ¨Åcation depends on mask predictions ( e.g. [33, 10, 26]).\\nOur approach follows the spirit of Fast R-CNN [12] that\\napplies bounding-box classiÔ¨Åcation and regression in par-\\nallel (which turned out to largely simplify the multi-stage\\npipeline of original R-CNN [13]).\\nFormally, during training, we deÔ¨Åne a multi-task loss on\\neach sampled RoI as L=Lcls+Lbox+Lmask . The clas-\\nsiÔ¨Åcation loss Lclsand bounding-box loss Lboxare identi-\\ncal as those deÔ¨Åned in [12]. The mask branch has a Km2-\\ndimensional output for each RoI, which encodes Kbinary\\nmasks of resolution m√óm, one for each of the Kclasses.\\nTo this we apply a per-pixel sigmoid, and deÔ¨Åne Lmask as\\nthe average binary cross-entropy loss. For an RoI associated\\nwith ground-truth class k,Lmask is only deÔ¨Åned on the k-th\\nmask (other mask outputs do not contribute to the loss).\\nOur deÔ¨Ånition of Lmask allows the network to generate\\nmasks for every class without competition among classes;\\nwe rely on the dedicated classiÔ¨Åcation branch to predict the\\nFigure 3. RoIAlign: The dashed grid rep-\\nresents a feature map, the solid lines an RoI\\n(with 2√ó2 bins in this example), and the dots\\nthe 4 sampling points in each bin. RoIAlign\\ncomputes the value of each sampling point\\nby bilinear interpolation from the nearby grid\\npoints on the feature map. No quantization is\\nperformed on any coordinates involved in the\\nRoI, its bins, or the sampling points.\\nclass label used to select the output mask. This decouples\\nmask and class prediction. This is different from common\\npractice when applying FCNs [30] to semantic segmenta-\\ntion, which typically uses a per-pixel softmax and a multino-\\nmial cross-entropy loss. In that case, masks across classes\\ncompete; in our case, with a per-pixel sigmoid and a binary\\nloss, they do not. We show by experiments that this formu-\\nlation is key for good instance segmentation results.\\nMask Representation: A mask encodes an input object‚Äôs\\nspatial layout. Thus, unlike class labels or box offsets\\nthat are inevitably collapsed into short output vectors by\\nfully-connected ( fc) layers, extracting the spatial structure\\nof masks can be addressed naturally by the pixel-to-pixel\\ncorrespondence provided by convolutions.\\nSpeciÔ¨Åcally, we predict an m√ómmask from each RoI\\nusing an FCN [30]. This allows each layer in the mask\\nbranch to maintain the explicit m√ómobject spatial lay-\\nout without collapsing it into a vector representation that\\nlacks spatial dimensions. Unlike previous methods that re-\\nsort to fclayers for mask prediction [33, 34, 10], our fully\\nconvolutional representation requires fewer parameters, and\\nis more accurate as demonstrated by experiments.\\nThis pixel-to-pixel behavior requires our RoI features,\\nwhich themselves are small feature maps, to be well aligned\\nto faithfully preserve the explicit per-pixel spatial corre-\\nspondence. This motivated us to develop the following\\nRoIAlign layer that plays a key role in mask prediction.\\nRoIAlign: RoIPool [12] is a standard operation for extract-\\ning a small feature map ( e.g., 7√ó7) from each RoI. RoIPool\\nÔ¨Årstquantizes a Ô¨Çoating-number RoI to the discrete granu-\\nlarity of the feature map, this quantized RoI is then subdi-\\nvided into spatial bins which are themselves quantized, and\\nÔ¨Ånally feature values covered by each bin are aggregated\\n(usually by max pooling). Quantization is performed, e.g.,\\non a continuous coordinate xby computing [x/16], where\\n16 is a feature map stride and [¬∑]is rounding; likewise, quan-\\ntization is performed when dividing into bins ( e.g., 7√ó7).\\nThese quantizations introduce misalignments between the\\nRoI and the extracted features. While this may not impact\\nclassiÔ¨Åcation, which is robust to small translations, it has a\\nlarge negative effect on predicting pixel-accurate masks.\\nTo address this, we propose an RoIAlign layer that re-\\nmoves the harsh quantization of RoIPool, properly aligning\\nthe extracted features with the input. Our proposed change\\nis simple: we avoid any quantization of the RoI boundaries\\n3'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/1703.06870v3.pdf', 'page': 3}, page_content='or bins ( i.e., we use x/16instead of [x/16]). We use bi-\\nlinear interpolation [22] to compute the exact values of the\\ninput features at four regularly sampled locations in each\\nRoI bin, and aggregate the result (using max or average),\\nsee Figure 3 for details. We note that the results are not sen-\\nsitive to the exact sampling locations, or how many points\\nare sampled, as long as no quantization is performed.\\nRoIAlign leads to large improvements as we show in\\n¬ß4.2. We also compare to the RoIWarp operation proposed\\nin [10]. Unlike RoIAlign, RoIWarp overlooked the align-\\nment issue and was implemented in [10] as quantizing RoI\\njust like RoIPool. So even though RoIWarp also adopts\\nbilinear resampling motivated by [22], it performs on par\\nwith RoIPool as shown by experiments (more details in Ta-\\nble 2c), demonstrating the crucial role of alignment.\\nNetwork Architecture: To demonstrate the generality of\\nour approach, we instantiate Mask R-CNN with multiple\\narchitectures. For clarity, we differentiate between: (i) the\\nconvolutional backbone architecture used for feature ex-\\ntraction over an entire image, and (ii) the network head\\nfor bounding-box recognition (classiÔ¨Åcation and regression)\\nand mask prediction that is applied separately to each RoI.\\nWe denote the backbone architecture using the nomen-\\nclature network-depth-features . We evaluate ResNet [19]\\nand ResNeXt [45] networks of depth 50 or 101 layers. The\\noriginal implementation of Faster R-CNN with ResNets\\n[19] extracted features from the Ô¨Ånal convolutional layer\\nof the 4-th stage, which we call C4. This backbone with\\nResNet-50, for example, is denoted by ResNet-50-C4. This\\nis a common choice used in [19, 10, 21, 39].\\nWe also explore another more effective backbone re-\\ncently proposed by Lin et al. [27], called a Feature Pyra-\\nmid Network (FPN). FPN uses a top-down architecture with\\nlateral connections to build an in-network feature pyramid\\nfrom a single-scale input. Faster R-CNN with an FPN back-\\nbone extracts RoI features from different levels of the fea-\\nture pyramid according to their scale, but otherwise the\\nrest of the approach is similar to vanilla ResNet. Using a\\nResNet-FPN backbone for feature extraction with Mask R-\\nCNN gives excellent gains in both accuracy and speed. For\\nfurther details on FPN, we refer readers to [27].\\nFor the network head we closely follow architectures\\npresented in previous work to which we add a fully con-\\nvolutional mask prediction branch. SpeciÔ¨Åcally, we ex-\\ntend the Faster R-CNN box heads from the ResNet [19]\\nand FPN [27] papers. Details are shown in Figure 4. The\\nhead on the ResNet-C4 backbone includes the 5-th stage of\\nResNet (namely, the 9-layer ‚Äòres5‚Äô [19]), which is compute-\\nintensive. For FPN, the backbone already includes res5 and\\nthus allows for a more efÔ¨Åcient head that uses fewer Ô¨Ålters.\\nWe note that our mask branches have a straightforward\\nstructure. More complex designs have the potential to im-\\nprove performance but are not the focus of this work.\\nave\\nRoI\\nRoI14√ó14\\n√ó2567√ó7\\n√ó256\\n14√ó14\\n√ó2561024\\n28√ó28\\n√ó2561024\\nmask14√ó14\\n√ó256class\\nbox2048RoI res57√ó7\\n√ó10247√ó7\\n√ó2048\\n√ó4class\\nbox\\n14√ó14\\n√ó80\\nmask28√ó28\\n√ó80Faster R-CNN\\nw/ ResNet [19]Faster R-CNN\\nw/ FPN [27]\\nFigure 4. Head Architecture : We extend two existing Faster R-\\nCNN heads [19, 27]. Left/Right panels show the heads for the\\nResNet C4 and FPN backbones, from [19] and [27], respectively,\\nto which a mask branch is added. Numbers denote spatial resolu-\\ntion and channels. Arrows denote either conv, deconv, or fclayers\\nas can be inferred from context (conv preserves spatial dimension\\nwhile deconv increases it). All convs are 3 √ó3, except the output\\nconv which is 1 √ó1, deconvs are 2 √ó2 with stride 2, and we use\\nReLU [31] in hidden layers. Left: ‚Äòres5‚Äô denotes ResNet‚Äôs Ô¨Åfth\\nstage, which for simplicity we altered so that the Ô¨Årst conv oper-\\nates on a 7 √ó7 RoI with stride 1 (instead of 14 √ó14 / stride 2 as in\\n[19]). Right : ‚Äò√ó4‚Äô denotes a stack of four consecutive convs.\\n3.1. Implementation Details\\nWe set hyper-parameters following existing Fast/Faster\\nR-CNN work [12, 36, 27]. Although these decisions were\\nmade for object detection in original papers [12, 36, 27], we\\nfound our instance segmentation system is robust to them.\\nTraining: As in Fast R-CNN, an RoI is considered positive\\nif it has IoU with a ground-truth box of at least 0.5 and\\nnegative otherwise. The mask loss Lmask is deÔ¨Åned only on\\npositive RoIs. The mask target is the intersection between\\nan RoI and its associated ground-truth mask.\\nWe adopt image-centric training [12]. Images are resized\\nsuch that their scale (shorter edge) is 800 pixels [27]. Each\\nmini-batch has 2 images per GPU and each image has N\\nsampled RoIs, with a ratio of 1:3 of positive to negatives\\n[12]. Nis 64 for the C4 backbone (as in [12, 36]) and 512\\nfor FPN (as in [27]). We train on 8 GPUs (so effective mini-\\nbatch size is 16) for 160k iterations, with a learning rate of\\n0.02 which is decreased by 10 at the 120k iteration. We\\nuse a weight decay of 0.0001 and momentum of 0.9. With\\nResNeXt [45], we train with 1 image per GPU and the same\\nnumber of iterations, with a starting learning rate of 0.01.\\nThe RPN anchors span 5 scales and 3 aspect ratios, fol-\\nlowing [27]. For convenient ablation, RPN is trained sep-\\narately and does not share features with Mask R-CNN, un-\\nless speciÔ¨Åed. For every entry in this paper, RPN and Mask\\nR-CNN have the same backbones and so they are shareable.\\nInference: At test time, the proposal number is 300 for the\\nC4 backbone (as in [36]) and 1000 for FPN (as in [27]). We\\nrun the box prediction branch on these proposals, followed\\nby non-maximum suppression [14]. The mask branch is\\nthen applied to the highest scoring 100 detection boxes. Al-\\nthough this differs from the parallel computation used in\\ntraining, it speeds up inference and improves accuracy (due\\nto the use of fewer, more accurate RoIs). The mask branch\\n4'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/1703.06870v3.pdf', 'page': 4}, page_content='horse1.00horse1.00horse1.00\\nbus1.00bus1.00\\ncar.98truck.88\\ncar.93\\ncar.78car.98\\ncar.91 car.96car.99\\ncar.94car.99car.98 truck.86\\ncar.99car.95car1.00car.93 car.98car.95\\ncar.97car.87\\ncar.99\\ncar.82car.78car.93\\ncar.95\\ncar.97\\nperson.99traffic light .73\\nperson1.00\\nperson.99person.95person.93\\nperson.93\\nperson1.00person.98\\nskateboard.82\\nsuitcase1.00\\nsuitcase.99suitcase.96suitcase1.00\\nsuitcase.93suitcase.98\\nsuitcase.88suitcase.72stop sign.88\\nperson1.00 person1.00\\nperson1.00person1.00\\nperson.99person.99\\nbench.76skateboard.91\\nskateboard.83handba g.81\\nsurfboard1.00person1.00person1.00 surfboard1.00person1.00person.98\\nsurfboard1.00person1.00\\nsurfboard.98surfboard1.00person.91\\nperson.74\\nperson1.00person1.00\\nperson1.00person1.00person1.00 person1.00 person.98person.99\\nperson1.00person.99 umbrella1.00\\nperson.95umbrella.99umbrella.97umbrella.97\\numbrella.96\\numbrella1.00\\nbackpack.96umbrella.98\\nbackpack.95person.80\\nbackpack.98\\nbicycle.93umbrella.89person.89handba g.97\\nhandba g.85\\nperson1.00person1.00 person1.00person1.00person1.00person1.00\\nmotorcycle.72kite.89\\nperson.99kite.95\\nperson.99\\nperson1.00person.81person.72kite.93\\nperson.89kite1.00\\nperson.98\\nperson1.00kite.84kite.97\\nperson.80\\nhandba g.80person.99kite.82\\nperson.98 person.96kite.98\\nperson.99person.82kite.81\\nperson.95 person.84kite.98kite.72\\nkite.99kite.84kite.99\\nperson.94person.72person.98kite.95\\nperson.98 person.77kite.73\\nperson.78 person.71 person.87kite.88kite.88\\nperson.94kite.86kite.89\\nzebra.99\\nzebra1.00zebra1.00\\nzebra.99zebra1.00zebra.96\\nzebra.74\\nzebra.96zebra.99zebra.90\\nzebra.88zebra.76\\ndining table.91dining table.78\\nchair.97person.99\\nperson.86\\nchair.94chair.98person.95\\nchair.95person.97\\nchair.92chair.99person.97\\nperson.99person.94 person.99person.87\\nperson.99\\nchair.83person.94person.99person.98\\nchair.87chair.95person.97\\nperson.96\\nchair.99person.86person.89\\nchair.89\\nwine glass.93person.98person.88person.97\\nperson.88person.88\\nperson.91 chair.96person.95\\nperson.77person.92\\nwine glass.94cup.83\\nwine glass.94\\nwine glass.83\\ncup.91chair.85 dining table.96\\nwine glass.91person.96\\ncup.98person.83\\ndining table.75\\ncup.96person.72\\nwine glass.80chair.98person.81person.82\\ndining table.81\\nchair.85chair.78\\ncup.75person.77\\ncup.71wine glass.80cup.79cup.93\\ncup.71\\nperson.99person.99\\nperson1.00person1.00\\nfrisbee1.00\\nperson.80person.82\\nelephant1.00elephant1.00elephant1.00\\nelephant.97\\nelephant.99\\nperson1.00person1.00\\ndining table.95person1.00person.88\\nwine glass1.00bottle.97\\nwine glass1.00wine glass.99tv.98 tv.84\\nperson1.00\\nbench.97person.98person1.00person1.00\\nhandba g.73person.86 potted plant.92\\nbird.93person.76person.98person.78 person.78 backpack.88handba g.91\\ncell phone.77 clock.73\\nperson.99person1.00person.98\\nperson1.00person1.00 person1.00person.99\\nperson.99 person.99 person1.00 person1.00person.98 person.99\\nhandba g.88person1.00 person.98person.92\\nhandba g.99person.97\\nperson.95\\nhandba g.88traffic light .99\\nperson.95\\nperson.87person.95traffic light .87\\ntraffic light .71\\nperson.80person.95 person.95 person.73person.74\\ntie.85\\ncar.99\\ncar.86car.97car1.00 car.95car.97traffic light 1.00traffic light .99\\ncar.99person.99car.95\\ncar.97 car.98car.98\\ncar.91\\ncar1.00car.96car.96\\nbicycle.86car.97car.97\\ncar.97car.94car.95\\ncar.94car.81\\nperson.87\\nparking meter.98car.89\\ndonut1.00donut.90\\ndonut.88donut.81\\ndonut.95\\ndonut.96donut1.00 donut.98\\ndonut.99donut.94donut.97donut.99\\ndonut.98donut1.00\\ndonut.95donut1.00\\ndonut.98donut.98donut.99\\ndonut.96\\ndonut.89donut.96donut.95donut.98donut.89\\ndonut.93donut.95\\ndonut.90donut.89\\ndonut.89donut.89\\ndonut.86donut.86\\nperson1.00person1.00 person1.00\\nperson1.00person1.00person1.00\\nperson1.00\\ndog1.00baseball bat.99\\nbaseball bat.85\\nbaseball bat.98\\ntruck.92\\ntruck.99truck.96truck.99truck.97\\nbus.99truck.93 bus.90\\nperson1.00person1.00horse.77horse.99\\ncow.93person.96person1.00\\nperson.99horse.97\\nperson.98 person.97person.98\\nperson.96\\nperson1.00\\ntennis racket1.00chair.73person.90person.77\\nperson.97\\nperson.81person.87\\nperson.71 person.96 person.99 person.98 person.94chair.97\\nchair.80\\nchair.71chair.94chair.92chair.99chair.93\\nchair.99\\nchair.91 chair.81 chair.98 chair.83chair.81chair.81\\nchair.93\\nsports ball.99\\nperson1.00\\ncouch.82person1.00\\nperson.99person1.00person1.00person1.00 person.99skateboard.99\\nperson.90person.98person.99person.91\\nperson.99person1.00\\nperson.80\\nskateboard.98Figure 5. More results of Mask R-CNN on COCO test images, using ResNet-101-FPN and running at 5 fps, with 35.7 mask AP (Table 1).\\nbackbone AP AP 50 AP75 APS APM APL\\nMNC [10] ResNet-101-C4 24.6 44.3 24.8 4.7 25.9 43.6\\nFCIS [26] +OHEM ResNet-101-C5-dilated 29.2 49.5 - 7.1 31.3 50.0\\nFCIS+++ [26] +OHEM ResNet-101-C5-dilated 33.6 54.5 - - - -\\nMask R-CNN ResNet-101-C4 33.1 54.9 34.8 12.1 35.6 51.1\\nMask R-CNN ResNet-101-FPN 35.7 58.0 37.8 15.5 38.1 52.4\\nMask R-CNN ResNeXt-101-FPN 37.1 60.0 39.4 16.9 39.9 53.5\\nTable 1. Instance segmentation mask AP on COCO test-dev . MNC [10] and FCIS [26] are the winners of the COCO 2015 and 2016\\nsegmentation challenges, respectively. Without bells and whistles, Mask R-CNN outperforms the more complex FCIS+++, which includes\\nmulti-scale train/test, horizontal Ô¨Çip test, and OHEM [38]. All entries are single-model results.\\ncan predict Kmasks per RoI, but we only use the k-th mask,\\nwhere kis the predicted class by the classiÔ¨Åcation branch.\\nThem√ómÔ¨Çoating-number mask output is then resized to\\nthe RoI size, and binarized at a threshold of 0.5.\\nNote that since we only compute masks on the top 100\\ndetection boxes, Mask R-CNN adds a small overhead to its\\nFaster R-CNN counterpart ( e.g.,‚àº20% on typical models).\\n4. Experiments: Instance Segmentation\\nWe perform a thorough comparison of Mask R-CNN to\\nthe state of the art along with comprehensive ablations on\\nthe COCO dataset [28]. We report the standard COCO met-\\nrics including AP (averaged over IoU thresholds), AP 50,\\nAP75, and AP S, AP M, AP L(AP at different scales). Un-\\nless noted, AP is evaluating using mask IoU. As in previous\\nwork [5, 27], we train using the union of 80k train images\\nand a 35k subset of val images ( trainval35k ), and re-\\nport ablations on the remaining 5k val images ( minival ).\\nWe also report results on test-dev [28].4.1. Main Results\\nWe compare Mask R-CNN to the state-of-the-art meth-\\nods in instance segmentation in Table 1. All instantia-\\ntions of our model outperform baseline variants of pre-\\nvious state-of-the-art models. This includes MNC [10]\\nand FCIS [26], the winners of the COCO 2015 and 2016\\nsegmentation challenges, respectively. Without bells and\\nwhistles, Mask R-CNN with ResNet-101-FPN backbone\\noutperforms FCIS+++ [26], which includes multi-scale\\ntrain/test, horizontal Ô¨Çip test, and online hard example min-\\ning (OHEM) [38]. While outside the scope of this work, we\\nexpect many such improvements to be applicable to ours.\\nMask R-CNN outputs are visualized in Figures 2 and 5.\\nMask R-CNN achieves good results even under challeng-\\ning conditions. In Figure 6 we compare our Mask R-CNN\\nbaseline and FCIS+++ [26]. FCIS+++ exhibits systematic\\nartifacts on overlapping instances, suggesting that it is chal-\\nlenged by the fundamental difÔ¨Åculty of instance segmenta-\\ntion. Mask R-CNN shows no such artifacts.\\n5'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/1703.06870v3.pdf', 'page': 5}, page_content='person1.00\\nperson1.00\\nperson1.00person1.00umbrella1.00 umbrella.99\\ncar.99car.93\\ngiraffe1.00 giraffe1.00\\nperson1.00person1.00person1.00person1.00\\nperson.95\\nsports ball1.00sports ball.98person1.00\\nperson1.00\\nperson1.00\\ntie.95\\ntie1.00\\nFCIS Mask R-CNN\\ntrain1.00\\ntrain.99\\ntrain.80\\nperson1.00 person1.00person1.00\\nperson1.00person1.00 person1.00\\nskateboard.98person.99 person.99\\nskateboard.99handba g.93\\nFigure 6. FCIS+++ [26] (top) vs. Mask R-CNN (bottom, ResNet-101-FPN). FCIS exhibits systematic artifacts on overlapping objects.\\nnet-depth-features AP AP 50 AP75\\nResNet-50-C4 30.3 51.2 31.5\\nResNet-101-C4 32.7 54.2 34.3\\nResNet-50-FPN 33.6 55.2 35.3\\nResNet-101-FPN 35.4 57.3 37.5\\nResNeXt-101-FPN 36.7 59.5 38.9\\n(a)Backbone Architecture : Better back-\\nbones bring expected gains: deeper networks\\ndo better, FPN outperforms C4 features, and\\nResNeXt improves on ResNet.AP AP 50 AP75\\nsoftmax 24.8 44.1 25.1\\nsigmoid 30.3 51.2 31.5\\n+5.5 +7.1 +6.4\\n(b)Multinomial vs. Independent Masks\\n(ResNet-50-C4): Decoupling via per-\\nclass binary masks (sigmoid) gives large\\ngains over multinomial masks (softmax).align? bilinear? agg. AP AP 50 AP75\\nRoIPool [12] max 26.9 48.8 26.4\\nRoIWarp [10]‚úì max 27.2 49.2 27.1\\n‚úì ave 27.1 48.9 27.1\\nRoIAlign‚úì ‚úì max 30.2 51.0 31.8\\n‚úì ‚úì ave 30.3 51.2 31.5\\n(c)RoIAlign (ResNet-50-C4): Mask results with various RoI\\nlayers. Our RoIAlign layer improves AP by ‚àº3 points and\\nAP75by‚àº5 points. Using proper alignment is the only fac-\\ntor that contributes to the large gap between RoI layers.\\nAP AP 50 AP75 APbbAPbb\\n50APbb\\n75\\nRoIPool 23.6 46.5 21.6 28.2 52.7 26.9\\nRoIAlign 30.9 51.8 32.1 34.0 55.3 36.4\\n+7.3 + 5.3 +10.5 +5.8 +2.6 +9.5\\n(d)RoIAlign (ResNet-50- C5,stride 32 ): Mask-level and box-level\\nAP using large-stride features. Misalignments are more severe than\\nwith stride-16 features (Table 2c), resulting in big accuracy gaps.mask branch AP AP 50 AP75\\nMLP fc: 1024‚Üí1024‚Üí80¬∑28231.5 53.7 32.8\\nMLP fc: 1024‚Üí1024‚Üí1024‚Üí80¬∑28231.5 54.0 32.6\\nFCN conv: 256‚Üí256‚Üí256‚Üí256‚Üí256‚Üí80 33.6 55.2 35.3\\n(e)Mask Branch (ResNet-50-FPN): Fully convolutional networks (FCN) vs.\\nmulti-layer perceptrons (MLP, fully-connected) for mask prediction. FCNs im-\\nprove results as they take advantage of explicitly encoding spatial layout.\\nTable 2. Ablations . We train on trainval35k , test on minival , and report mask AP unless otherwise noted.\\n4.2. Ablation Experiments\\nWe run a number of ablations to analyze Mask R-CNN.\\nResults are shown in Table 2 and discussed in detail next.\\nArchitecture: Table 2a shows Mask R-CNN with various\\nbackbones. It beneÔ¨Åts from deeper networks (50 vs. 101)\\nand advanced designs including FPN and ResNeXt. We\\nnote that notall frameworks automatically beneÔ¨Åt from\\ndeeper or advanced networks (see benchmarking in [21]).\\nMultinomial vs. Independent Masks: Mask R-CNN de-\\ncouples mask and class prediction: as the existing box\\nbranch predicts the class label, we generate a mask for each\\nclass without competition among classes (by a per-pixel sig-\\nmoid and a binary loss). In Table 2b, we compare this to\\nusing a per-pixel softmax and a multinomial loss (as com-\\nmonly used in FCN [30]). This alternative couples the tasks\\nof mask and class prediction, and results in a severe loss\\nin mask AP (5.5 points). This suggests that once the in-\\nstance has been classiÔ¨Åed as a whole (by the box branch),\\nit is sufÔ¨Åcient to predict a binary mask without concern for\\nthe categories, which makes the model easier to train.\\nClass-SpeciÔ¨Åc vs. Class-Agnostic Masks: Our default in-\\nstantiation predicts class-speciÔ¨Åc masks, i.e., one m√ómmask per class. Interestingly, Mask R-CNN with class-\\nagnostic masks ( i.e., predicting a single m√ómoutput re-\\ngardless of class) is nearly as effective: it has 29.7 mask AP\\nvs. 30.3 for the class-speciÔ¨Åc counterpart on ResNet-50-C4.\\nThis further highlights the division of labor in our approach\\nwhich largely decouples classiÔ¨Åcation and segmentation.\\nRoIAlign: An evaluation of our proposed RoIAlign layer is\\nshown in Table 2c. For this experiment we use the ResNet-\\n50-C4 backbone, which has stride 16. RoIAlign improves\\nAP by about 3 points over RoIPool, with much of the gain\\ncoming at high IoU (AP 75). RoIAlign is insensitive to\\nmax/average pool; we use average in the rest of the paper.\\nAdditionally, we compare with RoIWarp proposed in\\nMNC [10] that also adopt bilinear sampling. As discussed\\nin¬ß3, RoIWarp still quantizes the RoI, losing alignment\\nwith the input. As can be seen in Table 2c, RoIWarp per-\\nforms on par with RoIPool and much worse than RoIAlign.\\nThis highlights that proper alignment is key.\\nWe also evaluate RoIAlign with a ResNet-50-C5 back-\\nbone, which has an even larger stride of 32 pixels. We use\\nthe same head as in Figure 4 (right), as the res5 head is not\\napplicable. Table 2d shows that RoIAlign improves mask\\nAP by a massive 7.3 points, and mask AP 75by 10.5 points\\n6'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/1703.06870v3.pdf', 'page': 6}, page_content='backbone APbbAPbb\\n50APbb\\n75APbb\\nSAPbb\\nMAPbb\\nL\\nFaster R-CNN+++ [19] ResNet-101-C4 34.9 55.7 37.4 15.6 38.7 50.9\\nFaster R-CNN w FPN [27] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2\\nFaster R-CNN by G-RMI [21] Inception-ResNet-v2 [41] 34.7 55.5 36.7 13.5 38.1 52.0\\nFaster R-CNN w TDM [39] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1\\nFaster R-CNN, RoIAlign ResNet-101-FPN 37.3 59.6 40.3 19.8 40.2 48.8\\nMask R-CNN ResNet-101-FPN 38.2 60.3 41.7 20.1 41.1 50.2\\nMask R-CNN ResNeXt-101-FPN 39.8 62.3 43.4 22.1 43.2 51.2\\nTable 3. Object detection single-model results (bounding box AP), vs. state-of-the-art on test-dev . Mask R-CNN using ResNet-101-\\nFPN outperforms the base variants of all previous state-of-the-art models (the mask output is ignored in these experiments). The gains of\\nMask R-CNN over [27] come from using RoIAlign (+1.1 APbb), multitask training (+0.9 APbb), and ResNeXt-101 (+1.6 APbb).\\n(50% relative improvement ). Moreover, we note that with\\nRoIAlign, using stride-32 C5 features (30.9 AP) is more ac-\\ncurate than using stride-16 C4 features (30.3 AP, Table 2c).\\nRoIAlign largely resolves the long-standing challenge of\\nusing large-stride features for detection and segmentation.\\nFinally, RoIAlign shows a gain of 1.5 mask AP and 0.5\\nbox AP when used with FPN, which has Ô¨Åner multi-level\\nstrides. For keypoint detection that requires Ô¨Åner alignment,\\nRoIAlign shows large gains even with FPN (Table 6).\\nMask Branch: Segmentation is a pixel-to-pixel task and\\nwe exploit the spatial layout of masks by using an FCN.\\nIn Table 2e, we compare multi-layer perceptrons (MLP)\\nand FCNs, using a ResNet-50-FPN backbone. Using FCNs\\ngives a 2.1 mask AP gain over MLPs. We note that we\\nchoose this backbone so that the conv layers of the FCN\\nhead are not pre-trained, for a fair comparison with MLP.\\n4.3. Bounding Box Detection Results\\nWe compare Mask R-CNN to the state-of-the-art COCO\\nbounding-box object detection in Table 3. For this result,\\neven though the full Mask R-CNN model is trained, only\\nthe classiÔ¨Åcation and box outputs are used at inference (the\\nmask output is ignored). Mask R-CNN using ResNet-101-\\nFPN outperforms the base variants of all previous state-of-\\nthe-art models, including the single-model variant of G-\\nRMI [21], the winner of the COCO 2016 Detection Chal-\\nlenge. Using ResNeXt-101-FPN, Mask R-CNN further im-\\nproves results, with a margin of 3.0 points box AP over\\nthe best previous single model entry from [39] (which used\\nInception-ResNet-v2-TDM).\\nAs a further comparison, we trained a version of Mask\\nR-CNN but without the mask branch, denoted by ‚ÄúFaster\\nR-CNN, RoIAlign‚Äù in Table 3. This model performs better\\nthan the model presented in [27] due to RoIAlign. On the\\nother hand, it is 0.9 points box AP lower than Mask R-CNN.\\nThis gap of Mask R-CNN on box detection is therefore due\\nsolely to the beneÔ¨Åts of multi-task training.\\nLastly, we note that Mask R-CNN attains a small gap\\nbetween its mask and box AP: e.g., 2.7 points between 37.1\\n(mask, Table 1) and 39.8 (box, Table 3). This indicates that\\nour approach largely closes the gap between object detec-\\ntion and the more challenging instance segmentation task.4.4. Timing\\nInference: We train a ResNet-101-FPN model that shares\\nfeatures between the RPN and Mask R-CNN stages, follow-\\ning the 4-step training of Faster R-CNN [36]. This model\\nruns at 195ms per image on an Nvidia Tesla M40 GPU (plus\\n15ms CPU time resizing the outputs to the original resolu-\\ntion), and achieves statistically the same mask AP as the\\nunshared one. We also report that the ResNet-101-C4 vari-\\nant takes ‚àº400ms as it has a heavier box head (Figure 4), so\\nwe do not recommend using the C4 variant in practice.\\nAlthough Mask R-CNN is fast, we note that our design\\nis not optimized for speed, and better speed/accuracy trade-\\noffs could be achieved [21], e.g., by varying image sizes and\\nproposal numbers, which is beyond the scope of this paper.\\nTraining: Mask R-CNN is also fast to train. Training with\\nResNet-50-FPN on COCO trainval35k takes 32 hours\\nin our synchronized 8-GPU implementation (0.72s per 16-\\nimage mini-batch), and 44 hours with ResNet-101-FPN. In\\nfact, fast prototyping can be completed in less than one day\\nwhen training on the train set. We hope such rapid train-\\ning will remove a major hurdle in this area and encourage\\nmore people to perform research on this challenging topic.\\n5. Mask R-CNN for Human Pose Estimation\\nOur framework can easily be extended to human pose\\nestimation. We model a keypoint‚Äôs location as a one-hot\\nmask, and adopt Mask R-CNN to predict Kmasks, one for\\neach of Kkeypoint types ( e.g., left shoulder, right elbow).\\nThis task helps demonstrate the Ô¨Çexibility of Mask R-CNN.\\nWe note that minimal domain knowledge for human pose\\nis exploited by our system, as the experiments are mainly to\\ndemonstrate the generality of the Mask R-CNN framework.\\nWe expect that domain knowledge ( e.g., modeling struc-\\ntures [6]) will be complementary to our simple approach.\\nImplementation Details: We make minor modiÔ¨Åcations to\\nthe segmentation system when adapting it for keypoints.\\nFor each of the Kkeypoints of an instance, the training\\ntarget is a one-hot m√ómbinary mask where only a single\\npixel is labeled as foreground. During training, for each vis-\\nible ground-truth keypoint, we minimize the cross-entropy\\nloss over an m2-way softmax output (which encourages a\\n7'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/1703.06870v3.pdf', 'page': 7}, page_content='Figure 7. Keypoint detection results on COCO test using Mask R-CNN (ResNet-50-FPN), with person segmentation masks predicted\\nfrom the same model. This model has a keypoint AP of 63.1 and runs at 5 fps.\\nAPkpAPkp\\n50APkp\\n75APkp\\nMAPkp\\nL\\nCMU-Pose+++ [6] 61.8 84.9 67.5 57.1 68.2\\nG-RMI [32]‚Ä†62.4 84.0 68.5 59.1 68.1\\nMask R-CNN , keypoint-only 62.7 87.0 68.4 57.4 71.1\\nMask R-CNN , keypoint & mask 63.1 87.3 68.7 57.8 71.4\\nTable 4. Keypoint detection AP on COCO test-dev . Ours is a\\nsingle model (ResNet-50-FPN) that runs at 5 fps. CMU-Pose+++\\n[6] is the 2016 competition winner that uses multi-scale testing,\\npost-processing with CPM [44], and Ô¨Åltering with an object detec-\\ntor, adding a cumulative ‚àº5 points (clariÔ¨Åed in personal commu-\\nnication).‚Ä†: G-RMI was trained on COCO plus MPII [1] (25k im-\\nages), using two models (Inception-ResNet-v2 for bounding box\\ndetection and ResNet-101 for keypoints).\\nsingle point to be detected). We note that as in instance seg-\\nmentation, the Kkeypoints are still treated independently.\\nWe adopt the ResNet-FPN variant, and the keypoint head\\narchitecture is similar to that in Figure 4 (right). The key-\\npoint head consists of a stack of eight 3 √ó3 512-d conv lay-\\ners, followed by a deconv layer and 2 √óbilinear upscaling,\\nproducing an output resolution of 56 √ó56. We found that\\na relatively high resolution output (compared to masks) is\\nrequired for keypoint-level localization accuracy.\\nModels are trained on all COCO trainval35k im-\\nages that contain annotated keypoints. To reduce overÔ¨Åt-\\nting, as this training set is smaller, we train using image\\nscales randomly sampled from [640, 800] pixels; inference\\nis on a single scale of 800 pixels. We train for 90k iterations,\\nstarting from a learning rate of 0.02 and reducing it by 10 at\\n60k and 80k iterations. We use bounding-box NMS with a\\nthreshold of 0.5. Other details are identical as in ¬ß3.1.\\nMain Results and Ablations: We evaluate the person key-\\npoint AP (APkp) and experiment with a ResNet-50-FPN\\nbackbone; more backbones will be studied in the appendix.\\nTable 4 shows that our result (62.7 APkp) is 0.9 points higher\\nthan the COCO 2016 keypoint detection winner [6] that\\nuses a multi-stage processing pipeline (see caption of Ta-\\nble 4). Our method is considerably simpler and faster.\\nMore importantly, we have a uniÔ¨Åed model that can si-APbb\\nperson APmask\\nperson APkp\\nFaster R-CNN 52.5 - -\\nMask R-CNN, mask-only 53.6 45.8 -\\nMask R-CNN, keypoint-only 50.7 - 64.2\\nMask R-CNN, keypoint & mask 52.0 45.1 64.7\\nTable 5. Multi-task learning of box, mask, and keypoint about the\\nperson category, evaluated on minival . All entries are trained\\non the same data for fair comparisons. The backbone is ResNet-\\n50-FPN. The entries with 64.2 and 64.7 AP on minival have\\ntest-dev AP of 62.7 and 63.1, respectively (see Table 4).\\nAPkpAPkp\\n50APkp\\n75APkp\\nMAPkp\\nL\\nRoIPool 59.8 86.2 66.7 55.1 67.4\\nRoIAlign 64.2 86.6 69.7 58.7 73.0\\nTable 6. RoIAlign vs. RoIPool for keypoint detection on\\nminival . The backbone is ResNet-50-FPN.\\nmultaneously predict boxes, segments, and keypoints while\\nrunning at 5 fps. Adding a segment branch (for the per-\\nson category) improves the APkpto 63.1 (Table 4) on\\ntest-dev . More ablations of multi-task learning on\\nminival are in Table 5. Adding the mask branch to the\\nbox-only ( i.e., Faster R-CNN) or keypoint-only versions\\nconsistently improves these tasks. However, adding the\\nkeypoint branch reduces the box/mask AP slightly, suggest-\\ning that while keypoint detection beneÔ¨Åts from multitask\\ntraining, it does not in turn help the other tasks. Neverthe-\\nless, learning all three tasks jointly enables a uniÔ¨Åed system\\nto efÔ¨Åciently predict all outputs simultaneously (Figure 7).\\nWe also investigate the effect of RoIAlign on keypoint\\ndetection (Table 6). Though this ResNet-50-FPN backbone\\nhas Ô¨Åner strides ( e.g., 4 pixels on the Ô¨Ånest level), RoIAlign\\nstill shows signiÔ¨Åcant improvement over RoIPool and in-\\ncreases APkpby 4.4 points. This is because keypoint detec-\\ntions are more sensitive to localization accuracy. This again\\nindicates that alignment is essential for pixel-level localiza-\\ntion, including masks and keypoints.\\nGiven the effectiveness of Mask R-CNN for extracting\\nobject bounding boxes, masks, and keypoints, we expect it\\nbe an effective framework for other instance-level tasks.\\n8'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/1703.06870v3.pdf', 'page': 8}, page_content='training data AP [val] AP AP 50 person rider car truck bus train mcycle bicycle\\nInstanceCut [23] fine +coarse 15.8 13.0 27.9 10.0 8.0 23.7 14.0 19.5 15.2 9.3 4.7\\nDWT [4] fine 19.8 15.6 30.0 15.1 11.7 32.9 17.1 20.4 15.0 7.9 4.9\\nSAIS [17] fine - 17.4 36.7 14.6 12.9 35.7 16.0 23.2 19.0 10.3 7.8\\nDIN [3] fine +coarse - 20.0 38.8 16.5 16.7 25.7 20.6 30.0 23.4 17.1 10.1\\nSGN [29] fine +coarse 29.2 25.0 44.9 21.8 20.1 39.4 24.8 33.2 30.8 17.7 12.4\\nMask R-CNN fine 31.5 26.2 49.9 30.5 23.7 46.9 22.8 32.2 18.6 19.1 16.0\\nMask R-CNN fine + COCO 36.4 32.0 58.1 34.8 27.0 49.1 30.1 40.9 30.9 24.1 18.7\\nTable 7. Results on Cityscapes val (‚ÄòAP [ val]‚Äô column) and test (remaining columns) sets. Our method uses ResNet-50-FPN.\\nAppendix A: Experiments on Cityscapes\\nWe further report instance segmentation results on the\\nCityscapes [7] dataset. This dataset has fine annota-\\ntions for 2975 train, 500 val, and 1525 test images. It has\\n20kcoarse training images without instance annotations,\\nwhich we do notuse. All images are 2048 √ó1024 pixels.\\nThe instance segmentation task involves 8 object categories,\\nwhose numbers of instances on the fine training set are:\\nperson rider car truck bus train mcycle bicycle\\n17.9k 1.8k 26.9k 0.5k 0.4k 0.2k 0.7k 3.7k\\nInstance segmentation performance on this task is measured\\nby the COCO-style mask AP (averaged over IoU thresh-\\nolds); AP 50(i.e., mask AP at an IoU of 0.5) is also reported.\\nImplementation: We apply our Mask R-CNN models with\\nthe ResNet-FPN-50 backbone; we found the 101-layer\\ncounterpart performs similarly due to the small dataset size.\\nWe train with image scale (shorter side) randomly sampled\\nfrom [800, 1024], which reduces overÔ¨Åtting; inference is on\\na single scale of 1024 pixels. We use a mini-batch size of\\n1 image per GPU (so 8 on 8 GPUs) and train the model\\nfor 24k iterations, starting from a learning rate of 0.01 and\\nreducing it to 0.001 at 18k iterations. It takes ‚àº4 hours of\\ntraining on a single 8-GPU machine under this setting.\\nResults: Table 7 compares our results to the state of the\\nart on the val andtest sets. Without using the coarse\\ntraining set, our method achieves 26.2 AP on test , which\\nis over 30% relative improvement over the previous best en-\\ntry (DIN [3]), and is also better than the concurrent work of\\nSGN‚Äôs 25.0 [29]. Both DIN and SGN use fine +coarse\\ndata. Compared to the best entry using fine data only\\n(17.4 AP), we achieve a ‚àº50% improvement.\\nFor the person andcarcategories, the Cityscapes dataset\\nexhibits a large number of within -category overlapping in-\\nstances (on average 6 people and 9 cars per image). We\\nargue that within-category overlap is a core difÔ¨Åculty of in-\\nstance segmentation. Our method shows massive improve-\\nment on these two categories over the other best entries (rel-\\native‚àº40% improvement on person from 21.8 to 30.5 and\\n‚àº20% improvement on carfrom 39.4 to 46.9), even though\\nour method does not exploit the coarse data.\\nA main challenge of the Cityscapes dataset is training\\nmodels in a low-data regime, particularly for the categories\\noftruck ,bus, and train , which have about 200-500 train-\\ncar:1.00car:0.98\\ncar:0.98 car:0.95 car:0.81car:0.52person:1.00person:1.00person:1.00\\nperson:1.00 person:1.00person:1.00person:1.00\\nperson:1.00 person:1.00person:1.00\\nperson:1.00person:1.00\\nperson:1.00\\nperson:1.00person:0.99\\nperson:0.99person:0.99 person:0.99person:0.98\\nperson:0.98person:0.98person:0.98person:0.94 person:0.94person:0.82\\nperson:0.82person:0.79\\nperson:0.73person:0.67person:0.66person:0.59\\ntruck:0.66bus:1.00\\nbus:0.95rider:0.59\\nbicycle:0.83\\nbicycle:0.56car:1.00car:1.00\\ncar:1.00 car:1.00car:1.00car:1.00\\ncar:1.00car:1.00 car:1.00car:0.99car:0.95\\ncar:0.95car:0.95 car:0.69car:0.68 car:0.68car:0.64\\ncar:0.57 car:0.52person:1.00\\nperson:0.99person:0.99person:0.99 person:0.99person:0.98person:0.98 person:0.98 person:0.97 person:0.93person:0.92\\nperson:0.91person:0.86 person:0.84person:0.82 person:0.73person:0.72 person:0.72person:0.72 person:0.63 rider:0.68\\ncar:1.00\\ncar:1.00car:1.00car:1.00\\ncar:1.00car:1.00car:1.00car:1.00\\ncar:1.00 car:1.00\\ncar:1.00car:1.00\\ncar:1.00car:1.00car:1.00car:1.00car:1.00\\ncar:0.98car:0.97\\ncar:0.88car:0.76car:0.72car:0.72 car:0.65car:0.50person:1.00\\nperson:1.00 person:0.98person:0.93person:0.85person:0.78person:0.73 person:0.58\\nperson:1.00\\nperson:1.00person:1.00\\nperson:1.00\\nperson:1.00person:1.00\\nperson:1.00person:1.00person:1.00\\nperson:1.00person:1.00person:1.00\\nperson:1.00person:1.00\\nperson:1.00person:1.00person:1.00\\nperson:1.00person:1.00\\nperson:1.00person:1.00\\nperson:1.00person:0.99\\nperson:0.99 person:0.98person:0.97\\nperson:0.96person:0.92\\nperson:0.91person:0.70 person:0.59\\nbicycle:0.99bicycle:0.97car:1.00\\ncar:1.00 car:0.99\\ncar:0.89person:1.00\\nperson:1.00person:1.00\\nperson:1.00\\nperson:1.00person:0.96person:0.93\\nperson:0.89person:0.88person:0.75\\nrider:0.94\\ncar:1.00\\ncar:1.00car:1.00\\ncar:1.00car:1.00\\ncar:1.00car:1.00 car:0.99car:0.89 car:0.67person:1.00\\nperson:1.00\\nperson:1.00 person:1.00 person:0.82\\nbus:0.75Figure 8. Mask R-CNN results on Cityscapes test (32.0 AP).\\nThe bottom-right image shows a failure prediction.\\ning samples each. To partially remedy this issue, we further\\nreport a result using COCO pre-training. To do this, we ini-\\ntialize the corresponding 7 categories in Cityscapes from a\\npre-trained COCO Mask R-CNN model ( rider being ran-\\ndomly initialized). We Ô¨Åne-tune this model for 4k iterations\\nin which the learning rate is reduced at 3k iterations, which\\ntakes‚àº1 hour for training given the COCO model.\\nThe COCO pre-trained Mask R-CNN model achieves\\n32.0 AP on test , almost a 6 point improvement over the\\nfine -only counterpart. This indicates the important role\\nthe amount of training data plays. It also suggests that\\nmethods on Cityscapes might be inÔ¨Çuenced by their low-\\nshot learning performance. We show that using COCO pre-\\ntraining is an effective strategy on this dataset.\\nFinally, we observed a bias between the val andtest\\nAP, as is also observed from the results of [23, 4, 29]. We\\nfound that this bias is mainly caused by the truck ,bus,\\nand train categories, with the fine -only model having\\nval/test AP of 28.8/22.8, 53.5/32.2, and 33.0/18.6, re-\\nspectively. This suggests that there is a domain shift on\\nthese categories, which also have little training data. COCO\\npre-training helps to improve results the most on these cat-\\negories; however, the domain shift persists with 38.0/30.1,\\n57.5/40.9, and 41.2/30.9 val/test AP, respectively. Note\\nthat for the person andcarcategories we do not see any\\nsuch bias ( val/test AP are within¬±1point).\\nExample results on Cityscapes are shown in Figure 8.\\n9'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/1703.06870v3.pdf', 'page': 9}, page_content='description backbone AP AP 50 AP75 APbbAPbb\\n50APbb\\n75\\noriginal baseline X-101-FPN 36.7 59.5 38.9 39.6 61.5 43.2\\n+updated baseline X-101-FPN 37.0 59.7 39.0 40.5 63.0 43.7\\n+e2e training X-101-FPN 37.6 60.4 39.9 41.7 64.1 45.2\\n+ImageNet-5k X-101-FPN 38.6 61.7 40.9 42.7 65.1 46.6\\n+train-time augm. X-101-FPN 39.2 62.5 41.6 43.5 65.9 47.2\\n+deeper X-152-FPN 39.7 63.2 42.2 44.1 66.4 48.4\\n+Non-local [43] X-152-FPN-NL 40.3 64.4 42.8 45.0 67.8 48.9\\n+test-time augm. X-152-FPN-NL 41.8 66.0 44.8 47.3 69.3 51.5\\nTable 8. Enhanced detection results of Mask R-CNN on COCO\\nminival . Each row adds an extra component to the above row.\\nWe denote ResNeXt model by ‚ÄòX‚Äô for notational brevity.\\nAppendix B: Enhanced Results on COCO\\nAs a general framework, Mask R-CNN is compat-\\nible with complementary techniques developed for de-\\ntection/segmentation, including improvements made to\\nFast/Faster R-CNN and FCNs. In this appendix we de-\\nscribe some techniques that improve over our original re-\\nsults. Thanks to its generality and Ô¨Çexibility, Mask R-CNN\\nwas used as the framework by the three winning teams in\\nthe COCO 2017 instance segmentation competition, which\\nall signiÔ¨Åcantly outperformed the previous state of the art.\\nInstance Segmentation and Object Detection\\nWe report some enhanced results of Mask R-CNN in Ta-\\nble 8. Overall, the improvements increase mask AP 5.1\\npoints (from 36.7 to 41.8) and box AP 7.7 points (from 39.6\\nto 47.3). Each model improvement increases both mask AP\\nand box AP consistently, showing good generalization of\\nthe Mask R-CNN framework. We detail the improvements\\nnext. These results, along with future updates, can be repro-\\nduced by our released code at https://github.com/\\nfacebookresearch/Detectron , and can serve as\\nhigher baselines for future research.\\nUpdated baseline: We start with an updated baseline\\nwith a different set of hyper-parameters. We lengthen the\\ntraining to 180k iterations, in which the learning rate is re-\\nduced by 10 at 120k and 160k iterations. We also change\\nthe NMS threshold to 0.5 (from a default value of 0.3). The\\nupdated baseline has 37.0 mask AP and 40.5 box AP.\\nEnd-to-end training: All previous results used stage-\\nwise training, i.e., training RPN as the Ô¨Årst stage and Mask\\nR-CNN as the second. Following [37], we evaluate end-\\nto-end (‚Äòe2e‚Äô) training that jointly trains RPN and Mask R-\\nCNN. We adopt the ‚Äòapproximate‚Äô version in [37] that only\\ncomputes partial gradients in the RoIAlign layer by ignor-\\ning the gradient w.r.t. RoI coordinates. Table 8 shows that\\ne2e training improves mask AP by 0.6 and box AP by 1.2.\\nImageNet-5k pre-training: Following [45], we experi-\\nment with models pre-trained on a 5k-class subset of Ima-\\ngeNet (in contrast to the standard 1k-class subset). This 5 √ó\\nincrease in pre-training data improves both mask and box 1\\nAP. As a reference, [40] used ‚àº250√ómore images (300M)\\nand reported a 2-3 box AP improvement on their baselines.description backbone APkpAPkp\\n50APkp\\n75APkp\\nMAPkp\\nL\\noriginal baseline R-50-FPN 64.2 86.6 69.7 58.7 73.0\\n+updated baseline R-50-FPN 65.1 86.6 70.9 59.9 73.6\\n+deeper R-101-FPN 66.1 87.7 71.7 60.5 75.0\\n+ResNeXt X-101-FPN 67.3 88.0 73.3 62.2 75.6\\n+data distillation [35] X-101-FPN 69.1 88.9 75.3 64.1 77.1\\n+test-time augm. X-101-FPN 70.4 89.3 76.8 65.8 78.1\\nTable 9. Enhanced keypoint results of Mask R-CNN on COCO\\nminival . Each row adds an extra component to the above row.\\nHere we use only keypoint annotations but no mask annotations.\\nWe denote ResNet by ‚ÄòR‚Äô and ResNeXt by ‚ÄòX‚Äô for brevity.\\nTrain-time augmentation: Scale augmentation at train\\ntime further improves results. During training, we randomly\\nsample a scale from [640, 800] pixels and we increase the\\nnumber of iterations to 260k (with the learning rate reduced\\nby 10 at 200k and 240k iterations). Train-time augmenta-\\ntion improves mask AP by 0.6 and box AP by 0.8.\\nModel architecture: By upgrading the 101-layer\\nResNeXt to its 152-layer counterpart [19], we observe an\\nincrease of 0.5 mask AP and 0.6 box AP. This shows a\\ndeeper model can still improve results on COCO.\\nUsing the recently proposed non-local (NL) model [43],\\nwe achieve 40.3 mask AP and 45.0 box AP. This result is\\nwithout test-time augmentation, and the method runs at 3fps\\non an Nvidia Tesla P100 GPU at test time.\\nTest-time augmentation: We combine the model results\\nevaluated using scales of [400, 1200] pixels with a step of\\n100 and on their horizontal Ô¨Çips. This gives us a single-\\nmodel result of 41.8 mask AP and 47.3 box AP.\\nThe above result is the foundation of our submission to\\nthe COCO 2017 competition (which also used an ensemble,\\nnot discussed here). The Ô¨Årst three winning teams for the\\ninstance segmentation task were all reportedly based on an\\nextension of the Mask R-CNN framework.\\nKeypoint Detection\\nWe report enhanced results of keypoint detection in Ta-\\nble 9. As an updated baseline, we extend the training sched-\\nule to 130k iterations in which the learning rate is reduced\\nby 10 at 100k and 120k iterations. This improves APkpby\\nabout 1 point. Replacing ResNet-50 with ResNet-101 and\\nResNeXt-101 increases APkpto 66.1 and 67.3, respectively.\\nWith a recent method called data distillation [35], we are\\nable to exploit the additional 120k unlabeled images pro-\\nvided by COCO. In brief, data distillation is a self-training\\nstrategy that uses a model trained on labeled data to pre-\\ndict annotations on unlabeled images, and in turn updates\\nthe model with these new annotations. Mask R-CNN pro-\\nvides an effective framework for such a self-training strat-\\negy. With data distillation, Mask R-CNN APkpimprove by\\n1.8 points to 69.1. We observe that Mask R-CNN can ben-\\neÔ¨Åt from extra data, even if that data is unlabeled .\\nBy using the same test-time augmentation as used for\\ninstance segmentation, we further boost APkpto 70.4.\\n10'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/1703.06870v3.pdf', 'page': 10}, page_content='Acknowledgements: We would like to acknowledge Ilija\\nRadosavovic for contributions to code release and enhanced\\nresults, and the Caffe2 team for engineering support.\\nReferences\\n[1] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2D\\nhuman pose estimation: New benchmark and state of the art\\nanalysis. In CVPR , 2014. 8\\n[2] P. Arbel ¬¥aez, J. Pont-Tuset, J. T. Barron, F. Marques, and\\nJ. Malik. Multiscale combinatorial grouping. In CVPR ,\\n2014. 2\\n[3] A. Arnab and P. H. Torr. Pixelwise instance segmentation\\nwith a dynamically instantiated network. In CVPR , 2017. 3,\\n9\\n[4] M. Bai and R. Urtasun. Deep watershed transform for in-\\nstance segmentation. In CVPR , 2017. 3, 9\\n[5] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-\\noutside net: Detecting objects in context with skip pooling\\nand recurrent neural networks. In CVPR , 2016. 5\\n[6] Z. Cao, T. Simon, S.-E. Wei, and Y . Sheikh. Realtime multi-\\nperson 2d pose estimation using part afÔ¨Ånity Ô¨Åelds. In CVPR ,\\n2017. 7, 8\\n[7] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,\\nR. Benenson, U. Franke, S. Roth, and B. Schiele. The\\nCityscapes dataset for semantic urban scene understanding.\\nInCVPR , 2016. 9\\n[8] J. Dai, K. He, Y . Li, S. Ren, and J. Sun. Instance-sensitive\\nfully convolutional networks. In ECCV , 2016. 2\\n[9] J. Dai, K. He, and J. Sun. Convolutional feature masking for\\njoint object and stuff segmentation. In CVPR , 2015. 2\\n[10] J. Dai, K. He, and J. Sun. Instance-aware semantic segmen-\\ntation via multi-task network cascades. In CVPR , 2016. 2, 3,\\n4, 5, 6\\n[11] J. Dai, Y . Li, K. He, and J. Sun. R-FCN: Object detection via\\nregion-based fully convolutional networks. In NIPS , 2016. 2\\n[12] R. Girshick. Fast R-CNN. In ICCV , 2015. 1, 2, 3, 4, 6\\n[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\\nture hierarchies for accurate object detection and semantic\\nsegmentation. In CVPR , 2014. 2, 3\\n[14] R. Girshick, F. Iandola, T. Darrell, and J. Malik. Deformable\\npart models are convolutional neural networks. In CVPR ,\\n2015. 4\\n[15] B. Hariharan, P. Arbel ¬¥aez, R. Girshick, and J. Malik. Simul-\\ntaneous detection and segmentation. In ECCV . 2014. 2\\n[16] B. Hariharan, P. Arbel ¬¥aez, R. Girshick, and J. Malik. Hyper-\\ncolumns for object segmentation and Ô¨Åne-grained localiza-\\ntion. In CVPR , 2015. 2\\n[17] Z. Hayder, X. He, and M. Salzmann. Shape-aware instance\\nsegmentation. In CVPR , 2017. 9\\n[18] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\\nin deep convolutional networks for visual recognition. In\\nECCV . 2014. 1, 2\\n[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\\nfor image recognition. In CVPR , 2016. 2, 4, 7, 10\\n[20] J. Hosang, R. Benenson, P. Doll ¬¥ar, and B. Schiele. What\\nmakes for effective detection proposals? PAMI , 2015. 2[21] J. Huang, V . Rathod, C. Sun, M. Zhu, A. Korattikara,\\nA. Fathi, I. Fischer, Z. Wojna, Y . Song, S. Guadarrama, et al.\\nSpeed/accuracy trade-offs for modern convolutional object\\ndetectors. In CVPR , 2017. 2, 3, 4, 6, 7\\n[22] M. Jaderberg, K. Simonyan, A. Zisserman, and\\nK. Kavukcuoglu. Spatial transformer networks. In\\nNIPS , 2015. 4\\n[23] A. Kirillov, E. Levinkov, B. Andres, B. Savchynskyy, and\\nC. Rother. Instancecut: from edges to instances with multi-\\ncut. In CVPR , 2017. 3, 9\\n[24] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-\\nsiÔ¨Åcation with deep convolutional neural networks. In NIPS ,\\n2012. 2\\n[25] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\\nHoward, W. Hubbard, and L. D. Jackel. Backpropagation\\napplied to handwritten zip code recognition. Neural compu-\\ntation , 1989. 2\\n[26] Y . Li, H. Qi, J. Dai, X. Ji, and Y . Wei. Fully convolutional\\ninstance-aware semantic segmentation. In CVPR , 2017. 2,\\n3, 5, 6\\n[27] T.-Y . Lin, P. Doll ¬¥ar, R. Girshick, K. He, B. Hariharan, and\\nS. Belongie. Feature pyramid networks for object detection.\\nInCVPR , 2017. 2, 4, 5, 7\\n[28] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\\nmanan, P. Doll ¬¥ar, and C. L. Zitnick. Microsoft COCO: Com-\\nmon objects in context. In ECCV , 2014. 2, 5\\n[29] S. Liu, J. Jia, S. Fidler, and R. Urtasun. SGN: Sequen-\\ntial grouping networks for instance segmentation. In ICCV ,\\n2017. 3, 9\\n[30] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\\nnetworks for semantic segmentation. In CVPR , 2015. 1, 3, 6\\n[31] V . Nair and G. E. Hinton. RectiÔ¨Åed linear units improve re-\\nstricted boltzmann machines. In ICML , 2010. 4\\n[32] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tomp-\\nson, C. Bregler, and K. Murphy. Towards accurate multi-\\nperson pose estimation in the wild. In CVPR , 2017. 8\\n[33] P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to seg-\\nment object candidates. In NIPS , 2015. 2, 3\\n[34] P. O. Pinheiro, T.-Y . Lin, R. Collobert, and P. Doll ¬¥ar. Learn-\\ning to reÔ¨Åne object segments. In ECCV , 2016. 2, 3\\n[35] I. Radosavovic, P. Doll ¬¥ar, R. Girshick, G. Gkioxari, and\\nK. He. Data distillation: Towards omni-supervised learning.\\narXiv:1712.04440 , 2017. 10\\n[36] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-\\nwards real-time object detection with region proposal net-\\nworks. In NIPS , 2015. 1, 2, 3, 4, 7\\n[37] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-\\nwards real-time object detection with region proposal net-\\nworks. In TPAMI , 2017. 10\\n[38] A. Shrivastava, A. Gupta, and R. Girshick. Training region-\\nbased object detectors with online hard example mining. In\\nCVPR , 2016. 2, 5\\n[39] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be-\\nyond skip connections: Top-down modulation for object de-\\ntection. arXiv:1612.06851 , 2016. 4, 7\\n[40] C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting\\nunreasonable effectiveness of data in deep learning era. In\\nICCV , 2017. 10\\n11'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/1703.06870v3.pdf', 'page': 11}, page_content='[41] C. Szegedy, S. Ioffe, and V . Vanhoucke. Inception-v4,\\ninception-resnet and the impact of residual connections on\\nlearning. In ICLR Workshop , 2016. 7\\n[42] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.\\nSmeulders. Selective search for object recognition. IJCV ,\\n2013. 2\\n[43] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural\\nnetworks. arXiv:1711.07971 , 2017. 10\\n[44] S.-E. Wei, V . Ramakrishna, T. Kanade, and Y . Sheikh. Con-\\nvolutional pose machines. In CVPR , 2016. 8\\n[45] S. Xie, R. Girshick, P. Doll ¬¥ar, Z. Tu, and K. He. Aggregated\\nresidual transformations for deep neural networks. In CVPR ,\\n2017. 4, 10\\n12'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 0}, page_content='1\\nParameter-Efficient Fine-Tuning Methods for\\nPretrained Language Models: A Critical\\nReview and Assessment\\nLingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, Fu Lee Wang\\nAbstract ‚ÄîWith the continuous growth in the number of\\nparameters of transformer-based pretrained language models\\n(PLMs), particularly the emergence of large language models\\n(LLMs) with billions of parameters, many natural language\\nprocessing (NLP) tasks have demonstrated remarkable success.\\nHowever, the enormous size and computational demands of\\nthese models pose significant challenges for adapting them\\nto specific downstream tasks, especially in environments with\\nlimited computational resources. Parameter Efficient Fine-Tuning\\n(PEFT) offers an effective solution by reducing the number\\nof fine-tuning parameters and memory usage while achieving\\ncomparable performance to full fine-tuning. The demands for\\nfine-tuning PLMs, especially LLMs, have led to a surge in the\\ndevelopment of PEFT methods, as depicted in Fig. 1. In this\\npaper, we present a comprehensive and systematic review of\\nPEFT methods for PLMs. We summarize these PEFT methods,\\ndiscuss their applications, and outline future directions. Further-\\nmore, we conduct experiments using several representative PEFT\\nmethods to better understand their effectiveness in parameter\\nefficiency and memory efficiency. By offering insights into the\\nlatest advancements and practical applications, this survey serves\\nas an invaluable resource for researchers and practitioners\\nseeking to navigate the challenges and opportunities presented\\nby PEFT in the context of PLMs.\\nIndex Terms ‚ÄîParameter-efficient, fine-tuning, pretrained lan-\\nguage model, large language model, memory usage.\\nI. I NTRODUCTION\\nTRANSFORMER-BASED PLMs [1], [2], [3], [4] have\\ndemonstrated remarkable performance across a wide\\nrange of NLP tasks. To fully harness the potential of PLMs,\\nfine-tuning is employed to adapt the PLMs to task-specific\\ndata to enhance performance on downstream tasks. However,\\ntraditional fine-tuning involves updating all the pretrained\\nparameters of PLMs, which is time-consuming and computa-\\ntionally expensive. As the size of PLMs continues to increase,\\nfrom models like BERT [1] with 110 million parameters to\\nT5 [4] with 770 million parameters, computational resource\\nrequirements become a significant challenge. The advent of\\nThis work was supported by a research grant entitled ‚ÄùMedical Text Feature\\nRepresentations based on Pre-trained Language Models‚Äù (871238) and Faculty\\nResearch Grant (DB24A4) at Lingnan University, Hong Kong. (Corresponding\\nauthor: Haoran Xie.)\\nLingling Xu and Fu Lee Wang are with the Hong Kong Metropolitan Uni-\\nversity, Hong Kong (email: xxiao199409@gmail.com; pwang@hkmu.edu.hk).\\nHaoran Xie and Si-Zhao Joe Qin are with Lingnan University, Hong Kong\\n(email: hrxie@ln.edu.hk; joeqin@ln.edu.hk).\\nXiaohui Tao is with University of Southern Queensland, Queensland,\\nAustralia (email: xtao@usq.edu.au).LLMs [5], [6], [7], exemplified by Falcon [8] with a stag-\\ngering 180 billion parameters, further exacerbates the com-\\nputational demands. To perform task-specific full fine-tuning\\nwith Falcon-180B, a minimum of 5120GB of computational\\nresources may be required1. The enormous computational\\nresource requirements are prohibitive for anyone but the su-\\nperpower players to utilize LLMs for task-specific fine-tuning.\\nTo address this challenge, a prominent method known as\\nPEFT [9] has emerged as a viable solution to compensate\\nfor the tremendous computational cost of full parameter\\nfine-tuning. PEFT involves employing various deep learning\\ntechniques [9], [10], [11] to reduce the number of trainable\\nparameters while still maintaining comparable performance to\\nthe full fine-tuning. In addition, PEFT updates only a small\\nnumber of additional parameters or updates a subset of the\\npretrained parameters, preserving the knowledge captured by\\nthe PLM while adapting it to the target task and reducing\\nthe risk of catastrophic forgetting. Furthermore, since the size\\nof the fine-tuned dataset is typically much smaller than the\\npretrained dataset, performing full fine-tuning to update all\\nthe pretrained parameters may lead to overfitting, which is\\ncircumvented by the PEFT through selectively or not updating\\npretrained parameters.\\nRecently, there has been a significant surge in interest\\nregarding PEFT methods, as demonstrated by the growing\\nnumber of studies depicted in Fig. 1. This also leads to a\\nfew surveys on PEFT approaches for the PLMs. However,\\nthe existing surveys have certain limitations. Ding et al.\\n[12] conducted a comprehensive study on PEFT methods,\\nbut this survey did not cover much of the latest work in\\nthe field and only four PEFT methods were quantitatively\\nexperimented with. Lialin et al. [13] delved into the ideas and\\noperational implementations of PEFT methods in detail but\\ndo not perform relevant experiments. In this work, we address\\nthese gaps comprehensively. We meticulously categorize the\\nPEFT methods, providing detailed explanations of the ideas\\nand specific implementations of each method. We compare\\nthe similarities and differences among various types of PEFT\\nmethods, facilitating a better understanding of the evolving\\nlandscape of PEFT. Moreover, we conduct extensive fine-\\ntuning experiments with 11 representative PEFT methods.\\nIn this paper, we aim to provide a comprehensive and\\nsystematic study of PEFT methods for PLMs in NLP. We\\nundertake an in-depth exploration of these PEFT methods and\\n1https://huggingface.co/blog/falcon-180b#hardware-requirementsarXiv:2312.12148v1  [cs.CL]  19 Dec 2023'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 1}, page_content='2\\nMAM\\nAdapterProPETL\\n20192020202120222023\\nAdapter\\nDropHyperFormer++\\nResidual\\nAdapter(IA)3\\nPrefix-\\ntuningMPT\\nA TTEMPT\\nSPoT\\nLoRAKronA\\nIntrinsic\\nSAIDSAM\\nPrompt-\\ntuningW ARPUniPEL TAutoPEFT\\nThreshold\\nMASKBitFitFISH\\nMask\\nChild\\ntuningL T -FSTAdditive Fine-tuning\\nHybrid Fine-tuning\\nSequential\\nAdapterDif f\\nPruningAdapter\\nFusionParallel\\nAdapterKernel-mix-lite(qvo)\\nP-tuningP AST A\\nAttention\\nFusionT iny-Attn\\nAdapter\\nLSTCoDA\\nMerAAdapterSoupHardamard\\nAdapter\\nU/S-SAM\\nSparse\\nAdapter\\nCompacterS3Delta-MS4\\nAdaMixDyLoRALOFTQLoRA-F A\\nLaplace-LoRAMoELoRA\\nLoRAPrune\\nIncreLoRAU/S-BitFitQLoRAQA-LoRAL-LoRA\\nLoRA-Hub\\nAdaLoRADelta-LoRAPartial Fine-tuningReparameterized Fine-tuning\\nUnified Fine-tuning\\nFig. 1: The evolutionary development of PEFT methods in recent years. Models on the same branch have some common\\nfeatures. The vertical position of the models shows the timeline of their release dates. Notably, the year of the paper‚Äôs initial\\npublication is shown as the reference. For instance, if a paper is published in ACL 2022 but listed on arXiv in 2021, the year\\n2021 will be considered as the reference date.\\npresent a comprehensive taxonomy scheme in Section III. By\\ncategorizing PEFT methods into additive fine-tuning, partial\\nfine-tuning, reparameterized fine-tuning, hybrid fine-tuning,\\nand unified fine-tuning, we establish a structured framework\\nfor understanding these PEFT approaches, as depicted in\\nFig. 2. In Section IV, we conduct quantitative investigations\\nand analyses to assess the performance, parameters efficiency,\\nand memory usage of these PEFT approaches. Our quantitative\\nstudies primarily focus on natural language understanding\\n(NLU), machine translation (MT), and natural language gen-\\neration (NLG) tasks. Additionally, we extensively explorethe applications of PEFT in multi-task learning, cross-lingual\\ntransfer, and backdoor attack and defense, underscoring its\\neffectiveness. Furthermore, our research also unveils potential\\ndirections for future investigations in this rapidly evolving\\nfield. To summarize, the main contributions of this survey can\\nbe outlined as follows:\\n‚Ä¢We present a comprehensive analysis and review of PEFT\\nmethods for transformer-based PLMs.\\n‚Ä¢We identify the key techniques and approaches employed\\nin PEFT methods, and classify them into additive, partial,\\nreparameterized, hybrid, and unified fine-tuning methods.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 2}, page_content='3PEFT Methods for PLMsAdditive\\nFine-tuningAdapter-based\\nFine-tuningSequential Adapter [9], Residual Adapter [14], CoDA [15], Parallel Adapter [16], AdapterDrop [17],\\nTiny-Attn Adapter [18], AdapterFusion [19], MerA [20], Hyperformer++ [21], AdapterSoup [22]\\nSoft Prompt-based\\nFine-tuningWARP [23], Promt-tuning [24], Prefix-tuning [10], P-tuning [25], SPOT [26], ATTEMPT [27], MPT [28]\\nOthers LST [29], (IA)3[30], PASTA [31], AttentionFusion [32], Hadamard Adapter [33]\\nPartial\\nFine-tuningBias Update BitFit [34], U/S-BitFit [35],\\nPretrained Weight\\nMaskingThreshold-Mask [36], FISH Mask [37]\\nDelta Weight\\nMaskingLT-SFT [38], Child-Tuning [39], Diff Pruning [40], SAM [41]\\nReparameterized\\nFine-tuningLow-rank\\nDecompositionIntrinsic SAID [42], LoRA [11], KronA [43]\\nLoRA DerivativesLow-rank Adjustment DyLoRA [44], AdaLoRA [45], IncreLoRA [46]\\nLoRA-guided Pretrained\\nWeight UpdateDelta-LoRA [47], LoRAPrune [48]\\nQuantization Adaption QLoRA [49], QA-LoRA [50], LOFTQ [51]\\nLoRA-based\\nImprovementsKernel-mix-lite(qv)/(qvo) [52], Laplace-LoRA [53], LoRA-FA [54]\\nLoRA-based\\nMulti-task Fine-tuningLoRAHub [55], MoELoRA [56], L-LoRA [57]\\nHybrid\\nFine-tuningManual Combination MAM Adapter [16], U/S-MAM [35], Compacter [58], UniPELT [59],\\nAutomatic Combination AutoPEFT [60], S3Delta-M [61], S4[62]\\nUnified\\nFine-tuningAdaMix [63], SparseAdapter [64], ProPETL [65]\\nFig. 2: Taxonomy of Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models.\\n‚Ä¢We conduct extensive experiments to evaluate the effec-\\ntiveness of several representative PEFT methods, specifi-\\ncally examining their impact on parameter efficiency and\\nmemory usage.\\nII. P RELIMINARIES\\nA. Transformer\\nTransformer [66] has emerged as a foundational architecture\\nfor numerous PLMs, it adopts an encoder-decoder architecture,\\ncomprised of a stack of encoder and decoder layers, each\\nequipped with the self-attention mechanism. Both the encoder\\nand decoder in the Transformer architecture consist of a multi-\\nhead self-attention layer and a feed-forward network (FFN)\\nlayer, interconnected by a residual connection [67] followed\\nby layer normalization [68]. Residual connection allows the\\nmodel to effectively propagate information from one layer\\nto the subsequent layer without losing valuable information.\\nLayer normalization further stabilizes the training process by\\nnormalizing the inputs of each layer.\\nMulti-head self-attention layer employs the self-attention\\nfunction with hheads in parallel. For an input sequence\\nX‚ààRn√ódwith the sentence length nand hidden dimension\\nsize of d. The query ( Q), key ( K), and value ( V) vectors are\\nthe transformation of input sequence X,\\nK=XW k+bk,Q=XW q+bq,V=XW v+bv,(1)where Q, K, V ‚ààRn√ód,bk,bqandbvare typically learnable\\nparameter vectors that help model to better capture specific\\ninformation in the input vector Xand adjust the value of the\\nquery vector Qto better match the key vector K, thereby\\nimproving performance of the self-attention mechanism. The\\nself-attention output of input Xis computed as:\\nAttn(Q,K,V) =Softmax (QKT\\n‚àö\\nd)V, (2)\\nthen multi-head self-attention can be described as follows:\\nMHA (Q, K, V ) =Concat (head 1,¬∑¬∑¬∑,head h)WO, (3)\\nhead i=Attn(QWi\\nQ, KWi\\nK, V Wi\\nV). (4)\\nWhile the FFN consists of two linear transformations with\\na non-linear ReLU activation function in between:\\nFFN(X) =ReLU (XW 1+b1)W2+b2, (5)\\nwhere W1,b1,W2andb2are the weight matrices of two\\nlinear transformations. Most PEFT methods primarily focus on\\nthe self-attention layer and FFN layer, allowing models like\\nencoder-based RoBERTa [2], encoder-decoder-based T5 [4],\\nand decoder-based LLaMA [7] to leverage relevant techniques\\nfor parameters reduction.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 3}, page_content='4\\nB. Full Fine-tuning of PLMs\\nFull fine-tuning of transformer-based PLMs involves train-\\ning the entire model, including all layers and parameters, on\\na specific downstream task using task-specific data. Initially,\\nPLMs are trained on large-scale datasets with unsupervised\\nlearning objectives like language modeling or masked lan-\\nguage modeling, to learn general language representations [1],\\n[2], [4], [7]. However, these PLMs may not perform optimally\\nwhen applied to specific tasks like sentiment analysis, question\\nanswering, or translation due to a lack of appropriate domain\\nknowledge [69], [70], [71]. Full fine-tuning provides an effec-\\ntive solution to address this limitation.\\nDuring full fine-tuning, the PLM is initialized with pre-\\ntrained weights and subsequently trained on task-specific data\\nusing techniques like backpropagation and gradient descent\\n[72], [73]. All model parameters, including pretrained weights,\\nare updated to minimize a task-specific loss that quantifies\\nthe disparity between predicted outputs and ground truth. In\\nthis way, full fine-tuning enables the model to learn task-\\nspecific patterns and nuances from the labeled data, facili-\\ntating predictions or outputs tailored to the target tasks [74].\\nNotably, full fine-tuning necessitates substantial computational\\nresources and labeled data, as the model is trained from scratch\\nfor the specific target task. Moreover, as PLMs grow in size\\nand with the advent of LLMs containing billions of parameters,\\nfull fine-tuning places even greater demands on computational\\nresources. In contrast, PEFT methods aim to alleviate these re-\\nquirements by selectively updating or modifying specific parts\\nof the PLMs while still achieving performance comparable to\\nfull fine-tuning [34], [39]. Furthermore, full fine-tuning may\\ngive rise to overfitting when the task-specific dataset is small\\nor when the PLMs are already well-suited to the target task\\n[19], [75].\\nIII. P ARAMETER -EFFICIENT FINE-TUNING METHODS\\nA. Additive Fine-tuning\\nAdditive fine-tuning approaches involve introducing new\\nextra trainable parameters for task-specific fine-tuning. We\\nclassify additive fine-tuning into three groups: Adapter-based\\nFine-tuning [9], [14], [15], [16], [17], [18], [19], [20], [21],\\n[22], [76], in which the adapter module is incorporated into\\nthe transformer, allowing for fine-tuning without modifying\\nthe pretrained parameters, Soft Prompt-based Fine-tuning\\n[10], [23], [24], [25], [26], [27], [28], where soft prompts or\\nprefix vectors are appended to the input embeddings or hidden\\nstates during fine-tuning, and Others [29], [30], [31], [32],\\n[33], in which various methods that introduce supplementary\\nparameters for model fine-tuning are fall into this category.\\n1) Adapters-based Fine-tuning: The idea of Adapter is\\nfirst introduced in multi-domain image classification [77],\\nallowing for the efficient transfer of knowledge across multiple\\nvisual domains. Sequential Adapter [9] extends and applies\\nit to NLP tasks by inserting the adapter (trainable modules)\\ninto the transformer block and fine-tuning the parameters\\nof adapters to make the PLMs adapt to the downstream\\ntasks. Specifically, adapter networks are inserted after the\\nself-attention layer and feed-forward layer of the Transformersequentially. Each adapter are low-rank module that consists\\nof a down-projection, a non-linear activation function, and an\\nup-projection as well as a residual connection. For the input X,\\nthe output of a sequential adapter with the ReLU non-linear\\nactivation function can be defined with Equation 6. During\\nfine-tuning, only the parameters of adapter network Wupand\\nWdown need to be updated to make the PLMs adapt to the\\nspecific downstream tasks. The specific architecture of the\\nsequential adapter is presented in Fig. 3.\\nX= (ReLU (XW down))Wup+X, (6)\\nWdown‚ààRd√ók, Wup‚ààRk√ód.\\nInspired by sequential adapter, many adapter-based PEFT\\nmethods have been proposed. Residual Adapter [14] fur-\\nther improves efficiency by inserting the adapter module\\nonly after the feed-forward and layer normalization. Parallel\\nAdapter [16], [76] inserts the adapter network in parallel\\nwith both the attention layer and the feed-forward layer,\\nallowing for more efficient integration of the adapter module\\ninto the transformer. AdapterDrop [17] removes adapters\\nin each layer of the transformer that are not important to\\nthe given task to improve inference efficiency. While CoDA\\n(Condition Adapter) [15] employs parallel adapter for task-\\nspecific parameter fine-tuning and remains most pretrained\\nparameters fixed. However, unlike prior methods in which all\\ninput tokens are processed with pretrained transformer, CoDA\\nutilizes a router function to select kimportant input tokens\\nfor conditional computation. In this way, CoDA not only\\nenhances parameter efficiency but also inference efficiency.\\nTiny-Attn Adapter (Tiny-Attention Adapter) [18] introduces\\na dot-product attention module between the down- and up-\\nprojections, which can also be seen as a multi-head attention\\nmodule with its per-head dimensionality to be extremely small.\\nMoreover, the Tiny-Attn Adapter regards its multiple attention\\nheads as a mixture of experts and averages their weights to\\nfurther reduce inference costs. Akin to the sequential adapter,\\nthe Tiny-Attn Adapter is also injected right after the multi-\\nhead attention layer.\\nAdapterFusion [19] integrates multiple task-specific\\nadapters into a single adapter module, allowing for effective\\nknowledge transfer across related tasks without modifying\\nthe original pretrained model. AdapterFusion provides a prac-\\ntical and efficient approach to task composition, enabling\\nthe transferability of pretrained models across multiple tasks\\nwhile minimizing the computational costs associated with fine-\\ntuning the entire model. However, AdapterFusion requires\\nadditional trainable parameters in the composition layers,\\nincreasing computational costs. MerA (Merging Pretrained\\nAdapters) [20] adopts summation and averaging strategies to\\nmerge the parameters of pretrained adapters without introduc-\\ning extra trainable parameters. It employs the optimal transport\\nmethod [78], [79] to align the parameters of adapters based\\non weights and activations, which gives better performance\\nwith fewer trainable parameters compared to AdapterFusion.\\nHyperformer++ [21] utilizes the shared hypernetwork [80]\\nto learn task-specific and layer-specific adapter parameters\\nthat condition on task and layer id embeddings. By sharing'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 4}, page_content='5\\nMulti-Head Attention\\nLayer Normalization\\nLayer NormalizationFeed-Forward Network+\\n+Adapter NetworkAdapter Network\\nNonlinear \\nActivation\\n+Down-projection\\nUp-projectionk d\\nk d\\n(a) Sequential Adapter\\nMulti-Head Attention\\nLayer Normalization\\nLayer NormalizationFeed-Forward Network+\\n+Hidden States\\nWq Wk Wv\\nQ K V Pk Pv\\nAttention\\nPr efix-tuning (b) Prefix-tuning\\nMulti-Head Attention\\nLayer Normalization\\nLayer NormalizationFeed-Forward Network+\\n+Hidden States\\nWq Wk Wv\\nQ K V\\nAttentionLoRA\\nLoRA\\nLoRA\\n+ + +\\nDown-\\nprojectionUp-\\nprojection *LoRA (c) LoRA\\nFig. 3: The detailed architecture of (a) Sequential Adapter , (b) Prefix-tuning , and (c) LoRA .\\nknowledge across tasks via hypernetworks while enabling the\\nmodel to adapt to each task through task-specific adapters,\\nsignificantly reducing the number of trainable parameters.\\nAdapterSoup [22] is developed to address cross-domain task\\nadaptation, which first trains multiple adapters based on var-\\nious domains and then employs domain clustering [81] to\\nselect the most appropriate top- kadapters for the new domain.\\nFine-tuning parameters for the new domain in AdapterSoup\\nare determined by calculating the weighted average of the\\nselected kadapters. Apart from cross-domain task adaptation,\\nAdapterSoup can also be used to strengthen in-domain results\\nvia weight averaging of adapters trained on the same domain\\nbut with different hyperparameters.\\n2) Soft Prompt-based Fine-tuning: Soft prompt fine-tuning\\nis a class of methods in which trainable continuous vectors,\\nknown as soft prompts, are inserted into the input or hidden\\nstate of the model. Unlike manually designed hard prompts,\\nsoft prompts are generated by searching for prompts in a\\ndiscrete token space based on task-specific training data. Soft\\nprompts exhibit more flexibility and adaptability during fine-\\ntuning, as these prompts can be optimized and adjusted based\\non the specific task and training data.\\nWARP (Word-level Adversarial RePrograming) [23] in-\\nserts special prompt tokens [P1],[P2],¬∑¬∑¬∑,[Pl]and [Mask]\\ntoken before or after the sentences relying on the prompt\\ntemplate. The training objective is to minimize the cross-\\nentropy loss between the output of MLM and the verbalizer\\ntokens [V1],[V2],¬∑¬∑¬∑,[Vc]for classes {1,2,¬∑¬∑¬∑, c}. Only the\\nparameters of [P1],[P2],¬∑¬∑¬∑,[Pl]and[V1],[V2],¬∑¬∑¬∑,[Vc]are\\ntrainable, resulting in a significant reduction in the number\\nof fine-tuning parameters. Prompt-tuning [24] incorporates\\nadditional llearnable prompt tokens, P= [P1],[P2],¬∑¬∑¬∑,[Pl],\\ninto the model input X‚ààRn√ódand then concatenates them to\\ngenerate the final input ÀÜX, the new input can be expressed with\\nEquation 7. During fine-tuning, only the prompt parameters\\nofPare updated through gradient descent, while pretrained\\nparameters remain frozen. Thus, the parameter cost of prompt-\\ntuning is determined by multiplying the prompt length by\\nthe token embedding dimension, and extending the promptlength beyond a single token is critical for achieving good\\nperformance.\\nÀÜX=Concat (P, X) = [P, X]‚ààR(l+n)√ód. (7)\\nPrefix-tuning [10] proposes to prepend soft prompts P=\\n[P1],[P2],¬∑¬∑¬∑,[Pl](ldenotes the length of the prefix) to\\nthe hidden states of the multi-head attention layer, differing\\nfrom prompt-tuning that adds soft prompts to the input. To\\nensure stable training, a FFN is introduced to parameterize\\nthe soft prompts, as direct optimization of the soft prompts\\ncan lead to instability. Two sets of prefix vectors ÀÜPkand\\nÀÜPvare concatenated to the original key ( K) and value\\n(V) vectors of the attention layer. The self-attention mech-\\nanism with prefix-tuning can be represented by Equation 8.\\nDuring training, only ÀÜPk,ÀÜPv, and the parameters of FFN\\nare optimized, while all other parameters of PLMs remain\\nfrozen. The structure of prefix-tuning is illustrated in Fig. 3.\\nAfter training, the FFN is discarded, and only PkandPv\\nare used for inference. P-tuning [25] also considers insert-\\ning the soft prompts [P1],¬∑¬∑¬∑,[Pi],[Pi+1],¬∑¬∑¬∑,[Pl]into the\\nmodel input. Nonetheless, P-tuning differs by concatenating\\nthese prompts to form a template and maps it to obtain\\n{h1,¬∑¬∑¬∑, hi, e(x), hi+1,¬∑¬∑¬∑, hl, e(x)}, in which erepresents\\npretrained embedding layer. The training goal is to optimize\\nthe continuous prompts {h1,¬∑¬∑¬∑, hl}. As the weights of PLMs\\nare fixed and only a few parameters need to be fine-tuned,\\nthe template can be effectively learned in few-shot learning\\nscenarios. P-tuning employs a bidirectional long short-term\\nmemory network (LSTM) with a ReLU-activated multilayer\\nperceptron (MLP) to initialize the embedding of soft prompts\\nthrough MLP(LSTM( h1,¬∑¬∑¬∑, hi): LSTM( hi,¬∑¬∑¬∑, hl)).\\nhead=Attn(XW q,[ÀÜPk, XW k],[ÀÜPv, XW v]), (8)\\nÀÜPk=FFN(Pk),ÀÜPv=FFN(Pv). (9)\\nSPOT (Soft Prompt Transfer) [26] is a multitask prompt\\nmethod that builds upon the prompt-tuning, in which ‚Äúprompt\\npertaining‚Äù is introduced between PLMs and prompt-tuning\\nof target tasks. There are two variants of SPOT: generic'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 5}, page_content='6\\nSPOT and targeted SPOT. Generic SPOT first learns a generic\\nprompt on one or more source tasks and then employs the\\nlearned prompt to initialize target prompts for specific target\\ntasks. Targeted SPOT learns separate prompts for various\\nsource tasks, creating a source prompt library. Subsequently,\\nthe optimal source prompt, which exhibits higher similarity to\\nthe target task embedding, is retrieved and used to initialize\\nthe target prompt for the target task. ATTEMPT (ATTEntional\\nMixtures of Prompt Tuning) [27] begins by pretraining trans-\\nferable soft prompts (source prompts) on large-scale source\\ntasks that possess valuable knowledge applicable to other\\ntasks. The new target prompt is initialized specifically for a\\ngiven target task. ATTEMPT employs a shared and lightweight\\nnetwork that is trained simultaneously to learn an attention-\\nweighted combination of source prompts and target prompt.\\nThis enables modular multi-task learning, as pretrained soft\\nprompts can be flexibly combined, reused, or removed to\\nleverage knowledge from different tasks. MPT (multitask\\nprompt tuning) [28] utilizes multitask data to decompose and\\ndistill knowledge from the source prompts to learn a single\\nshared prompt. MPT then learns a multiplicative low-rank\\nmatrix to update the shared prompt, efficiently adapting it to\\neach downstream target task. Specifically, MPT assumes that\\nthe soft prompts for the k-th source task, denoted as ÀÜPk, can be\\ndecomposed into the shared prompts across all source tasks P‚àó\\nand a task-specific low-rank matrix Wk. The decomposition is\\ngiven by ÀÜPk=P‚àó‚äôWk=P‚àó‚äô(uk‚äóvT\\nk), where ‚äôdenotes\\nthe Hadamard product, ‚äódenotes the Kronecker product, and\\nukandvkare task-specific vectors for the task k.\\n3) Others: Apart from adapters family and soft prompts\\nfine-tuning methods, there are some other approaches that also\\nincorporate extra trainable parameters during fine-tuning. They\\ninvolve adding a ladder side network operating alongside the\\ntransformer, introducing an additional diff vector to rescale\\nthe attention, incorporating an extra vector to the special token\\nrepresentations, using the late fusion technique to integrate ad-\\nditional attention weight, or combing an extra joint importance\\nweight for each token representation.\\nLST (Ladder Side-Tuning) [29] trains a ladder side network\\nin conjunction with the pretrained network and takes interme-\\ndiate activations as input via shortcut connections, known as\\nladders, from pretrained network. Since all training parameters\\nare stored in the ladder side network, back-propagation is\\nachieved through side networks and ladder connections rather\\nthan pretrained networks, reducing the number of fine-tuned\\nparameters. In addition, LST further boosts parameter effi-\\nciency by utilizing structural pruning [82] to retrieve a smaller\\npruned network to initialize the side network and dropping\\ncertain layers of the side network. (IA)3(Infused Adapter\\nby Inhibiting and Amplifying Inner Activations) [30] lever-\\nages learned vectors to scale activations, leading to improved\\nperformance while introducing a relatively small number of\\nnew parameters. (IA)3introduces three learned vectors, lk,lv,\\nandlff, to rescale the key (K) and value (V) vectors in the\\nattention networks and hidden activations in the position-wise\\nFFN. As a result, the attention output and hidden activationoutput can be rescaled using the following expressions:\\nAttn(Q, K, V ) = (Q(lk‚äôKT)‚àödk)(lv‚äôV), (10)\\nFFN(X) = (lff‚äôŒ≥(XW 1))W2, (11)\\nin which ‚äôrepresents element-wise multiplication, W1and\\nW2are the weight matrices of FFN, and Œ≥is activation\\nfunction. (IA)3only optimizes three learned vectors lk,lv, and\\nlfffor each transformer block, resulting in great parameter\\nefficiency. Notably, (IA)3incurs minimal overhead because lk\\nandlvcan be seamlessly integrated into the corresponding\\nlinear layers, with the only additional overhead arising from\\nlff.PASTA (PArameter-efficient tuning with Special Token\\nAdaptation) [31] improves parameter efficiency by modifying\\nthe special token representations (e.g., [SEP] and [CLS] in\\nBERT) with an extra special trainable vector before the self-\\nattention layer at each transformer layer. Assuming that the\\ninputs to the transformer layer are denoted as H={hi}N\\ni=1,\\nPASTA modifies the inputs as follows:\\nHmod={hi+mi}N\\ni=1, (12)\\nmi=e(vp), iis the p-th special token ;otherwise, mi= 0,\\nmiis the special token adaptation. PASTA enables a re-\\nmarkable reduction in trainable parameters by training only\\nthe trainable vector e(vp)to update the representations of\\nspecial tokens. The reasons for using [CLS] and [SEP] as\\nspecial tokens in PASTA are that the [CLS] representation\\nprovides a global representation of the input text and that\\nthe attention scores in PLMs are primarily allocated to the\\n[CLS] or [SEP] tokens across attention heads [83], [84].\\nAttentionFusion [32] introduces the late fusion technique,\\nwhich involves combining features or representations from\\ndiverse tasks or layers to generate a final joint representation,\\nto adjust the the importance of each token representation. For\\na given task t, let the attention query vector be denoted by Qt\\nand the representation of token iat layer jbeVj\\ni, then the\\nrepresentation of token ifor task t,ÀÜVj\\ni, is expressed as:\\nÀÜVj\\ni=X\\njŒ±j\\ni(t)Vj\\ni, Œ±j\\ni(t) =exp(QtVj\\ni)P\\nkexp(QtVj\\ni), (13)\\nwhere Œ±j\\ni(t)represents the attention weight of token iat layer\\njfor task t. The number of extra parameters that need to be\\nupdated in AttentionFusion is determined by the size of the\\nquery vector Qt, which is the same as the hidden dimension of\\nthe pretrained encoder. By employing the attention weight as\\nextra trainable parameters, AttentionFusion adjusts the impor-\\ntance of each token representation dynamically. Hadamard\\nAdapter [33] is an additive fine-tuning method that intro-\\nduces a weight vector and a bias vector in each transformer\\nwith the same dimensions as the output of the multi-head\\nattention module for fine-tuning. A weight vector and a bias\\nvector are injected right after the multi-head attention layer\\nto perform element-wise multiplication (Hadamard product)\\nwith the multi-head attention outputs. Notably, the number\\nof Hadamard adapter is the same as that of the transformer\\nlayers in the PLM. During fine-tuning, only the parameters in'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 6}, page_content='7\\nTABLE I: The weight update in pretrained weight masking and delta weight masking. ‚äôdenotes the Hadamard product.\\nMethod Weight Update Mask Criterion Mask Matrix\\nThreshold-Mask ÀÜW=M‚äôW Threshold M=Isi,j>œÑ\\nFISH Mask ÀÜW=M‚äôW Fisher information M=Itop‚àík(fi,j)\\nLT-SFT ÀÜW=W+M‚äô ‚àá WL(W) Absolute difference of parameters M=Itop‚àík(|W1‚àíW0|)\\nChild-Tuning FÀÜW=W‚àíM‚äôŒ∑‚àáWL(W) Bernoulli distribution M={0,1}n\\nChild-Tuning DÀÜW=W‚àíM‚äôŒ∑‚àáWL(W) Fisher information M=Itop‚àík(fi,j)\\nDiff Pruning ÀÜW=W+M‚äô‚àÜW Fixed sparsity M={0,1}n\\nSAM ÀÜW=W+M‚àÜW Analytical solution Mi,j= 0,‚àÄiÃ∏=j;Mi,i‚àà {0,1}\\nthe Hadamard adapter, layer normalization, and classifier are\\nupdated.\\nB. Partial Fine-tuning\\nPartial fine-tuning methods aim to reduce the number of\\nfine-tuned parameters by selecting a subset of pre-trained pa-\\nrameters that are critical to downstream tasks while discarding\\nunimportant ones. We categorize partial fine-tuning methods\\ninto three groups: Bias Update [34], [35], in which only\\nthe bias term in the attention layer, feed-forward layer and\\nlayer normalization of the transformer is updated, Pretrained\\nWeight Masking [36], [37], where the pretrained weights are\\nmasked using various pruning criterion, and Delta Weight\\nMasking [38], [39], [40], [41], in which delta weights are\\nmasked via pruning techniques and optimization approxima-\\ntion. A detailed analysis of pretrained weight and delta weight\\nmasking is provided in Table I.\\n1) Bias Update: Bit-Fit (Bias-term Fine-tuning) [34]\\nachieves parameter efficiency by only updating the bias terms\\nand the task-specific classification layer while keeping the\\nmajority of parameters in the transformer-based PLMs frozen.\\nThe bias parameters are involved in the attention layer, where\\nthey are involved in calculating query, key, value, and combin-\\ning multiple attention heads, as well as in the fee-forward and\\nlayer normalization layers. Further, U/S-BitFit [35] combines\\nNAS algorithm [85] and pruning technique to automatically\\ndetermine which parameters of the network need to be fine-\\ntuned based on BitFit. U-BitFit (Unstructured BitFit) decides\\nwhich PEFT parameters to prune based on the first-order\\napproximation of the change in training loss resulting from\\npruning the PEFT parameter W, i.e.,‚àíW¬∑‚àáWL(W). While\\nS-BitFit (Structured BitFit) sums the criterion over the overall\\nbias update ‚àÜb(bis the bias term).\\n2) Pretrained Weight Masking: Pretrained weight masking\\nemploys pruning criteria like threshold and Fisher information\\nto measure the importance of pretrained weight to construct\\na binary mask matrix for weight masking. Threshold-Mask\\n[36] utilizes the threshold to construct a binary mask ma-\\ntrix to select pretrained weights Wof the attention and\\nFFN layers through element-wise multiplication, expressed as\\nÀÜW=W‚äôM(‚äôdenotes the Hadamard product). To begin,\\na random uniformly distributed real-valued matrix S, which\\nshares the same dimensions as matrices WandM, is created.\\nSubsequently, if an element in Ssurpasses a predetermined\\nglobal threshold œÑ, the corresponding position in the binarymask matrix is assigned a value of 1; otherwise, it is assigned\\n0.FISH Mask (Fisher-Induced Sparse uncHanging) [37] uses\\nthe Fisher information of pretrained weight to measure their\\nimportance and construct a sparse binary mask. FISH Mask\\nselects the top- kparameters with the largest Fisher information\\nto construct the sparse binary mask, where the positions\\ncorresponding to the top- kparameters are set to be 1 and\\nthe rest are set to 0. Note that kis preset based on the desired\\nmask sparsity level of the mask, and the resulting sparse binary\\nmask can be reused across many subsequent iterations.\\n3) Delta Weight Masking: Delta weight masking also em-\\nploys various pruning techniques and criteria to construct a\\nbinary mask matrix to reduce trainable parameters. However,\\nDelta weight pruning typically involves an update at each\\niteration. LT-SFT (Lottery Ticket Sparse Fine-Tuning) [38]\\nis a novel PEFT method inspired by the Lottery Ticket\\nHypothesis2[86]. LT-SFT first fine-tunes the PLM on target\\ndata using pretrained parameters W0to obtain the fully fine-\\ntuned parameters W1, and then identifies the top- kpretrained\\nparameters with the greatest absolute differences ( |W1‚àíW0|).\\nThe top- kparameters are selected for further fine-tuning using\\nbinary mask M, in which the positions corresponding to the\\nselected kparameters are set to 1 and the remaining positions\\nto 0. LT-SFT then resets the model parameters to their original\\npretrained weights W0but fine-tunes only the selected k\\nparameters while keeping the remaining parameters frozen,\\nand can be expressed as Œ¥=M‚äô‚àÜW(‚àÜW=‚àáWL(W). By\\niteratively repeating this process, the method gradually fine-\\ntunes only a small fraction of the model‚Äôs parameters. Child-\\nTuning [39] calls the network formed by the parameters to be\\nupdated a child network and masks out the gradients of non-\\nchild networks to improve parameter efficiency. The parameter\\nupdates in Child-Tuning is expressed as Œ¥=M‚äô‚àÜW\\n(‚àÜW=Œ∑‚àáWL(W),Œ∑denotes learning rate). Child-Tuning\\nprovides two variants: Child-Tuning F(Fstands for Task-\\nFree) and Child-Tuning D(Dstands for Task-Driven). Child-\\nTuning Fgenerates the binary mask matrix Musing Bernoulli\\ndistribution with a probability denoted as p F. Increasing the\\nvalue of p Fupdates a larger number of parameters, and Child-\\nTuning Fis equivalent to full fine-tuning when p F= 1. In\\ncontrast, Child-Tuning Duses Fisher information estimation to\\nidentify a subset of parameters (i.e., child network) that are\\nhighly correlated with a specific downstream task. The binary\\n2Lottery Ticket Hypothesis states that each neural model contains a sub-\\nnetwork (a ‚Äúwinning ticket‚Äù) that can match or even outperform the perfor-\\nmance of the original model when trained in isolation.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 7}, page_content='8\\nmask matrix in Child-Tuning Dis constructed by setting the\\nposition of the child network to be 1 and the non-child network\\nto be 0.\\nDiff Pruning [40] introduces a sparse task-specific ‚Äúdiff‚Äù\\nvector Œ¥during fine-tuning while remaining the pretrained\\nmodel parameters fixed. To make the diff vector Œ¥sparse,\\nDiff Pruning introduces a learnable binary mask Mon the\\nDelta weight and decomposes Œ¥=M‚äô‚àÜW. The binary\\nmask Mis learnable and is used as a regularizer during fine-\\ntuning. It acts as a differentiable approximation to the L0-\\nnorm of diff vector Œ¥. This approach is well-suited for multi-\\ntask deployment in edge (mobile) applications with limited\\nstorage. Significantly, Diff pruning incurs higher memory\\nconsumption compared to traditional fine-tuning, which may\\nbecome problematic as model sizes continue to grow. SAM\\n(Second-order Approximation Method) [41] also employs the\\nsparse mask matrix to update the delta weight. However, SAM\\ndirectly optimizes the approximation function to obtain an\\nanalytical solution for the mask matrix, which is then used to\\nupdate the pretrained weight. Concretely, SAM [41] views the\\nPEFT methods as p-sparse fine-tuned model by representing\\nfine-tuned parameter as W=W0+M‚àÜW,Mis a mask\\nmatrix, and the optimization problem is\\nmin‚àÜW,ML(W0+M‚àÜW), s.t.\\n‚à•M‚à•0=‚åämp‚åã, Mi,j= 0,‚àÄiÃ∏=j;andMi,i‚àà {0,1}.\\nSAM approximates the loss function using its second-order\\nTaylor expansion as:\\nL(W0+M‚àÜW)‚âàL(W0) + ‚àÜL(W0)T‚àÜL(W0)TM‚àÜW\\n+1\\n2(M‚àÜW)THM‚àÜW, (14)\\nin which His the Hessian matrix. In practice, SAM first\\nobtains the gradient ‚àáL(W0)ifor the i-th parameter Wi, then\\ncalculates\\x0c\\x0c‚àáL(W0)2\\ni\\x0c\\x0c, and selects the top ‚åämp‚åãdelta weight\\nfor optimization.\\nC. Reparameterized Fine-tuning\\nReparameterized fine-tuning methods utilize low-rank trans-\\nformation to reduce the number of trainable parameters while\\nallowing operating with high-dimensional matrices (e.g., pre-\\ntrained weights). We categorize reparameterized fine-tuning\\nmethods into two groups: Low-rank Decomposition [11],\\n[42], [43], in which various low-rank decomposition tech-\\nniques are used to reparameterize the updated matrix, and\\nLoRA derivatives [44], [45], [46], [47], [48], [49], [50], [51],\\n[52], [53], [54], [55], [56], [57], where a series of PEFT\\nmethods are developed based on LoRA. Specific details of\\n‚àÜWparameters reparameterization of various approaches can\\nbe seen in Table II.\\n1) Low-rank Decomposition: This involves finding a lower-\\nrank matrix that captures the essential information of the\\noriginal matrix while reducing computational complexity and\\nmemory usage by reparameterizing the updated delta weight.\\nReparameterization covers transforming the delta weight ma-\\ntrix into a low-rank representation using methods such asFastfood transformation, low-rank down-up projection, or Kro-\\nnecker product projection.\\nIntrinsic SAID (Structure-Aware Intrinsic Dimension) [42]\\nleverages the concept of intrinsic dimensionality to reduce\\nthe number of parameters during fine-tuning. The intrinsic\\ndimensionality refers to the minimum dimensionality required\\nto solve a high-dimensional optimization problem. In the\\ncontext of PLMs, measuring the intrinsic dimensionality helps\\nestimate the minimum number of parameters needed to adapt\\nto new tasks. Instead of optimizing the empirical loss in\\nthe original parameterization, Intrinsic SAID fine-tunes the\\nmodel by reparametrization the model in a lower-dimensional\\nspace, i.e., ‚àÜW=F(Wr), in which Wris the parameter\\nto be optimized and F:Rr‚ÜíRdis a Fastfood transform3\\n[87] that projects parameters from low-dimensional rto high-\\ndimensional d. However, Intrinsic SAID is not practical for\\nfine-tuning larger networks due to the O(d)memory com-\\nplexity of the Fastfood transform and the need to update all\\nof the model‚Äôs parameters.\\nInspired by Intrinsic SAID, LoRA (Low-Rank Adaptation)\\n[11] introduces two trainable low-rank matrices for weight\\nupdate. In LoRA, a down-projection matrix and an up-\\nprojection matrix are utilized in parallel with the query (Q),\\nkey (K), and value (V) matrices in the attention layer of the\\ntransformer, shown in Fig. 3. For a pretrained weight matrix\\nW‚ààRd√ók, LoRA updates Wusing low-rank decomposition\\n‚àÜW=WdownWup. During training, the weights of PLM\\nare frozen, and only the low-rank matrices of LoRA, i.e.,\\nWdown‚ààRd√órandWup‚ààRr√ókare fine-tuned ( r‚â™ {d, k}).\\nDuring inference, the LoRA weights are merged with the\\noriginal weight matrix of the PLMs without increasing the\\ninference time. Practically, a scaling factor ( s= 1/r) is added\\nto the LoRA module. KronA (Kronecker Adapter) [43] is\\nstructurally similar to LoRA but replaces the low-rank de-\\ncomposition in LoRA with Kronecker product decomposition,\\n‚àÜW=Wdown‚äóWup. Kronecker product decomposition\\nmaintains the rank of the input matrix (i.e., rank (A‚äóB) =\\nrank(A)√órank(B)), ensuring that important information is\\npreserved during the adaptation process. Moreover, Kronecker\\nproduct can speed up computation and reduce the number\\nof required floating-point operations (FLOPS) by avoiding\\nthe explicit reconstruction of the Kronecker product matrix.\\nKronA has two variants: KronA Band KronAB\\nres. KronA B\\ninserts the KronA module in parallel to the FFN layer, while\\nKronAB\\nresinserts the KronA module alongside the FFN layer\\nand incorporates a learnable residual connection.\\n2) LoRA Derivatives: LoRA derivatives refer to a series\\nof PEFT methods that are improved based on LoRA, includ-\\ningLow-Rank Adjustment [44], [45], [46], where different\\nmethods are developed to adjust the rank of LoRA dynami-\\ncally, LoRA-guided Pretrained Weight Update [47], [48],\\nin which LoRA is used to guide the update of pretrained\\nweight, Quantization Adaption [49], [50], [51], in which\\nvarious quantization techniques are proposed to improve the\\nhigh precision fine-tuning and inference of LoRA, LoRA-\\n3Fastfood transform is a computationally efficient dimensionality expansion\\nmethod.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 8}, page_content='9\\nTABLE II: Delta weight reparameterization of various reparameterized fine-tuning methods.\\nMethod ‚àÜWReparameterization Notes\\nIntrinsic SAID ‚àÜW=F(Wr) F:Rr‚ÜíRd,Wr‚ààRris parameters to be optimized, and r‚â™d.\\nLoRA ‚àÜW=WdowmWup Wdown‚ààRk√ór,Wup‚ààRr√ód, and r‚â™ {k, d}.\\nKronA ‚àÜW=Wdown‚äóWup rank(Wdown‚äóWup) =rank(Wdown)√órank(Wup).\\nDyLoRA ‚àÜW=Wdown‚ÜìbWup‚Üìb Wdown‚Üìb=Wdown[:b,:],Wup‚Üìb=Wup[:,:b],b‚àà {rmin,¬∑¬∑¬∑, rmax}.\\nAdaLoRA ‚àÜW=PŒõQ PPT=PTP=I=QQT=QTQ,Œõ = diag( œÉ1, œÉ2, . . . , œÉ r).\\nIncreLoRA ‚àÜW=WdownŒõWup Œõ = [ Œª1, Œª2,¬∑¬∑¬∑, Œªr]withŒªicould be an arbitrary constant.\\nDeltaLoRA ‚àÜW=WdownWup W(t+1)‚ÜêW(t)+ (W(t+1)\\ndownW(t+1)\\nup‚àíW(t)\\ndownW(t)\\nup).\\nLoRAPrune ‚àÜW=WdownWup‚äôM Œ¥= (W+WdownWup)‚äôM,M‚àà {0,1}1√óG,Gis group number.\\nQLoRA ‚àÜW=WBF16\\ndown WBF16\\nup YBF16=XBF16doubleDequant (cFP32\\n1, cFP8\\n2, WNF4) +XBF16WBF16\\ndown WBF16\\ndown .\\nQA-LoRA ‚àÜW=WdownWup Wdown‚ààRk√ór,Wup‚ààRr√óL,Lis the quantization group number of W.\\nLOFTQ ‚àÜW=SVD(W‚àíQt) Qt=qN(W‚àíWt‚àí1\\ndownWt‚àí1\\nup),qNisN-bit quantization function.\\nKernel-mix ‚àÜWh= (BLoRA , Bh) \\nAh\\nLoRA\\nAh!\\nBLoRA is shared across all heads, Bh,Ahprovide rank- rupdate in each head.\\nLoRA-FA ‚àÜW=WdownWup=QRW up Wdown is frozen, and only update Wup.\\nbased Improvements [52], [53], [54], in which several novel\\ntechnique are incorporated into LoRA for improvements, and\\nLoRA-based Multi-task Fine-tuning [55], [56], [57], where\\nmultiple LoRA modules are combined for cross-task transfer\\nto fine-tune model on a novel task.\\nLow-rank Adjustment. DyLoRA (Dynamic LoRA) [44] is\\nintroduced to overcome two limitations of LoRA: (a) LoRA‚Äôs\\nrank is fixed and prevents any changes after training (b)\\ndetermining the optimal rank for LoRA requires exhaustive\\nsearch and considerable effort. DyLoRA trains LoRA modules\\nfor a range of ranks instead of a single rank, allowing\\nfor adaptability. DyLoRA addresses these limitations during\\ntraining by sorting the representations learned at various ranks.\\nSpecifically, DyLoRA operates within the range of ranks\\ndenoted as r‚àà[rmin, rmax]for a series of iterations. In\\neach iteration, DyLoRA randomly selects a specific rank b\\nfrom{rmin,¬∑¬∑¬∑, rmax}. It then truncates the down-projection\\nmatrix as Wdown‚Üìb=Wdown[:b,:]and the up-projection\\nmatrix as Wup‚Üìb=Wup[:,:b]and only update truncated\\nparameter matrices Wdown‚ÜìbandWup‚Üìb. The parameter up-\\ndates in each iteration of DyLoRA could be expressed as\\n‚àÜW=Wdown‚ÜìbWup‚Üìb. By allowing dynamic low-rank adap-\\ntation and search-free low-rank adaptation, DyLoRA reduces\\nthe computational cost and training time required to identify\\nthe optimal rank for a particular task. AdaLoRA (Adaptive\\nLow-Rank Adaptation) [45] extends LoRA by dynamically\\nadjusting the rank of matrices to control the allocation budget.\\nIn AdaLoRA, the incremental update ‚àÜWis reparameterized\\nusing singular value decomposition (SVD) and then truncates\\nthe smallest singular values, i.e., ‚àÜW=PŒõQ. Both P\\nandQare orthogonal matrices, and Œõis a diagonal matrix\\ncontaining the singular values {œÉ1, œÉ2, . . . , œÉ r}. Here, rrep-\\nresents the rank of the matrix Œõ. During training, PandQ\\nare initialized with Gaussian distribution with a regularizer\\nto ensure the orthogonality, while Œõis initialized with zero\\nand iteratively pruned to adjust the rank. AdaLoRA employs\\nthe sensitivity-base importance scoring [88], [89] with a new\\nmetric to prune the singular values of unimportant updates to\\nupdate the Œõ. By doing this, AdaLoRA effectively improvesparameter efficiency and allocation budgets. IncreLoRA [46]\\ndynamically incorporates trainable parameters into LoRA by\\nincreasing their ranks, guided by importance scores assigned\\nto each module during training. The allocation process assigns\\nlower ranks, possibly 0 to indicate no parameter updates,\\nto less important modules, while allocating higher ranks to\\nmore important modules. The parameter updates in IncreLoRA\\ncan be expressed as ‚àÜW=WdownŒõWup, in which Œõ =\\n[Œª1, Œª2,¬∑¬∑¬∑, Œªr]is a diagonal matrix with Œªicould be any\\narbitrary constant, ris the rank of the each LoRA module.\\nBesides, an upper bound on the rank is set for each module\\nto control the parameter growth. Additionally, IncreLoRA\\nintroduces a unique pretraining technique called ‚Äúadvance\\nlearning‚Äù, which ensures that the newly added parameters\\nin each module begin with favorable initial states. In this\\nway, it prevents insufficient training of subsequently added\\nparameters, allowing for effective utilization of the incremental\\nparameter allocation. Unlike LoRA, which operates on the\\nquery (Q), key (K), and value (V) projection modules of the\\nattention layer, the parameter updates are applied to all linear\\nlayers in IncreLoRA.\\nLoRA-guided Pretrained Weight Update. Delta-LoRA\\n[47] updates the pretrained weight Was well as two low-\\nrank matrices Wdown andWup, while using the same memory\\nas the original LoRA. The two low-rank matrices Wdown\\nandWupare automatically updated as usual. The pretrained\\nweight, however, leverages the mathematical property that\\n‚àáWL(W, W down, Wup) = ‚àáWdownWupL(W, W down, Wup)\\n(it is achieved by removing the dropout layer in the original\\nLoRA module) for parameters update. Specifically, Wis\\nupdated with the delta of the product of two low-rank matrices\\nin consecutive iterations, i.e., W‚ÜêW+ ‚àÜWdownWup=\\nW+ (Wdown(t+ 1)Wup(t+ 1)‚àíWdown(t)Wup(t)).Lo-\\nRAPrune [48] introduces a LoRA-guided pruning criterion,\\nwhich utilizes the weights and gradients of LoRA instead of\\nthe gradients of pretrained weights for importance estimation\\nto prune parameters of LoRA and pretrained weights. To\\naddress the substantial memory overhead associated with un-\\nstructured pruning and dependency-aware structured pruning,'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 9}, page_content='10\\nLoRAPrune devises a structured iterative pruning procedure\\nthat selectively eliminates redundant channels and heads.\\nLoRA-guided pruning criterion involves using low-rank matri-\\ncesWdown andWup, along with their corresponding gradients\\n‚àáWdown and‚àáWup, to calculate the importance score4. This\\nscore determines which weights are deemed unimportant and\\nsubsequently removed. Notably, LoRAPrune not only prunes\\nstructured weights, such as heads and channels, from the pre-\\ntrained weights, but also prunes the corresponding weights in\\nthe LoRA, i.e., Œ¥= (W+WdownWup)‚äôM,M‚àà {0,1}1√óG,\\nGis the group number. Binary mask Mis set to 0 when\\nthe corresponding group is unimportant, and 1 when it is\\nimportant. Therefore, after pruning and fine-tuning, the LoRA\\nweights can seamlessly merge with the pretrained weights,\\nensuring that no additional computations are necessary during\\ninference.\\nQuantization Adaption. QLoRA [49], a quantized variant\\nof LoRA, effectively addresses the limited computational\\nresource of LoRA for fine-tuning LLMs by quantizing the\\ntransformer model to 4-bit NormalFloat (NF4) precision with\\ndouble quantization processing, and using a paged optimizer\\nto deal with memory spikes. NF4 is a new data type that\\nis theoretically optimal for normally distributed weights. Al-\\nthough QLoRA quantizes pretrained weight Wfrom FP16 into\\nNF4 so that LLMs can be fine-tuned with fewer GPUs, the\\nauxiliary weight of LoRA matrix WdownWupmakes the final\\nweight return to FP16 again after fine-tuning. To this end, QA-\\nLoRA (Quantization-Aware Low-rank Adaption) [50] employs\\ngroup-wise quantization with low-rank adaptation to the pre-\\ntrained weight W, in which each column of Wis partitioned\\nintoLgroups for quantization. In this way, QA-LoRA ensures\\nthat pretrained weights Wand auxiliary weights are integrated\\ninto a quantized form after fine-tuning, resulting in a faster and\\nmore accurate computation during inference. While LOFTQ\\n(LoRA-Fine-Tuning-aware Quantization) [51] applies an N-bit\\nquantized weight Qand low-rank approximation Wdown‚àà\\nRd1√ór,Wup‚ààRd2√órto approximate the original high-\\nprecision pretrained weight W‚ààRd1√ód2as the initialization\\nof LoRA fine-tuning. Such an initialization alleviates the\\nquantization discrepancy in QLoRA and significantly improves\\nthe generalization in downstream tasks.\\nLoRA-based Improvements. Kernel-wise Adapter [52]\\ntreats the different attention heads in the transformer as\\nindependent kernel estimators and utilizes the kernel structure\\nin self-attention to guide the assignment of tunable parameters.\\nLoRA is used as the underlying model to combine kernel-wise\\nadaptation for its flexibility in parameter assignment for dif-\\nferent weight matrices. Kernel-wise adapter has two variants:\\nKernel-mix-lite (qv) and Kernel-mix-lite (qvo). Kernel-mix-\\nlite (qv) provides a lightweight solution for scenarios with\\nlimited parameter budgets, while Kernel-mix (qvo) is suitable\\nfor scenarios with intermediate parameter budgets. The suffix\\n(qv) means that the method will adjust WqandWv, while the\\nsuffix (qvo) means that the method will modify Wq,Wv, and\\nWo.Laplace-LoRA [53] incorporates Bayesian inference into\\n4Importance score Iis calculate via I=‚àáW‚äôW,‚àáW‚âàWdown¬∑\\n‚àáWup+‚àáWdown¬∑Wup‚àí ‚àáWdown¬∑ ‚àáWup.the LoRA parameters to address the issue of overconfidence\\nand improve calibration. A key challenge lies in obtaining\\nthe posterior distribution for Bayesian inference, which is\\nresolved by using Laplace approximation [90]. Laplace-LoRA\\ncan be viewed as an approximation of the posterior distribution\\nover LoRA parameters using Laplace approximation. Hence,\\nLaplace-LoRA maintains existing pretraining and fine-tuning\\nprocedures while reducing the dimensionality of Bayesian\\ninference. LoRA-FA (LoRA with Frozen-A) [54] is pro-\\nposed to reduce the expensive activation memory of LoRA\\nwithout introducing any computational overhead. LoRA-FA\\nkeeps the pretrained weight Wand down-projection matrix\\nWdown frozen and only updates the up-projection matrix Wup.\\nWdown is decomposed into QandRvia QR decomposi-\\ntion, and ‚àÜW=WdownWup=QRW up=QÀÜWup=Pr\\ni=1Q:,iÀÜWup,i,:, in which {Q:,i}r\\ni=1are orthogonal unit vec-\\ntors (ris the rank of Wdown ). Thus, ‚àÜWis a combination of\\nrorthogonal vectors, limiting the change of weight residing in\\na low-rank space. Consequently, there is no need to store full-\\nrank input activations simultaneously, alleviating the memory\\nburden associated with activation storage.\\nLoRA-based Multi-task Fine-tuning. LoRAHub [55]\\nleverages a composition of multiple trained LoRA modules\\nfor cross-task transfer to fine-tune the model on new tasks.\\nSpecifically, LoRAHub trains task-specific LoRA modules in\\na variety of tasks to obtain a synthesized module, ÀÜm=\\n(w1W1\\ndown+¬∑¬∑¬∑+wNWN\\ndown)(w1W1\\nup+¬∑¬∑¬∑+wNWN\\nup), which\\nis then amalgamated with the LLMs to adapt the new task.\\nThus, the objective of LoRAHub is to find the best weight\\nset{w1, w2,¬∑¬∑¬∑, wN}, which is achieved by the gradient-free\\ncombinatorial optimization approach Shiwa [91]. MOELoRA\\n[56] combines LoRA with mixture-of-experts (MoE) for multi-\\ntask fine-tuning, in which each expert is a LoRA module\\nfor learning task-specific knowledge. Additionally, MOELoRA\\ndevises a task-motivated gate function to produce distinct\\nfine-tuned parameters for various tasks. L-LoRA (Linearized\\nLoRA) [57] is a linearized PEFT method to improve the\\nmulti-task fusion capability of fine-tuned task-specific models\\nwith low computation costs. L-LoRA constructs a linear\\nfunction using a fir-order Taylor expansion, as illustrated in\\nEquation 15. In L-LoRA, only the linearized LoRA modules\\nare fine-tuned in the tangent space, incurring fewer trainable\\nparameters compared to LoRA. For the multi-task fusion meth-\\nods, simple average, task arithmetic [92], [93], ties-merging\\n[94], and LoRAhub [55] are employed for multi-task fusion.\\nfŒ∏0(x;œï(t))‚âàflin\\nŒ∏0(x;œï(t)) =fŒ∏0(x;œï(0))\\n+‚àáœïfŒ∏0(x;œï(0))T(œï(t)‚àíœï(0)). (15)\\nD. Hybrid Fine-Tuning\\nHybrid fine-tuning approaches aim to combine various\\nPEFT approaches, such as adapter, prefix-tuning, and LoRA,\\nto leverage the strengths of each method and mitigate their\\nweaknesses. By integrating different features of PEFT meth-\\nods, hybrid fine-tuning achieves improved overall perfor-\\nmance compared to individual PEFT methods. These works\\nare classified into two approaches: Mannual Combination'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 10}, page_content='11\\n[16], [35], [58], [59], in which multiple PEFT methods are\\ncombined manually by sophisticated design, and Automatic\\nCombination [60], [61], [62], where various PEFT methods\\nare incorporated automatically via structure search.\\n1) Manual Combination: Manual combination mainly in-\\nvolves integrating the structure or features of one PEFT\\nmethod into another PEFT method to enhance performance\\nwhile achieving parameter efficiency. MAM Adapter (Mix-\\nAnd-Match Adapter) [16] is the combination of scaled parallel\\nadapter and prefix-tuning, which employs prefix-tuning with\\nsmaller bottleneck dimensions at the attention layer and al-\\nlocates more parameter budget to modify the representation\\nof FFN using the scaling parallel adapter. Scaled parallel\\nadapter denotes the parallel adapter with a scaling factor\\nto adjust the adapter output. Concretely, the output of the\\nMAM adapter hcan be expressed with h=LN(X+\\nscale‚àóFFN(LN(Attn([Pk, X]) + [ Pv, X])))for the input X.\\nFurther, U-MAM (Unstructured MAM) and S-MAM (struc-\\ntured MAM) [35] are proposed by combining NAS algorithm\\n[85] and pruning technique to automatically determine which\\nparameters of the network need to be fine-tuned based on\\nMAM adapter. NAS algorithm takes the maximum number\\nof parameters required for PEFT architectures as input and\\napplies the pruning operation to reduce trainable parameters.\\nThe criteria for deciding which PEFT parameters to prune\\nare based on the first-order approximation of the change in\\ntraining loss resulting from pruning the PEFT parameter W,\\ni.e.,‚àíW¬∑‚àáWL(W). U-MAM directly employs this criterion\\nto prune the parameters in MAM, while S-MAM sums the\\ncriterion over each column of Wdown .\\nCompacter [58] is developed based on adapters, low-\\nrank optimization, and a parameterized hypercomplex mul-\\ntiplication (PHM) layer [95]. It follows a similar structure\\nto adapters, consisting of a down-projection, a nonlinear\\nactivation function, and an up-projection. However, Compacter\\nreplaces the down-projection and up-projection in the adapters\\nwith the low-rank parameterized hypercomplex multiplication\\n(LPHM) layer, which is an extension of PHM that incorporates\\nlow-rank optimization. Structurally, PHM layer resembles a\\nfully connected layer, but with the learned Wrepresented as a\\nsum of Kronecker products, i.e., W=Pn\\ni=1Ai‚äóBi. Notably,\\nwhen the weights of down-projection and up-projection are\\ncalculated as in that of the PHM layer, Aiis a shared parameter\\nacross all adapter layers, while Birepresents adapter-specific\\nparameters. This kind of adapter is called PHM Adapter .\\nSimilarly, Compacter obtains the weight matrix in each LPHM\\nlayer utilizing the sum of Kronecker products, but Compacter\\nreparameterizes Bias the product of two independent ranks\\nwith one weight, and the weight matrix in Compacter is\\ncalculated as follows:\\nW=nX\\ni=1Ai‚äóBi=nX\\ni=1Ai‚äó(sitT\\ni). (16)\\nW‚ààRk√ód, Ai‚ààRn√ón, Bi‚ààRk\\nn√ód\\nn;si‚ààRk\\nn√ór, ti‚ààRr√ód\\nn.\\nCompacter++ is a variant of Compacter that inserts a Com-\\npacter layer after the FFN layer of each transformer module\\nand requires fewer parameters to be updated than Compacter.UniPELT [59] incorporates sequential adapter, prefix-\\ntuning, and LoRA via a gating mechanism. In UniPELT,\\nadapters are added after the feed-forward layer, prefix-tuning\\nis employed to the key ( K) and value ( V) vectors of the multi-\\nhead attention layer, and LoRA is used in attention matrices of\\nWqandWvof the transformer. Each PEFT module is equipped\\nwith a gating mechanism composed of a linear function with\\nthe dimension of the output being 1, a sigmoid function,\\nand a mean function. The gating mechanism controls the\\nactivation of each submodule, dynamically assigning higher\\nweights to submodules that make positive contributions to\\na given task. The trainable parameters encompass low-rank\\nLoRA matrices Wdown andWup, prefix-tuning parameters\\nPkandPv, adapter parameters, and weights for the gating\\nfunction. Consequently, UniPELT requires more parameters\\nand inference time than adapter, prefix-tuning, and LoRA, but\\nachieves better performance compared with the performance\\nof the best individual PEFT method.\\n2) Automatic Combination: Automatic combination ex-\\nplores how to configure PEFT methods like adapters, prefix-\\ntuning, BitFit, and LoRA to different layers of the transformers\\nautomatically using various structure search and optimization\\napproaches. However, it typically requires more time and cost\\ndue to the need to perform optimization searches in the model\\nor structure. AutoPEFT [60] integrates sequential adapter,\\nparallel adapter, and prefix-tuning into the transformer block.\\nThe serial adapter receives the hidden state from the FFN\\noutput as input, while the parallel adapter takes the hidden\\nstate before the FFN layer as its input. In addition, the prefix-\\ntuning module concatenates two prefix vectors, PkandPv,\\nwith the original key and value vectors, respectively, enabling\\nmulti-head attention to adapt to specific target tasks. Motivated\\nby the success of NAS algorithm, AutoPEFT proposes to use\\nthe Bayesian optimization approach to automatically search\\nfor an appropriate neural architecture network that selectively\\nactivates certain layers to incorporate these PEFT modules.\\nBayesian optimization is not only sample-efficient and zeroth-\\norder but also well-suited for multi-objective setups, enabling\\ncost-efficient optimization and facilitating the trade-off be-\\ntween performance and cost. Moreover, it is more paralleliz-\\nable during search, which can decrease memory usage.\\nS3Delta-M (Search for Sparse Structure of Delta Tun-\\ning Mix) [61] is a mixture of LoRA, Compacter (low-\\nrank adapter), BitFit, and LNFit5. Different from the simple\\nincorporation of PEFT techniques, S3Delta-M is developed\\nby conducting a differentiable delta tuning structure search.\\nIt explicitly controls sparsity and searches for an optimal\\ncombination of these techniques in a unified search space.\\nIn S3Delta-M, each PEFT module (LoRA, Compacter, BitFit,\\nand LNFit) is inserted into the corresponding layers of the\\nPLM to ensure the best performance is achieved. The specific\\ncombination and placement of these modules are determined\\nthrough the structure search process, which is guided by\\nexplicit sparsity control. S4[62] is a combination of Sequential\\nAdapter, Prefix-tuning, BitFit, and LoRA. Unlike previous\\n5LNFit is trained only on the variance vectors in the layer normalization\\nmodule of the PLMs, inspired by [86] which trains only on the batch\\nnormalization module in convolutional neural networks.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 11}, page_content='12\\nmethods that utilize the same PEFT module uniformly across\\nall layers of the transformer, S4is designed by searching\\nfor various layer groupings, trainable parameter allocations,\\ntunable groups, and PEFT module assignments. In S4, the\\nlayers of the PLMs are divided into four groups, G1,G2,\\nG3,G4, in a ‚Äúspindle‚Äù pattern. This means that more layers\\nare allocated to the middle groups ( G2andG3) while fewer\\nlayers are assigned to the top and bottom groups ( G1andG4).\\nHowever, all trainable parameters are allocated uniformly, i.e.,\\nthe number of trainable parameters in each layer remains the\\nsame across all groups. Different groups are equipped with dif-\\nferent combinations of sequential adapter, prefix-tuning, BitFit,\\nand LoRA. Extensive experimental results demonstrate that\\nbetter performance is achieved when each group is equipped\\nwith the following combinations of PEFT methods. Adenotes\\nsequential adapter, Pdenotes prefix-tuning, Bdenotes BitFit,\\nandLdenotes LoRA.\\nG1: (A, L);G2: (A, P);\\nG3: (A, P, B );G4: (P, B, L ).\\nE. Unified Fine-tuning\\nUnified fine-tuning presents a unified framework for fine-\\ntuning, which streamlines the incorporation of diverse fine-\\ntuning methods into a cohesive architecture, ensuring consis-\\ntency and efficiency across the adaptation and optimization of\\nmodels. Unlike hybrid fine-tuning methods, unified fine-tuning\\nmethods typically utilize a single PEFT method rather than a\\ncombination of various PEFT methods.\\nAdaMix [63] leverages a mixture of adaptation module\\napproaches to obtain a unified framework for fine-tuning.\\nMotivated by sparsely-activated MoE [96], AdaMix treats\\neach adaptation module as an individual expert and employs\\nstochastic routing to randomly select a down-projection ma-\\ntrix and an up-projection matrix for weight updates. Such\\nstochastic routing allows the adaption module to learn multiple\\nviews for the given task, but it also poses a challenge in\\ndeciding which adaption module to use during inference.\\nTo this end, Adamix utilizes consistency regularization and\\nadaption module merging (i.e., average weights of all down-\\nand up-projection matrices) to select the trained adaption\\nmodule and obtain the same computational cost as that of a\\nsingle module. Notably, adaption modules in Adamix could be\\nadapters like sequential adapter [9] or low-rank decomposition\\nmatrices like LoRA [11].\\nSparseAdapter [64] utilizes network pruning technique to\\nconstruct a unified framework in which various PEFT methods,\\nincluding adapters family and LoRA [9], [11], [16], can be\\nfurther pruned to improve parameter efficiency. SparseAdapter\\nsets a target sparsity, denoted as s, and assigns a score,\\ndenoted as z, to all parameters of adapters and LoRA. Pa-\\nrameters with scores below the threshold zs(corresponding\\nto the s-th lowest percentile of z) are considered redundant\\nand removed. The score zcan be computed using pruning\\nmethods, such as random pruning, magnitude pruning [97],\\nErdos-Renyi [98], SNIP [99], or GraSP [100], based on the\\nadapter weight W, with SNIP-based SparseAdapter yielding\\nthe best results. Furthermore, SparseAdapter exhibits improvedperformance compared to full fine-tuning when utilizing the\\n‚ÄúLarge-Sparse‚Äù setting, which involves larger bottleneck di-\\nmensions and higher sparsity ratios. Notably, the network\\npruning technique proposed in SparseAdapter is a plug-in\\nmethod that can be applied to any adapter variants, such as\\nLoRA [11], MAM Adapter [16], and AdapterFusion [19]. The\\noptimized parameters in SparseAdapter can be represented as\\nÀÜW=W‚äôM, in which Mis a binary mask matrix with\\nM=I{z‚â•zs}andz=score(W).\\nProPETL [65] introduces a single prototype network (e.g.,\\nadapter, prefix-tuning, and LoRA) across layers and tasks and\\nconstructs different sub-networks for each layer using various\\nbinary masks. Inspired by ALBERT [101], ProPETL leverages\\nparameter sharing within the prototype network modules in\\neach layer of the transformer, enhancing parameter efficiency\\nand reducing storage requirements. In ProPETL, binary masks\\nM‚àà {0,1}nare introduced in each layer of the transformer, in\\nwhich nis the number of parameters in a single PEFT module.\\nEach mask corresponds to a specific sub-network of the shared\\nprototype network. By doing so, though each layer shares\\nthe parameters of the same prototype network, each layer\\nhas a different sub-network to capture meaningful semantic\\nrepresentations. The final objective of the task adaptation for\\nthe PLMs can be expressed as follows:\\nmax\\nŒ∏pro,m1,m2,¬∑¬∑¬∑,mLNX\\ni=0logP(Yi|Xi;Œ∏lm, Œ∏sub), (17)\\nŒ∏sub= [Œ∏pro‚äôm1, Œ∏pro‚äôm2,¬∑¬∑¬∑, Œ∏pro‚äômL].\\nHere, Œ∏lmrepresents the frozen pretrained parameters of the\\nPLMs, mi(i= 1,2,¬∑¬∑¬∑, L) is binary mask matrix, and Œ∏sub\\ndenotes the parameters to be optimized.\\nIV. E XPERIMENTS\\nA. Experimental Settings\\n1) PLMs and Datasets: We use the encoder-only models\\nRoBERTa-base (125M) and RoBERTa-large (355M) [2] to\\nevaluate on the GLUE benchmark [100], encoder-decoder\\nmodels T5-base (220M) and T5-large (770M) [4] to evaluate\\non the WMT16 En-Ro dataset6, and decoder-only models\\nLLaMA-7B and LLaMA-13B [7] fine-tuned with the Alpaca\\ndataset [102] to evaluate on the MMLU benchmark [103].\\nAll these PLMs with different model types and model scales\\nare based on the encoder, decoder, or encoder-decoder of the\\nTransformer architecture. The datasets we use for experi-\\nments cover a wide range of tasks, from NLU to MT and\\nNLG. The GLUE benchmark covers a collection of NLU\\ntasks, including single-sentence classification, and sentence-\\npair classification tasks. WMT16 En-Ro dataset consists of\\nparallel data pairs, where each pair consists of an English\\nsentence and its corresponding translation into Romanian.\\nAlpaca [102] is an instruction dataset containing 52k samples.\\nMMLU Benchmark [103] encompasses a comprehensive range\\nof 57 disciplines spanning science, humanities, social sciences,\\nand more. The level of difficulty of the benchmark ranges from\\n6https://huggingface.co/datasets/wmt16'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 12}, page_content='13\\nTABLE III: Fine-tuning RoBERTa-base (RoB B) and RoBERTa-large (RoB L) models on the GLUE benchmark. Specifically,\\nwe report the Matthews correlation for COLA, accuracy/F1 score for MRPC and QQP, Pearson/Spearman correlation for\\nSTS-B, averaged matched accuracy for MNLI, and accuracy for other NLU tasks. Higher values indicate better performance\\nacross all metrics. We present the number of trainable parameters (# TPs) of each method, excluding Child-Tuning Ddue to\\nits randomness during network pruning. We also bold the maximum values and underline the minimum values.\\nModel PEFT Method #TPs CoLA SST2 MRPC STS-B QQP MNLI QNLI RTE Avg.\\nRoBBFT 124.6M 59.07 92.89 88.24/91.58 90.87/90.61 90.81/87.72 86.27 91.07 72.20 84.00/84.00\\nAdapterS7.41M 63.32 94.31 90.44/93.18 91.25/90.94 90.81/86.55 87.33 92.06 73.56 85.39/85.16\\nPrompt-tuning 0.61M 49.37 92.09 70.83/81.72 82.44/83.11 82.99/78.35 80.57 80.03 58.12 74.56/75.42\\nPrefix-tuning 0.96M 59.31 93.81 87.25/91.03 88.48/88.32 87.75/84.09 85.21 90.77 54.51 80.89/80.88\\n(IA)30.66M 59.58 93.92 87.00/90.52 90.30/90.32 87.99/84.10 83.95 90.88 71.12 83.09/83.05\\nBitFit 0.69M 61.32 94.72 89.22/92.41 90.34/90.27 88.12/84.11 84.64 91.09 77.98 84.68/84.57\\nChild-Tuning D - 60.33 93.58 89.22/92.20 91.14/90.93 90.98/88.04 87.40 92.20 77.62 85.31/85.29\\nLoRA 0.89M 62.09 94.04 87.50/90.68 90.66/90.83 88.83/85.21 86.54 92.02 72.92 84.33/84.29\\nAdaLoRA 1.03M 59.82 93.92 87.99/91.33 90.83/90.73 88.58/84.98 86.26 91.43 70.04 83.61/83.56\\nMAM Adapter 46.78M 61.42 94.87 89.31/92.21 90.74/90.42 88.31/83.20 86.63 90.19 72.62 84.26/83.95\\nProPELT Adapter 1.87M 66.33 93.85 87.25/90.82 91.33/91.04 89.22/85.79 86.49 92.56 75.54 85.32/85.30\\nProPELT Prefix 10.49M 61.79 94.30 88.73/91.98 90.30/90.19 88.54/85.05 86.22 91.51 63.31 83.08/83.04\\nProPELT LoRA 1.77M 60.38 94.11 87.42/90.87 90.76/90.55 88.90/85.55 86.84 92.04 67.39 83.48/83.47\\nRoBLFT 355.3M 65.78 95.54 89.22/92.28 91.74/91.76 89.30/86.68 89.42 93.61 81.23 86.98/87.04\\nAdapterS19.77M 67.03 96.37 89.94/92.54 92.58/92.42 92.19/88.50 91.00 94.31 85.25 88.58/88.43\\nPrompt-tuning 1.07M 61.13 94.61 73.04/81.29 78.51/78.99 80.74/75.16 68.15 89.13 60.29 75.70/76.09\\nPrefix-tuning 2.03M 59.01 95.76 88.24/91.37 90.92/91.07 88.88/85.45 89.30 93.32 74.01 84.93/84.91\\n(IA)31.22M 61.15 94.61 86.52/90.33 92.22/92.03 89.45/86.25 88.63 94.25 81.23 86.00/86.06\\nBitFit 1.32M 68.01 96.10 90.93/93.38 91.93/91.77 89.48/86.43 89.98 94.47 87.73 88.57/88.47\\nChild-Tuning D - 63.08 95.07 90.69/93.43 92.36/92.18 91.52/88.75 35.45 93.15 86.25 80.95/80.92\\nLoRA 1.84M 64.47 96.67 87.50/91.19 91.66/91.44 90.15/86.91 90.76 95.00 79.78 87.00/87.03\\nAdaLoRA 2.23M 65.85 94.95 89.46/92.34 92.05/91.80 89.60/86.30 90.36 94.62 77.98 86.86/86.78\\nMAM Adapter 122.2M 67.39 95.81 90.12/92.77 92.44/92.18 90.87/86.65 90.62 94.31 86.62 88.52/88.29\\nProPELT Adapter 5.40M 65.55 96.27 89.71/92.54 91.92/91.67 90.67/87.74 91.37 95.20 88.89 88.70/88.65\\nProPELT Prefix 26.85M 62.24 96.17 90.04/92.92 90.70/90.49 89.30/86.30 90.33 94.73 79.71 86.65/86.61\\nProPELT LoRA 4.19M 61.90 95.93 89.06/92.19 91.66/91.38 90.93/88.05 90.53 94.93 83.57 87.31/87.31\\nbeginner to advanced levels of expertise, testing both world\\nknowledge and problem-solving abilities.\\n2) PEFT Methods: Eleven representative PEFT methods:\\nsequential adapter (AdapterS) [9], prompt-tuning [24], prefix-\\ntuning [10], (IA)3[30], BitFit [34], Child-Tuning [39], LoRA\\n[11], AdaLoRA [45], QLoRA [49], MAM adapter [16], and\\nProPELT [65] are chosen. Since the GLUE benchmark consists\\nof a series of NLU tasks, it serves as the preferred evaluation\\ndataset used by most PLMs to validate the effectiveness of\\nPEFT methods. Ten representative PEFT methods other than\\nQLoRA are selected to fine-tune RoBERTa-base/large. For T5-\\nbase/large, we use (IA)3and LoRA for fine-tuning. As for\\nLLaMA-7B/13B, (IA)3, LoRA, and QLoRA are used for fine-\\ntuning.\\n3) Implementation Details: Since ‚Äúprompt-tuning, prefix-\\ntuning, (IA)3, LoRA, and AdaLoRA‚Äù have been integrated\\ninto the PEFT library7. Therefore, we directly utilize the PEFT\\nlibrary to invoke these PEFT methods for fine-tuning. For\\nBitFit, Child-tuing D, MAM adapter, QLoRA, and ProPELT,\\nwe experiment using their original code. Significantly, we\\nexperiment with sequential adapter using code from the MAM\\nadapter. For RoBERTa-base/large, all PEFT methods are fine-\\ntuned using a batch size of 32 and a sequence length of 128,\\nexcept for (IA)3which is fine-tuned using batch size 8. We\\nuse the batch size 64 for T5-base and 32 for T5-large. For\\nLLaMA-7B/13B, we use batch size 16 for fine-tuning. All\\nexperiments are implemented with A800 GPU.\\n7https://huggingface.co/docs/peft/indexB. Fine-tuning Performance and Parameter Efficiency\\n1) RoBERTa Base/Large on GLUE: Experimental results\\nof full fine-tuning and 11 representative PEFT methods with\\nRoBERTa-base/large on the GLUE benchmark are presented\\nin Table III, the following findings are observed:\\n‚Ä¢All PEFT methods reduce the number of trainable\\nparameters, and most PEFT methods achieve perfor-\\nmance matching or even better than full fine-tuning on\\nthe GLUE benchmark. For RoBERTa-base, the aver-\\nage performance of prompt-tuning, prefix-tuning, IA3,\\nAdaLoRA, ProPELT prefix and ProPELT LoRA on GLUE all\\nunderperforms full finetuning, while that of sequential\\nadapter, BitFit, Child-Tuning D, LoRA, MAM adapter,\\nand ProPELT Adapter outperforms full fine-tuning. For\\nRoBERTa-large, the average performance of prompt-\\ntuning, prefix-tuning, IA3, AdaLoRA, ProPELT prefix and\\nChild-Tuning Don GLUE underperforms full fine-tuning,\\nwhile that of sequential adapter, BitFit, LoRA, MAM\\nadapter, ProPELT Adapter and ProPELT LoRA outperforms\\nfull fine-tuning.\\n‚Ä¢ProPELT adapter , a unified fine-tuning method that em-\\nploys the AdapterFusion as the backbone, uses about\\n1.50% of the trainable parameters to fine-tune RoBERT-\\nbase and RoBERTa-large, but achieves optimal average\\nperformance on the GLUE benchmark, outperforming\\nRoBERT-base (FT) by about 1.30% and RoBERT-large\\n(FT) by about 1.65%.\\n‚Ä¢MAM Adapter, a hybrid fine-tuning method that com-\\nbines parallel adapters and prefix-tuning, achieves better\\nperformance than prefix-tuning, but also consumes a large'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 13}, page_content='14\\nTABLE IV: Fine-tuning T5-base and T5-large models on the\\nWMT16 En-Ro dataset and evaluating their performance using\\nBLEU score. The higher BLEU score indicates a better quality\\nof translation output.\\nModel PEFT Method # TPs BLEU\\nT5-baseFT 222.9M 27.42\\n(IA)30.07M 27.58\\nLoRA 0.88M 27.78\\nT5-largeFT 737.7M 28.13\\n(IA)30.19M 28.12\\nLoRA 2.36M 28.12\\namount of trainable parameters.\\n‚Ä¢Sequential adapter requires more trainable parameters\\nthan prompt-tuning, prefix-tuning, (IA)3, BitFit, Child-\\nTuning D, LoRA, and AdaLoRA, but achieves better\\nperformance than them on the GLUE benchmark.\\n‚Ä¢Prompt-tuning with the virtual marker length set to 20\\nachieves the smallest trainable parameter, but also the\\nworst performance, with its average performance on the\\nGLUE benchmark being about 10% lower than full fine-\\ntuning.\\n‚Ä¢Child-Tuning Dperforms well when fine-tuning RoBERT-\\nbase on the GLUE benchmark and obtains better perfor-\\nmance than full fine-tuning, but performs poorly when\\nfine-tuning RoBERT-large on the MNLI dataset, which\\nwe guess it caused by the learning rate.\\n2) T5 Base/Large on WMT16 En-Ro Dataset: As depicted\\nin Table IV, both (IA)3and LoRA significantly reduce the\\nnumber of trainable parameters compared to full fine-tuning,\\nwhile maintaining comparable performance. Specifically, (IA)3\\nemploys only 0.03% of trainable parameters and achieves a\\nBLEU score [104] 0.16 higher than full fine-tuning for T5-\\nbase and 0.01 lower for T5-large. LoRA achieves a BLEU\\nscore 0.36 higher than full fine-tuning on T5-base using only\\n0.39% of trainable parameters, and 0.01 lower than full fine-\\ntuning on T5-large using only 0.32% of trainable parameters.\\n3) LLaMA on MMLU: We first tracked the 5-shot MMLU\\ndev accuracy of LLaMA-7B-Alpaca and LLaMA-13B-Alpaca\\nwith full fine-tuning and PEFT approaches LoRA, QLoRA,\\nand (IA)3, following the work in [49]. As depicted in Fig. 4,\\nthere are significant performance fluctuations in the 5-shot\\nMMLU dev accuracy throughout model training, particularly\\nin LoRA and QLoRA. Moreover, we discovered that full\\nfine-tuning performance of LLaMA-7B-Alpaca on the MMLU\\nbenchmark is extremely sensitive to the learning rate, as shown\\nin Table V. Subsequently, we select the checkpoint with the\\nbest performance on the dev set and perform 5-shot accuracy\\nexperiments on the test set of the MMLU benchmark.\\nAs illustrated in Table VI, full fine-tuning of both LLaMA-\\n7B and LLaMA-13B produces better 5-shot MMLU test\\naccuracy compared to other PEFT methods. (IA)3, LoRA, and\\nQLoRA methods all greatly reduce the number of trainable\\nparameters with (IA)3performs best. Although (IA)3only\\nconsumes 0.02% of full fine-tuning parameters, it performs\\n2-4% lower than full fine-tuning. LoRA and QLoRA requireTABLE V: Full fine-tuning performance of LLaMA-7B-\\nAlpaca on the test set of MMLU benchmark with different\\nlearning rates.\\nLearning rate 5-shot MMLU Accuracy\\n2e-4 25.71\\n5e-5 26.65\\n1e-6 41.79\\nabout 2% of full fine-tuning parameters, achieving 5-shot\\nMMLU accuracy that is about 2% lower than full fine-tuning.\\nIn particular, QLoRA only uses half the number of trainable\\nparameters of LoRA but achieves comparable performance.\\nThis reduction of parameters in QLoRA can be attributed to\\nthe incorporation of 4-bit NormalFloat quantization.\\nC. Memory Efficiency\\nIt has been demonstrated that PEFT methods effectively re-\\nduce the number of trainable parameters. However, it remains\\nunclear whether they can also reduce GPU memory usage.\\nTo assess the impact of PEFT methods on GPU memory,\\nwe compare the GPU memory cost of full fine-tuning and\\nPEFT methods across various models and benchmarks. The\\nspecific experimental settings can be seen in the section of\\nimplementation details. As presented in Table VII, the mem-\\nory usage of full fine-tuning in RoBERTa, T5, and LLaMA\\nis positively related to total model parameters. RoBERTa,\\nspecifically RoBERTa-base, consumes less memory, requiring\\nonly 5.38GB. In contrast, LLaMA demands significantly larger\\nmemory, notably LLaMA-13B, necessitating approximately\\n290GB for full fine-tuning.\\nIn RoBERTa-base/large, prompt-tuning, prefix-tuning,\\n(IA)3, LoRA and AdaLoRA (implemented using the PEFT\\nlibrary), and BitFit significantly reduce the GPU memory\\nfootprint compared to full fine-tuning. Surprisingly, sequential\\nadapter, MAM adapter, Child-Tuning D, and ProPELT all use\\nmore memory than full fine-tuning. Both sequential adapter\\nand MAM adapter exhibit higher memory consumption,\\naround three times that of full fine-tuning, with the MAM\\nadapter consuming even more memory. For T5-base/large\\nmodels, (IA)3and LoRA all demonstrate effective memory\\nreduction during fine-tuning, with LoRA outperforming (IA)3.\\nNotably, (IA)3consumes less GPU memory than LoRA in\\nRoBERTa-base/large, which is caused by the smaller batch\\nsize during (IA)3fine-tuning ((IA)3sets the batch size to 8).\\nLikewise, (IA)3, LoRA, and QLoRA all significantly reduce\\nthe GPU footprint compared to full finetuning in LLaMA-\\n7B/13B. In addition, we discovered that the PEFT method is\\nmore effective in reducing memory usage when the number\\nof model parameters is larger. For example, in LLaMA-\\n7B-Alpaca, compared with full fine-tuning, IA3, LoRA, and\\nQLoRA reduce memory usage by 24.08%, 26.30%, and\\n66.66%, respectively; while in LLaMA-13B-Alpaca, compared\\nwith full fine-tuning, IA3, LoRA, and QLoRA reduce memory\\nusage by 33.55%, 39.46% and 76.86% of memory usage.\\nNotably, QLoRA dramatically reduces GPU memory con-\\nsumption, with QLoRA fine-tuning the LLaMA-7B requiring'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 14}, page_content='15\\n2000 4000 6000 8000 10000\\nEvaluation Steps3234363840425-shot MMLU Accuracy(%)LLaMA-7B-Alpaca (FT)\\n(a)LLaMA-7B-Alpaca-FT\\n2000 4000 6000 8000 10000\\nEvaluation Steps3234363840425-shot MMLU Accuracy(%)LLaMA-7B-Alpaca ((IA)3) (b)LLaMA-7B-Alpaca-(IA)3\\n2000 4000 6000 8000 10000\\nEvaluation Steps3234363840425-shot MMLU Accuracy(%)LLaMA-7B-Alpaca (LoRA)(c)LLaMA-7B-Alpaca-LoRA\\n2000 4000 6000 8000 10000\\nEvaluation Steps3234363840425-shot MMLU Accuracy(%)LLaMA-7B-Alpaca (QLoRA) (d)LLaMA-7B-Alpaca-QLoRA\\n2000 4000 6000 8000 10000\\nEvaluation Steps44454647484950515-shot MMLU Accuracy(%)LLaMA-13B-Alpaca (FT)\\n(e)LLaMA-13B-Alpaca-FT\\n2000 4000 6000 8000 10000\\nEvaluation Steps44454647484950515-shot MMLU Accuracy(%)LLaMA-13B-Alpaca ((IA)3) (f)LLaMA-13B-Alpaca-(IA)3\\n2000 4000 6000 8000 10000\\nEvaluation Steps44454647484950515-shot MMLU Accuracy(%)LLaMA-13B-Alpaca (LoRA)(g)LLaMA-13B-Alpaca-LoRA\\n2000 4000 6000 8000 10000\\nEvaluation Steps44454647484950515-shot MMLU Accuracy(%)LLaMA-13B-Alpaca (QLoRA) (h)LLaMA-13B-Alpaca-QLoRA\\nFig. 4: The 5-shot accuracy fluctuates on the MMLU dev set with the increase in evaluation steps when fine-tuning LLaMA-\\n7B-Alpaca and LLaMA-7B-Alpaca using the IA3, LoRA, and QLoRA methods.\\nTABLE VI: Comparison of the average 5-shot MMLU test accuracy of LLaMA-7B and LLaMA-13B models fine-tuned with\\nAlpaca. The higher the MMLU accuracy, the better. We also report total model parameters (# APs) and the ratio of trainable\\nparameters.\\nModel PEFT Method # TPs # APs % Params 5-shot MMLU Accuracy\\nLLaMA-7B-AlpacaFT 6738.4M 6738.4M 100 41.79\\n(IA)31.58M 6740.0M 0.02 37.88\\nLoRA 159.9M 6898.3M 2.32 40.67\\nQLoRA 79.9M 3660.3M 2.18 39.96\\nLLaMA-13B-AlpacaFT 13015.9M 13015.9M 100 49.60\\n(IA)32.48M 13018.3M 0.02 47.42\\nLoRA 250.3M 13266.2M 1.88 47.49\\nQLoRA 125.2M 6922.3M 1.81 47.29\\nonly 1/3 of the memory required for full fine-tuning, and fine-\\ntuning the LLaMA-13B requiring less than 1/4 of the memory\\nrequired for full fine-tuning. This advancement opens up the\\npossibility of fine-tuning LLMs for various downstream tasks\\nin computational resource-constrained scenarios.\\nV. A PPLICATIONS\\nA. Multi-task Learning\\nMulti-task learning is a method that involves training a\\nmodel on multiple related tasks and exploiting the informa-\\ntion shared and transferred between them to improve the\\nperformance of each task. PEFT methods such as adapters,\\nprompt-tuning, and LoRA utilize additional modules that can\\nbe plugged into PLMs and thus can be used for task-specific\\nfine-tuning to improve generalization of multi-task learning.\\nFor instance, studies from [19], [21], [22], [75] leverage task-\\nspecific adapters to learn information stored in multiple tasks\\nto achieve more robust transfer learning on new tasks. Several\\nworks [26], [27], [28] employ prompt-tuning for multi-task\\nlearning. They either utilize pretrained soft prompts frommultiple source tasks to initialize the soft prompt of the\\ntarget task, based on the similarity between the source and\\ntarget tasks, or employ multi-task data to learn a single\\nshared prompt and transfer it to the target task. Similar to\\nthe adapter, a composition of multiple task-specific LoRA\\nmodules is also leveraged to transfer knowledge to new tasks\\n[55], [56]. L-LoRA [57] enhances the fusion capabilities of\\nmulti-task learning by preventing negative inference between\\ntask-specific representations. Additionally, [93] utilizes arith-\\nmetic operators, such as the addition and negation operators,\\nto merge parameters of various PEFT methods trained on\\ndifferent tasks for multi-task learning.\\nB. Cross-Lingual Transfer\\nCross-lingual transfer involves transferring knowledge or\\nmodels from one language to another. Numerous works have\\nemployed PEFT methods, such as adapters, for cross-lingual\\ntransfer due to their unique modular design. Bapna and Firat\\n[105] utilize sequential adapter [9] to fine-tune and restore\\nthe performance of a multilingual neural machine translation'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 15}, page_content='16\\nTABLE VII: The peak GPU memory usage when fine-tuning RoBERT-base, RoBERTa-large, T5-base, T5-large, LLaMA-7B,\\nand LLaMA-13B model using full fine-tuning and various PEFT methods.\\nModel & Method Memory (GB) Model & Method Memory (GB)\\nRoBERTa-base (FT) 5.38 RoBERTa-large (FT) 11.96\\nRoBERTa-base (AdapterS) 15.29 RoBERTa-large (AdapterS) 37.17\\nRoBERTa-base (Prompt-tuning) 3.84 RoBERTa-large (Prompt-tuning) 7.98\\nRoBERTa-base (Prefix-tuning) 3.56 RoBERTa-large (Prefix-tuning) 7.58\\nRoBERTa-base ((IA)3) 2.62 RoBERTa-large ((IA)3) 4.83\\nRoBERTa-base (BitFit) 3.27 RoBERTa-large (BitFit) 7.50\\nRoBERTa-base (Child-Tuning D) 6.02 RoBERTa-large (Child-Tuning D) 13.67\\nRoBERTa-base (LoRA) 3.59 RoBERTa-large (LoRA) 7.50\\nRoBERTa-base (AdaLoRA) 3.57 RoBERTa-large (AdaLoRA) 7.43\\nRoBERTa-base (MAM Adapter) 15.35 RoBERTa-large (MAM Adapter) 37.82\\nRoBERTa-base (ProPELT Adapter ) 8.63 RoBERTa-large (ProPELT Adapter ) 19.82\\nRoBERTa-base (ProPELT Prefix) 9.47 RoBERTa-large (ProPELT Prefix) 22.85\\nRoBERTa-base (ProPELT LoRA) 8.25 RoBERTa-large (ProPELT LoRA) 19.52\\nT5-base (FT) 25.17 T5-large (FT) 30.17\\nT5-base ((IA)3) 21.36 T5-large ((IA)3) 25.71\\nT5-base (LoRA) 19.43 T5-large (LoRA) 23.77\\nLLaMA-7B-Alpaca (FT) 169.36 LLaMA-13B-Alpaca (FT) 287.79\\nLLaMA-7B-Alpaca ((IA)3) 128.57 LLaMA-13B-Alpaca ((IA)3) 191.24\\nLLaMA-7B-Alpaca (LoRA) 124.82 LLaMA-13B-Alpaca (LoRA) 174.24\\nLLaMA-7B-Alpaca (QLoRA) 56.46 LLaMA-13B-Alpaca (QLoRA) 66.60\\nmodel on high-resource languages. Artetxe et al. [106] employ\\nsequential adapter [9] to transfer a pretrained monolingual\\nmodel to an unseen language. MAD-X [75], [107] uses\\nlanguage-specific, task-specific, and invertible adapter to learn\\nlanguage-specific and task-specific transformations, as well\\nas address vocabulary mismatches between multilingual and\\ntarget languages in a modular manner, enabling the adaptation\\nof pretrained multilingual models to target languages. MAD-G\\n[108] generates language adapters from language representa-\\ntions based on typological features, allowing the sharing of lin-\\nguistic knowledge across languages for cross-lingual transfer.\\nLT-SFT [38] employs sparse fine-tuning to train the model on\\nthe source language and learn task-specific sparse difference\\nvectors for cross-lingual transfer. While BAD-X [109] trains a\\nbilingual language-pair adapter on both the source and target\\nlanguages for zero-shot cross-lingual transfer.\\nC. Backdoor Attacks and Defense\\nBackdoor attacks pose a significant security threat, where\\na small portion of training samples are contaminated with\\nmalicious backdoor triggers. When trained on such poisoned\\ndatasets, the model behaves normally on benign samples but\\npredicts attacker-selected labels on samples containing the\\npredefined triggers. The susceptibility of PLMs to backdoor at-\\ntacks poses a substantial risk to real-world applications [110].\\nBuilding on the vulnerability of pretrained weights, Gu et al.\\n[111] employ the PEFT methods to construct backdoor attacks,\\nin which backdoor attacks are directly injected into PEFT\\nmodules. However, Zhu et al. [112] discover that PEFT can\\nserve as a backdoor defense solution by reducing the model\\ncapacity via optimizing only a small number of parameters.\\nThe findings from [113] also confirm that PEFT can slightlyweaken the backdoor attacks and design a novel trojan attack\\nfor the PEFT paradigm.\\nVI. F URTHER DIRECTIONS\\nA. Lightweight Hybrid PEFT Methods\\nThere exist many approaches [16], [35], [58], [59], [60],\\n[61], [62] to combine multiple PEFT methods, aiming to\\nleverage the distinctive advantages of each PEFT method and\\nachieve enhanced performance. Nevertheless, the exploration\\nhas been limited to PEFT methods such as adapter, LoRA,\\nprefix-tuning, and BitFit, leaving room for further exploitation\\nby incorporating additional combinations of PEFT methods.\\nMoreover, while drawing inspiration from the NAS algorithm,\\nseveral PEFT methods [60], [61] have been investigated using\\ndiverse optimization techniques to explore optimal neural\\nnetwork architectures for configuring these PEFT methods.\\nThere remains potential for continued exploration in utilizing\\nother optimization methods to automatically search for neural\\nnetwork architectures and configure specific combinations of\\nPEFT modules at specific layers. Additionally, utilizing mul-\\ntiple PEFT methods typically results in increased parameter\\nand memory usage, although it enhances performance. Hence,\\nan intriguing research direction involves investigating how\\nto leverage multiple PEFT methods to improve performance\\nwhile minimizing the number of trainable parameters.\\nB. LoRA-derived PEFT Methods\\nRecently, a multitude of LoRA-based PEFT methods have\\nemerged, as demonstrated in Fig. 1. These methods further\\nenhance LoRA by incorporating adaptive rank adjustment, un-\\nstructured pruning techniques, weight quantization, and multi-\\ntask integration. This encourages future research to develop'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 16}, page_content='17\\nmore LoRA-derived PEFT approaches build upon LoRA.\\nParticular emphasis should be placed on pruning technol-\\nogy and weight quantification. The application of pruning\\ntechniques can be extended not only to AdaLoRA [45] for\\nrank adjustment but also to LoRAPrune [48] for pruning\\nboth pretrained and LoRA weights. Notably, pruning and\\nweight quantization techniques effectively reduce the number\\nof trainable parameters, compress model size, optimize storage\\nand computational requirements of PLMs (especially LLMs),\\nand enhance their utility and scalability across downstream\\ntasks. These techniques can be further explored in conjunction\\nwith LoRA to unlock synergistic benefits.\\nC. Developing PEFT Library\\nNumerous PEFT methods have emerged, but employing\\nthem is not a straightforward endeavor. To address this\\nchallenge, the PEFT library8and AdapterHub9have been\\ndeveloped. These libraries integrate commonly used PEFT\\nmethods such as prefix-tuning, LoRA, and AdaLoRA. With\\njust a few lines of code, users can directly invoke these\\nPEFT methods, simplifying their usage. Moreover, both the\\nPEFT and AdapterHub libraries offer a range of examples\\nillustrating how to apply these PEFT methods to various PLMs\\nand LLMs for fine-tuning downstream tasks. However, not\\nall PEFT methods are currently integrated into these two\\nlibraries. Future efforts can be directed towards expanding\\nthe integration of additional methods, further boosting the\\napplication development of PEFT methods.\\nD. Explainability of PEFT Methods\\nThough numerous PEFT methods have been proposed, there\\nis a lack of comprehensive studies exploring the reasons\\nbehind their ability to achieve comparable performance and\\nreduce trainable parameters. Work from [41] unifies PEFT\\nmethods under the concept of sparse fine-tuned models and\\nprovides a theoretical analysis demonstrating that sparsity can\\nserve as a regularization technique for the original model,\\neffectively controlling the upper bound of stability. While\\n[114] explores and analyzes the express power of LoRA for\\nfully connected neural networks and transformer networks,\\nshowing the conditions under which there exist effective low-\\nrank adapters for a given task. These studies shed light on the\\nworking mechanism and effectiveness of certain PEFT meth-\\nods, but still lack generalization. Future research endeavors\\ncould focus on advancing theoretical studies to unravel the\\nunderlying working mechanisms of PEFT methods.\\nE. Exploring PEFT Methods in Computer Vision and Multi-\\nmodal Learning\\nThough PEFT methods have been extensively studied in\\nNLP, their application in computer vision and multimodal\\nlearning also shows great potential for further exploration.\\nThe sequential adapter in NLP, initially inspired by multi-\\ndomain image classification [77], [115], has paved the way for\\n8https://github.com/huggingface/peft/tree/main\\n9https://adapterhub.ml/rapid advancements in PEFT methods for PLMs. Moreover,\\nresearchers have increasingly delved into various PEFT tech-\\nniques for computer vision [116], [117], as well as language-\\nimage and image-audio multimodal learning [118], [119],\\nbuilding upon PEFT methods in NLP [9], [11], [58]. However,\\nthere is still significant room for further exploration and\\nexploitation in these domains. In particular, PEFT methods\\nhold the potential to facilitate cross-modality transfer in multi-\\nmodal learning. By fine-tuning pretrained models using PEFT\\ntechniques, knowledge acquired from one modality can be\\neffectively transferred to another, resulting in improved per-\\nformance in multimodal tasks. Consequently, the application\\nof PEFT methods in computer vision and multimodal learning\\nholds tremendous promise as a future research direction.\\nVII. C ONCLUSIONS\\nThis paper presents a comprehensive and structured study of\\nPEFT methods for PLMs. By classifying the PEFT methods in\\nNLP, we identify the main techniques and challenges associ-\\nated with them. We employ several representative PEFT meth-\\nods to fine-tune encoder-based RoBERTa, encoder-decoder-\\nbased T5, and decoder-based LLaMA on various downstream\\ntasks. Experimental results reveal that most PEFT methods\\nsignificantly improve parameter efficiency and achieve com-\\nparable or even better performance compared to full fine-\\ntuning. Additionally, most PEFT methods lower the memory\\nfootprint, with QLoRA drastically reducing the computational\\nmemory requirement, and alleviating the memory challenge\\nwhen fine-tuning LLMs. Furthermore, we introduce common\\napplications of PEFT methods and outline future research\\ndirections. As the development of LLMs continues, there is\\na clear need to develop PEFT methods that can effectively\\nreduce computational resource demands and memory usage\\nduring fine-tuning. This survey aims to provide a bird‚Äôs-\\neye view of PEFT methods for PLMs and inspiring further\\nresearch in this area.\\nREFERENCES\\n[1] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT: Pre-\\ntraining of deep bidirectional transformers for language understanding,‚Äù\\ninProc. Conf. North Amer. Chapter Assoc. Comput. Linguistics: Hum.\\nLang. Technol. , 2019, pp. 4171‚Äì4186.\\n[2] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, and V . Stoyanov, ‚ÄúRoberta: A robustly optimized bert\\npretraining approach,‚Äù in Proc. Int. Conf. Learn. Representations , 2020.\\n[3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. ,\\n‚ÄúLanguage models are unsupervised multitask learners,‚Äù OpenAI blog ,\\nvol. 1, no. 8, p. 9, 2019.\\n[4] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY . Zhou, W. Li, and P. J. Liu, ‚ÄúExploring the limits of transfer learning\\nwith a unified text-to-text transformer,‚Äù J. Mach. Learn. Res. , vol. 21,\\nno. 1, pp. 5485‚Äì5551, 2020.\\n[5] S. Zhang, M. Diab, and L. Zettlemoyer, ‚ÄúDemocratizing access to large-\\nscale language models with opt-175b,‚Äù Meta AI , 2022.\\n[6] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ¬¥c, D. Hesslow,\\nR. Castagn ¬¥e, A. S. Luccioni, F. Yvon, M. Gall ¬¥eet al. , ‚ÄúBloom: A 176b-\\nparameter open-access multilingual language model,‚Äù arXiv preprint\\narXiv:2211.05100 , 2022.\\n[7] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al. , ‚ÄúLlama:\\nOpen and efficient foundation language models,‚Äù arXiv preprint\\narXiv:2302.13971 , 2023.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 17}, page_content='18\\n[8] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojocaru,\\nM. Alhammadi, M. Daniele, D. Heslow, J. Launay, Q. Malartic et al. ,\\n‚ÄúThe falcon series of language models: Towards open frontier models,‚Äù\\nHugging Face repository , 2023.\\n[9] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe,\\nA. Gesmundo, M. Attariyan, and S. Gelly, ‚ÄúParameter-efficient transfer\\nlearning for nlp,‚Äù in Proc. Int. Conf. Mach. Learn. PMLR, 2019, pp.\\n2790‚Äì2799.\\n[10] X. L. Li and P. Liang, ‚ÄúPrefix-tuning: Optimizing continuous prompts\\nfor generation,‚Äù in Proc. Annu. Meeting Assoc. Comput. Linguistics,\\nInt. Joint Conf. Natural Lang. Process. , 2021, pp. 4582‚Äì4597.\\n[11] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,\\nand W. Chen, ‚ÄúLoRA: Low-rank adaptation of large language models,‚Äù\\ninProc. Int. Conf. Learn. Representations , 2022.\\n[12] N. Ding, Y . Qin, G. Yang, F. Wei, Z. Yang, Y . Su, S. Hu, Y . Chen,\\nC.-M. Chan, W. Chen et al. , ‚ÄúDelta tuning: A comprehensive study\\nof parameter efficient methods for pre-trained language models,‚Äù arXiv\\npreprint arXiv:2203.06904 , 2022.\\n[13] V . Lialin, V . Deshpande, and A. Rumshisky, ‚ÄúScaling down to\\nscale up: A guide to parameter-efficient fine-tuning,‚Äù arXiv preprint\\narXiv:2303.15647 , 2023.\\n[14] Z. Lin, A. Madotto, and P. Fung, ‚ÄúExploring versatile generative\\nlanguage model via parameter-efficient transfer learning,‚Äù in Proc.\\nFindings Conf. Empir. Methods Natural Lang. Process. , 2020, pp. 441‚Äì\\n459.\\n[15] T. Lei, J. Bai, S. Brahma, J. Ainslie, K. Lee, Y . Zhou, N. Du, V . Y .\\nZhao, Y . Wu, B. Li et al. , ‚ÄúConditional adapters: Parameter-efficient\\ntransfer learning with fast inference,‚Äù arXiv preprint arXiv:2304.04947 ,\\n2023.\\n[16] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and G. Neubig, ‚ÄúTowards\\na unified view of parameter-efficient transfer learning,‚Äù in Proc. Int.\\nConf. Learn. Representations , 2022.\\n[17] A. R ¬®uckl¬¥e, G. Geigle, M. Glockner, T. Beck, J. Pfeiffer, N. Reimers,\\nand I. Gurevych, ‚ÄúAdapterDrop: On the efficiency of adapters in\\ntransformers,‚Äù in Proc. Conf. Empir. Methods Natural Lang. Process. ,\\n2021, pp. 7930‚Äì7946.\\n[18] H. Zhao, H. Tan, and H. Mei, ‚ÄúTiny-attention adapter: Contexts are\\nmore important than the number of parameters,‚Äù in Proc. Conf. Empir.\\nMethods Natural Lang. Process. , 2022, pp. 6626‚Äì6638.\\n[19] J. Pfeiffer, A. Kamath, A. R ¬®uckl¬¥e, K. Cho, and I. Gurevych, ‚ÄúAdapter-\\nFusion: Non-destructive task composition for transfer learning,‚Äù in\\nProc. Conf. Eur. Chapter Assoc. Comput. Linguistics , 2021, pp. 487‚Äì\\n503.\\n[20] S. He, R.-Z. Fan, L. Ding, L. Shen, T. Zhou, and D. Tao, ‚ÄúMera:\\nMerging pretrained adapters for few-shot learning,‚Äù arXiv preprint\\narXiv:2308.15982 , 2023.\\n[21] R. Karimi Mahabadi, S. Ruder, M. Dehghani, and J. Henderson,\\n‚ÄúParameter-efficient multi-task fine-tuning for transformers via shared\\nhypernetworks,‚Äù in Proc. Annu. Meeting Assoc. Comput. Linguistics,\\nInt. Joint Conf. Natural Lang. Process. , 2021, pp. 565‚Äì576.\\n[22] A. Chronopoulou, M. Peters, A. Fraser, and J. Dodge, ‚ÄúAdapterSoup:\\nWeight averaging to improve generalization of pretrained language\\nmodels,‚Äù in Proc. Findings Assoc. Comput. Linguistics , 2023, pp. 2054‚Äì\\n2063.\\n[23] K. Hambardzumyan, H. Khachatrian, and J. May, ‚ÄúWARP: Word-level\\nAdversarial ReProgramming,‚Äù in Proc. Annu. Meeting Assoc. Comput.\\nLinguistics, Int. Joint Conf. Natural Lang. Process. , 2021, pp. 4921‚Äì\\n4933.\\n[24] B. Lester, R. Al-Rfou, and N. Constant, ‚ÄúThe power of scale for\\nparameter-efficient prompt tuning,‚Äù in Proc. Conf. Empir. Methods\\nNatural Lang. Process. , 2021, pp. 3045‚Äì3059.\\n[25] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang, ‚ÄúGpt\\nunderstands, too,‚Äù arXiv preprint arXiv:2103.10385 , 2021.\\n[26] T. Vu, B. Lester, N. Constant, R. Al-Rfou‚Äô, and D. Cer, ‚ÄúSPoT: Better\\nfrozen model adaptation through soft prompt transfer,‚Äù in Proc. Annu.\\nMeeting Assoc. Comput. Linguistics , 2022, pp. 5039‚Äì5059.\\n[27] A. Asai, M. Salehi, M. Peters, and H. Hajishirzi, ‚ÄúATTEMPT:\\nParameter-efficient multi-task tuning via attentional mixtures of soft\\nprompts,‚Äù in Proc. Conf. Empir. Methods Natural Lang. Process. , 2022,\\npp. 6655‚Äì6672.\\n[28] Z. Wang, R. Panda, L. Karlinsky, R. Feris, H. Sun, and Y . Kim,\\n‚ÄúMultitask prompt tuning enables parameter-efficient transfer learning,‚Äù\\ninProc. Int. Conf. Learn. Representations , 2023.\\n[29] Y .-L. Sung, J. Cho, and M. Bansal, ‚ÄúLST: Ladder side-tuning for\\nparameter and memory efficient transfer learning,‚Äù in Proc. Adv. Neural\\nInf. Process. Syst. , 2022.[30] H. Liu, D. Tam, M. Mohammed, J. Mohta, T. Huang, M. Bansal,\\nand C. Raffel, ‚ÄúFew-shot parameter-efficient fine-tuning is better and\\ncheaper than in-context learning,‚Äù in Proc. Adv. Neural Inf. Process.\\nSyst., 2022.\\n[31] X. Yang, J. Y . Huang, W. Zhou, and M. Chen, ‚ÄúParameter-efficient\\ntuning with special token adaptation,‚Äù in Proc. Conf. Eur. Chapter\\nAssoc. Comput. Linguistics , 2023, pp. 865‚Äì872.\\n[32] J. Cao, C. Satya Prakash, and W. Hamza, ‚ÄúAttention fusion: a light yet\\nefficient late fusion mechanism for task adaptation in NLU,‚Äù in Proc.\\nFindings Assoc. Comput. Linguistics , 2022, pp. 857‚Äì866.\\n[33] Y . Chen, Q. Fu, G. Fan, L. Du, J.-G. Lou, S. Han, D. Zhang, Z. Li, and\\nY . Xiao, ‚ÄúHadamard adapter: An extreme parameter-efficient adapter\\ntuning method for pre-trained language models,‚Äù in Proc. 32nd ACM\\nInt. Conf. Inf. Knowl. Manage. , 2023, pp. 276‚Äì285.\\n[34] E. Ben Zaken, Y . Goldberg, and S. Ravfogel, ‚ÄúBitFit: Simple\\nparameter-efficient fine-tuning for transformer-based masked language-\\nmodels,‚Äù in Proc. Annu. Meeting Assoc. Comput. Linguistics , 2022, pp.\\n1‚Äì9.\\n[35] N. Lawton, A. Kumar, G. Thattai, A. Galstyan, and G. Ver Steeg,\\n‚ÄúNeural architecture search for parameter-efficient fine-tuning of large\\npre-trained language models,‚Äù in Proc. Findings Assoc. Comput. Lin-\\nguistics , 2023, pp. 8506‚Äì8515.\\n[36] M. Zhao, T. Lin, F. Mi, M. Jaggi, and H. Sch ¬®utze, ‚ÄúMasking as an\\nefficient alternative to finetuning for pretrained language models,‚Äù in\\nProc. Conf. Empir. Methods Natural Lang. Process. , 2020, pp. 2226‚Äì\\n2241.\\n[37] Y .-L. Sung, V . Nair, and C. Raffel, ‚ÄúTraining neural networks with\\nfixed sparse masks,‚Äù in Proc. Adv. Neural Inf. Process. Syst. , 2021.\\n[38] A. Ansell, E. Ponti, A. Korhonen, and I. Vuli ¬¥c, ‚ÄúComposable sparse\\nfine-tuning for cross-lingual transfer,‚Äù in Proc. Annu. Meeting Assoc.\\nComput. Linguistics , 2022, pp. 1778‚Äì1796.\\n[39] R. Xu, F. Luo, Z. Zhang, C. Tan, B. Chang, S. Huang, and F. Huang,\\n‚ÄúRaise a child in large language model: Towards effective and gen-\\neralizable fine-tuning,‚Äù in Proc. Conf. Empir. Methods Natural Lang.\\nProcess. , 2021, pp. 9514‚Äì9528.\\n[40] D. Guo, A. Rush, and Y . Kim, ‚ÄúParameter-efficient transfer learning\\nwith diff pruning,‚Äù in Proc. Annu. Meeting Assoc. Comput. Linguistics,\\nInt. Joint Conf. Natural Lang. Process. , 2021, pp. 4884‚Äì4896.\\n[41] Z. Fu, H. Yang, A. M.-C. So, W. Lam, L. Bing, and N. Collier, ‚ÄúOn the\\neffectiveness of parameter-efficient fine-tuning,‚Äù in Proc. AAAI Conf.\\nArtif. Intell. , vol. 37, no. 11, 2023, pp. 12 799‚Äì12 807.\\n[42] A. Aghajanyan, S. Gupta, and L. Zettlemoyer, ‚ÄúIntrinsic dimensionality\\nexplains the effectiveness of language model fine-tuning,‚Äù in Proc.\\nAnnu. Meeting Assoc. Comput. Linguistics, Int. Joint Conf. Natural\\nLang. Process. , 2021, pp. 7319‚Äì7328.\\n[43] A. Edalati, M. Tahaei, I. Kobyzev, V . P. Nia, J. J. Clark, and M. Reza-\\ngholizadeh, ‚ÄúKrona: Parameter efficient tuning with kronecker adapter,‚Äù\\narXiv preprint arXiv:2212.10650 , 2022.\\n[44] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and A. Ghodsi, ‚ÄúDy-\\nLoRA: Parameter-efficient tuning of pre-trained models using dynamic\\nsearch-free low-rank adaptation,‚Äù in Proc. Conf. Eur. Chapter Assoc.\\nComput. Linguistics , 2023, pp. 3274‚Äì3287.\\n[45] Q. Zhang, M. Chen, A. Bukharin, P. He, Y . Cheng, W. Chen, and\\nT. Zhao, ‚ÄúAdaptive budget allocation for parameter-efficient fine-\\ntuning,‚Äù in Proc. Int. Conf. Learn. Representations , 2023.\\n[46] F. Zhang, L. Li, J. Chen, Z. Jiang, B. Wang, and Y . Qian, ‚ÄúIncrelora:\\nIncremental parameter allocation method for parameter-efficient fine-\\ntuning,‚Äù arXiv preprint arXiv:2308.12043 , 2023.\\n[47] B. Zi, X. Qi, L. Wang, J. Wang, K.-F. Wong, and L. Zhang, ‚ÄúDelta-lora:\\nFine-tuning high-rank parameters with the delta of low-rank matrices,‚Äù\\narXiv preprint arXiv:2309.02411 , 2023.\\n[48] M. Zhang, C. Shen, Z. Yang, L. Ou, X. Yu, B. Zhuang et al. , ‚ÄúPrun-\\ning meets low-rank parameter-efficient fine-tuning,‚Äù arXiv preprint\\narXiv:2305.18403 , 2023.\\n[49] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, ‚ÄúQlora: Ef-\\nficient finetuning of quantized llms,‚Äù arXiv preprint arXiv:2305.14314 ,\\n2023.\\n[50] Y . Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen, X. Zhang,\\nand Q. Tian, ‚ÄúQa-lora: Quantization-aware low-rank adaptation of large\\nlanguage models,‚Äù arXiv preprint arXiv:2309.14717 , 2023.\\n[51] Y . Li, Y . Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and\\nT. Zhao, ‚ÄúLoftq: Lora-fine-tuning-aware quantization for large language\\nmodels,‚Äù arXiv preprint arXiv:2310.08659 , 2023.\\n[52] Y . Chen, D. Hazarika, M. Namazifar, Y . Liu, D. Jin, and D. Hakkani-\\nTur, ‚ÄúEmpowering parameter-efficient transfer learning by recognizing\\nthe kernel structure in self-attention,‚Äù in Proc. Findings Assoc. Comput.\\nLinguistics , 2022, pp. 1375‚Äì1388.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 18}, page_content='19\\n[53] A. X. Yang, M. Robeyns, X. Wang, and L. Aitchison, ‚ÄúBayesian\\nlow-rank adaptation for large language models,‚Äù arXiv preprint\\narXiv:2308.13111 , 2023.\\n[54] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li, ‚ÄúLora-fa: Memory-\\nefficient low-rank adaptation for large language models fine-tuning,‚Äù\\narXiv preprint arXiv:2308.03303 , 2023.\\n[55] C. Huang, Q. Liu, B. Y . Lin, T. Pang, C. Du, and M. Lin, ‚ÄúLorahub:\\nEfficient cross-task generalization via dynamic lora composition,‚Äù arXiv\\npreprint arXiv:2307.13269 , 2023.\\n[56] Q. Liu, X. Wu, X. Zhao, Y . Zhu, D. Xu, F. Tian, and Y . Zheng,\\n‚ÄúMoelora: An moe-based parameter efficient fine-tuning method for\\nmulti-task medical applications,‚Äù arXiv preprint arXiv:2310.18339 ,\\n2023.\\n[57] A. Tang, L. Shen, Y . Luo, Y . Zhan, H. Hu, B. Du, Y . Chen, and D. Tao,\\n‚ÄúParameter efficient multi-task model fusion with partial linearization,‚Äù\\narXiv preprint arXiv:2310.04742 , 2023.\\n[58] R. Karimi Mahabadi, J. Henderson, and S. Ruder, ‚ÄúCompacter: Effi-\\ncient low-rank hypercomplex adapter layers,‚Äù Proc. Adv. Neural Inf.\\nProcess. Syst. , vol. 34, pp. 1022‚Äì1035, 2021.\\n[59] Y . Mao, L. Mathias, R. Hou, A. Almahairi, H. Ma, J. Han, S. Yih, and\\nM. Khabsa, ‚ÄúUniPELT: A unified framework for parameter-efficient\\nlanguage model tuning,‚Äù in Proc. Annu. Meeting Assoc. Comput.\\nLinguistics , 2022, pp. 6253‚Äì6264.\\n[60] H. Zhou, X. Wan, I. Vuli ¬¥c, and A. Korhonen, ‚ÄúAutopeft: Automatic\\nconfiguration search for parameter-efficient fine-tuning,‚Äù arXiv preprint\\narXiv:2301.12132 , 2023.\\n[61] S. Hu, Z. Zhang, N. Ding, Y . Wang, Y . Wang, Z. Liu, and M. Sun,\\n‚ÄúSparse structure search for delta tuning,‚Äù Proc. Adv. Neural Inf.\\nProcess. Syst. , vol. 35, pp. 9853‚Äì9865, 2022.\\n[62] J. Chen, A. Zhang, X. Shi, M. Li, A. Smola, and D. Yang, ‚ÄúParameter-\\nefficient fine-tuning design spaces,‚Äù in Proc. Int. Conf. Learn. Repre-\\nsentations , 2023.\\n[63] Y . Wang, S. Agarwal, S. Mukherjee, X. Liu, J. Gao, A. H. Awadallah,\\nand J. Gao, ‚ÄúAdaMix: Mixture-of-adaptations for parameter-efficient\\nmodel tuning,‚Äù in Proc. Conf. Empir. Methods Natural Lang. Process. ,\\n2022, pp. 5744‚Äì5760.\\n[64] S. He, L. Ding, D. Dong, J. Zhang, and D. Tao, ‚ÄúSparseAdapter: An\\neasy approach for improving the parameter-efficiency of adapters,‚Äù in\\nProc. Findings Conf. Empir. Methods Natural Lang. Process. , 2022,\\npp. 2184‚Äì2190.\\n[65] G. Zeng, P. Zhang, and W. Lu, ‚ÄúOne network, many masks: Towards\\nmore parameter-efficient transfer learning,‚Äù in Proc. Annu. Meeting\\nAssoc. Comput. Linguistics , 2023, pp. 7564‚Äì7580.\\n[66] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù Proc.\\nAdv. Neural Inf. Process. Syst. , vol. 30, 2017.\\n[67] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\\nrecognition,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2016,\\npp. 770‚Äì778.\\n[68] J. L. Ba, J. R. Kiros, and G. E. Hinton, ‚ÄúLayer normalization,‚Äù arXiv\\npreprint arXiv:1607.06450 , 2016.\\n[69] L. Xu and W. Wang, ‚ÄúImproving aspect-based sentiment analysis with\\ncontrastive learning,‚Äù Natural Language Processing Journal , vol. 3, p.\\n100009, 2023.\\n[70] Y . Xie, W. Yang, L. Tan, K. Xiong, N. J. Yuan, B. Huai, M. Li, and\\nJ. Lin, ‚ÄúDistant supervision for multi-stage fine-tuning in retrieval-\\nbased question answering,‚Äù in Proceedings of The Web Conference ,\\n2020, pp. 2934‚Äì2940.\\n[71] R. Dabre, A. Fujita, and C. Chu, ‚ÄúExploiting multilingualism through\\nmultistage fine-tuning for low-resource neural machine translation,‚Äù in\\nProc. Conf. Empir. Methods Natural Lang. Process., Int. Joint Conf.\\nNatural Lang. Process. , 2019, pp. 1410‚Äì1416.\\n[72] M. T. Hosseini, A. Ghaffari, M. S. Tahaei, M. Rezagholizadeh,\\nM. Asgharian, and V . P. Nia, ‚ÄúTowards fine-tuning pre-trained language\\nmodels with integer forward and backward propagation,‚Äù in Proc.\\nFindings Assoc. Comput. Linguistics , 2023, pp. 1867‚Äì1876.\\n[73] S.-i. Amari, ‚ÄúBackpropagation and stochastic gradient descent method,‚Äù\\nNeurocomputing , vol. 5, no. 4-5, pp. 185‚Äì196, 1993.\\n[74] L. Xu, H. Xie, Z. Li, F. L. Wang, W. Wang, and Q. Li, ‚ÄúContrastive\\nlearning models for sentence representations,‚Äù ACM Trans. Intel. Syst.\\nTec., vol. 14, no. 4, pp. 1‚Äì34, 2023.\\n[75] J. Pfeiffer, I. Vuli ¬¥c, I. Gurevych, and S. Ruder, ‚ÄúMAD-X: An Adapter-\\nBased Framework for Multi-Task Cross-Lingual Transfer,‚Äù in Proc.\\nConf. Empir. Methods Natural Lang. Process. , 2020, pp. 7654‚Äì7673.\\n[76] Y . Zhu, J. Feng, C. Zhao, M. Wang, and L. Li, ‚ÄúCounter-\\ninterference adapter for multilingual machine translation,‚Äù arXiv\\npreprint arXiv:2104.08154 , 2021.[77] S.-A. Rebuffi, H. Bilen, and A. Vedaldi, ‚ÄúLearning multiple visual\\ndomains with residual adapters,‚Äù Proc. Adv. Neural Inf. Process. Syst. ,\\nvol. 30, 2017.\\n[78] J. Solomon, F. De Goes, G. Peyr ¬¥e, M. Cuturi, A. Butscher, A. Nguyen,\\nT. Du, and L. Guibas, ‚ÄúConvolutional wasserstein distances: Efficient\\noptimal transportation on geometric domains,‚Äù ACM Trans. Graph. ,\\nvol. 34, no. 4, pp. 1‚Äì11, 2015.\\n[79] S. P. Singh and M. Jaggi, ‚ÄúModel fusion via optimal transport,‚Äù Proc.\\nAdv. Neural Inf. Process. Syst. , vol. 33, pp. 22 045‚Äì22 055, 2020.\\n[80] D. Ha, A. M. Dai, and Q. V . Le, ‚ÄúHypernetworks,‚Äù in Proc. Int. Conf.\\nLearn. Representations , 2017.\\n[81] R. Aharoni and Y . Goldberg, ‚ÄúUnsupervised domain clusters in pre-\\ntrained language models,‚Äù in Proc. Annu. Meeting Assoc. Comput.\\nLinguistics , 2020, pp. 7747‚Äì7763.\\n[82] H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf, ‚ÄúPruning\\nfilters for efficient convnets,‚Äù in Proc. Int. Conf. Learn. Representations ,\\n2017.\\n[83] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning, ‚ÄúWhat does\\nBERT look at? an analysis of BERT‚Äôs attention,‚Äù in Proc. of 2019 ACL\\nWorkshop BlackboxNLP , 2019, pp. 276‚Äì286.\\n[84] O. Kovaleva, A. Romanov, A. Rogers, and A. Rumshisky, ‚ÄúRevealing\\nthe dark secrets of BERT,‚Äù in Proc. Conf. Empir. Methods Natural\\nLang. Process., Int. Joint Conf. Natural Lang. Process. , 2019, pp.\\n4365‚Äì4374.\\n[85] T. Elsken, J. H. Metzen, and F. Hutter, ‚ÄúNeural architecture search: A\\nsurvey,‚Äù J. Mach. Learn. Res. , vol. 20, no. 1, pp. 1997‚Äì2017, 2019.\\n[86] J. Frankle and M. Carbin, ‚ÄúThe lottery ticket hypothesis: Finding\\nsparse, trainable neural networks,‚Äù in Proc. Int. Conf. Learn. Repre-\\nsentations , 2019.\\n[87] Q. Le, T. Sarl ¬¥os, and A. Smola, ‚ÄúFastfood-computing hilbert space\\nexpansions in loglinear time,‚Äù in Proc. Int. Conf. Mach. Learn. PMLR,\\n2013, pp. 244‚Äì252.\\n[88] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz, ‚ÄúImportance\\nestimation for neural network pruning,‚Äù in Proc. IEEE Conf. Comput.\\nVis. Pattern Recognit. , 2019, pp. 11 264‚Äì11 272.\\n[89] V . Sanh, T. Wolf, and A. Rush, ‚ÄúMovement pruning: Adaptive sparsity\\nby fine-tuning,‚Äù Proc. Adv. Neural Inf. Process. Syst. , vol. 33, pp.\\n20 378‚Äì20 389, 2020.\\n[90] D. J. MacKay, ‚ÄúA practical bayesian framework for backpropagation\\nnetworks,‚Äù Neural Comput. , vol. 4, no. 3, pp. 448‚Äì472, 1992.\\n[91] J. Liu, A. Moreau, M. Preuss, J. Rapin, B. Roziere, F. Teytaud, and\\nO. Teytaud, ‚ÄúVersatile black-box optimization,‚Äù in Proc. of the 2020\\nGenet. and Evolut. Comput. Conf. , 2020, pp. 620‚Äì628.\\n[92] G. Ilharco, M. T. Ribeiro, M. Wortsman, L. Schmidt, H. Hajishirzi, and\\nA. Farhadi, ‚ÄúEditing models with task arithmetic,‚Äù in Proc. Int. Conf.\\nLearn. Representations , 2023.\\n[93] J. Zhang, S. Chen, J. Liu, and J. He, ‚ÄúComposing parameter-efficient\\nmodules with arithmetic operations,‚Äù arXiv preprint arXiv:2306.14870 ,\\n2023.\\n[94] P. Yadav, D. Tam, L. Choshen, C. Raffel, and M. Bansal, ‚ÄúResolving\\ninterference when merging models,‚Äù arXiv preprint arXiv:2306.01708 ,\\n2023.\\n[95] A. Zhang, Y . Tay, S. Zhang, A. Chan, A. T. Luu, S. Hui, and J. Fu,\\n‚ÄúBeyond fully-connected layers with quaternions: Parameterization of\\nhypercomplex multiplications with $1/n$ parameters,‚Äù in Proc. Int.\\nConf. Learn. Representations , 2021.\\n[96] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\\nand J. Dean, ‚ÄúOutrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer,‚Äù arXiv preprint arXiv:1701.06538 , 2017.\\n[97] J. Frankle, G. K. Dziugaite, D. Roy, and M. Carbin, ‚ÄúPruning neural\\nnetworks at initialization: Why are we missing the mark?‚Äù in Proc. Int.\\nConf. Learn. Representations , 2021.\\n[98] D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu,\\nand A. Liotta, ‚ÄúScalable training of artificial neural networks with\\nadaptive sparse connectivity inspired by network science,‚Äù Nature\\ncommunications , vol. 9, no. 1, p. 2383, 2018.\\n[99] N. Lee, T. Ajanthan, and P. Torr, ‚ÄúSNIP: Single-shot network pruning\\nbased on connection sensitivity,‚Äù in Proc. Int. Conf. Learn. Represen-\\ntations , 2019.\\n[100] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman,\\n‚ÄúGLUE: A multi-task benchmark and analysis platform for natural\\nlanguage understanding,‚Äù in Proc. of 2018 EMNLP Workshop Black-\\nboxNLP , 2018, pp. 353‚Äì355.\\n[101] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\\n‚ÄúAlbert: A lite bert for self-supervised learning of language represen-\\ntations,‚Äù in Proc. Int. Conf. Learn. Representations , 2020.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PERT for NLP.pdf', 'page': 19}, page_content='20\\n[102] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang,\\nand T. B. Hashimoto, ‚ÄúStanford alpaca: An instruction-following llama\\nmodel,‚Äù 2023.\\n[103] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, ‚ÄúMeasuring massive multitask language understanding,‚Äù\\ninProc. Int. Conf. Learn. Representations , 2021.\\n[104] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ‚ÄúBleu: a method for\\nautomatic evaluation of machine translation,‚Äù in Proc. Annu. Meeting\\nAssoc. Comput. Linguistics , 2002, pp. 311‚Äì318.\\n[105] A. Bapna and O. Firat, ‚ÄúSimple, scalable adaptation for neural machine\\ntranslation,‚Äù in Proc. Conf. Empir. Methods Natural Lang. Process., Int.\\nJoint Conf. Natural Lang. Process. , 2019, pp. 1538‚Äì1548.\\n[106] M. Artetxe, S. Ruder, and D. Yogatama, ‚ÄúOn the cross-lingual transfer-\\nability of monolingual representations,‚Äù in Proc. Annu. Meeting Assoc.\\nComput. Linguistics , 2020, pp. 4623‚Äì4637.\\n[107] J. Pfeiffer, I. Vuli ¬¥c, I. Gurevych, and S. Ruder, ‚ÄúUNKs everywhere:\\nAdapting multilingual language models to new scripts,‚Äù in Proc. Conf.\\nEmpir. Methods Natural Lang. Process. , 2021, pp. 10 186‚Äì10 203.\\n[108] A. Ansell, E. M. Ponti, J. Pfeiffer, S. Ruder, G. Glava Àás, I. Vuli ¬¥c, and\\nA. Korhonen, ‚ÄúMAD-G: Multilingual adapter generation for efficient\\ncross-lingual transfer,‚Äù in Proc. Findings Conf. Empir. Methods Natural\\nLang. Process. , 2021, pp. 4762‚Äì4781.\\n[109] M. Parovi ¬¥c, G. Glava Àás, I. Vuli ¬¥c, and A. Korhonen, ‚ÄúBAD-X: Bilingual\\nadapters improve zero-shot cross-lingual transfer,‚Äù in Proc. Conf. North\\nAmer. Chapter Assoc. Comput. Linguistics: Hum. Lang. Technol. , 2022,\\npp. 1791‚Äì1799.\\n[110] M. T ¬®anzer, S. Ruder, and M. Rei, ‚ÄúMemorisation versus generalisa-\\ntion in pre-trained language models,‚Äù in Proc. Annu. Meeting Assoc.\\nComput. Linguistics , 2022, pp. 7564‚Äì7578.\\n[111] N. Gu, P. Fu, X. Liu, Z. Liu, Z. Lin, and W. Wang, ‚ÄúA gradient control\\nmethod for backdoor attacks on parameter-efficient tuning,‚Äù in Proc.\\nAnnu. Meeting Assoc. Comput. Linguistics , 2023, pp. 3508‚Äì3520.\\n[112] B. Zhu, Y . Qin, G. Cui, Y . Chen, W. Zhao, C. Fu, Y . Deng, Z. Liu,\\nJ. Wang, W. Wu, M. Sun, and M. Gu, ‚ÄúModerate-fitting as a natural\\nbackdoor defender for pre-trained language models,‚Äù in Proc. Adv.\\nNeural Inf. Process. Syst. , 2022.\\n[113] L. Hong and T. Wang, ‚ÄúFewer is more: Trojan attacks on parameter-\\nefficient fine-tuning,‚Äù arXiv preprint arXiv:2310.00648 , 2023.\\n[114] Y . Zeng and K. Lee, ‚ÄúThe expressive power of low-rank adaptation,‚Äù\\narXiv preprint arXiv:2310.17513 , 2023.\\n[115] S.-A. Rebuffi, H. Bilen, and A. Vedaldi, ‚ÄúEfficient parametrization of\\nmulti-domain deep neural networks,‚Äù in Proc. IEEE Conf. Comput. Vis.\\nPattern Recognit. , 2018, pp. 8119‚Äì8127.\\n[116] X. He, C. Li, P. Zhang, J. Yang, and X. E. Wang, ‚ÄúParameter-efficient\\nmodel adaptation for vision transformers,‚Äù in Proc. AAAI Conf. Artif.\\nIntell. , vol. 37, no. 1, 2023, pp. 817‚Äì825.\\n[117] Z. Xu, Z. Chen, Y . Zhang, Y . Song, X. Wan, and G. Li, ‚ÄúBridging vision\\nand language encoders: Parameter-efficient tuning for referring image\\nsegmentation,‚Äù in IEEE Int. Conf. Comput. Vis. , 2023, pp. 17 503‚Äì\\n17 512.\\n[118] Y .-L. Sung, J. Cho, and M. Bansal, ‚ÄúVl-adapter: Parameter-efficient\\ntransfer learning for vision-and-language tasks,‚Äù in Proc. IEEE Conf.\\nComput. Vis. Pattern Recognit. , 2022, pp. 5227‚Äì5237.\\n[119] J. Pan, Z. Lin, X. Zhu, J. Shao, and H. Li, ‚ÄúSt-adapter: Parameter-\\nefficient image-to-video transfer learning,‚Äù Proc. Adv. Neural Inf.\\nProcess. Syst. , vol. 35, pp. 26 462‚Äì26 477, 2022.\\nLingling Xu (Student Member, IEEE) is cur-\\nrently pursuing her Ph.D. degree at Hong Kong\\nMetropolitan University. She received a Master de-\\ngree in Mathematics from Shandong University. Her\\nresearch interests include parameter-efficient fine-\\ntuning, contrastive learning, representation learning,\\nand aspect-based sentiment analysis.\\nHaoran Xie (Senior Member, IEEE) received a\\nPh.D. degree in Computer Science from City Uni-\\nversity of Hong Kong and an Ed.D degree in Digital\\nLearning from the University of Bristol. He is cur-\\nrently the Department Head and Associate Professor\\nat the Department of Computing and Decision Sci-\\nences, Lingnan University, Hong Kong. His research\\ninterests include artificial intelligence, big data, and\\neducational technology. He has published 393 re-\\nsearch publications, including 224 journal articles\\nsuch as IEEE TPAMI, IEEE TKDE, IEEE TAFFC,\\nand IEEE TCVST. He is the Editor-in-Chief of Natural Language Processing\\nJournal, Computers & Education: Artificial Intelligence and Computers &\\nEducation: X Reality. He has been selected listed as the World‚Äôs Top 2%\\nScientists by Stanford University.\\nSi-Zhao Joe Qin (Fellow, IEEE) received the B.S.\\nand M.S. degrees in automatic control from Ts-\\ninghua University, Beijing, China, in 1984 and 1987,\\nrespectively, and the Ph.D. degree in chemical en-\\ngineering from the University of Maryland, College\\nPark, MD, USA, in 1992. He is currently the Wai\\nKee Kau Chair Professor and President of Lingnan\\nUniversity, Hong Kong. His research interests in-\\nclude data science and analytics, machine learning,\\nprocess monitoring, model predictive control, system\\nidentification, smart manufacturing, smart cities, and\\npredictive maintenance. Prof. Qin is a Fellow of the U.S. National Academy\\nof Inventors, IFAC, and AIChE. He was the recipient of the 2022 CAST\\nComputing Award by AIChE, 2022 IEEE CSS Transition to Practice Award,\\nU.S. NSF CAREER Award, and NSF-China Outstanding Young Investigator\\nAward. His h-indices for Web of Science, SCOPUS, and Google Scholar are\\n66, 73, and 84, respectively.\\nXiaohui Tao (Senior Member, IEEE) is currently\\na Full Professor with the University of Southern\\nQueensland, Toowoomba, QLD, Australia. His re-\\nsearch interests include artificial intelligence, data\\nanalytics, machine learning, knowledge engineering,\\ninformation retrieval, and health informatics. His\\nresearch outcomes have been published across more\\nthan 150 papers including many top-tier journals and\\nconferences. He is a Senior Member of ACM and\\nthe Vice Chair of IEEE Technical Committee of\\nIntelligent Informatics. He was the recipient of an\\nARC DP in 2022. He is the Editor-in-Chief of Natural Language Processing\\nJournal.\\nFu Lee Wang (Senior Member, IEEE) received\\nthe B.Eng. degree in computer engineering and the\\nM.Phil. degree in computer science and information\\nsystems from The University of Hong Kong, Hong\\nKong, and the Ph.D. degree in systems engineering\\nand engineering management from The Chinese\\nUniversity of Hong Kong, Hong Kong. Prof. Wang\\nis the Dean of the School of Science and Tech-\\nnology, Hong Kong Metropolitan University, Hong\\nKong. He has over 300 publications in international\\njournals and conferences and led more than 20\\ncompetitive grants with a total greater than HK$20 million. His current\\nresearch interests include educational technology, information retrieval, com-\\nputer graphics, and bioinformatics. Prof. Wang is a fellow of BCS, HKIE and\\nIET and a Senior Member of ACM. He was the Chair of the IEEE Hong\\nKong Section Computer Chapter and ACM Hong Kong Chapter.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 0}, page_content='Large Language Models: A Survey\\nShervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu\\nRichard Socher, Xavier Amatriain, Jianfeng Gao\\nAbstract ‚ÄîLarge Language Models (LLMs) have drawn a\\nlot of attention due to their strong performance on a wide\\nrange of natural language tasks, since the release of ChatGPT\\nin November 2022. LLMs‚Äô ability of general-purpose language\\nunderstanding and generation is acquired by training billions of\\nmodel‚Äôs parameters on massive amounts of text data, as predicted\\nby scaling laws [1], [2]. The research area of LLMs, while very\\nrecent, is evolving rapidly in many different ways. In this paper,\\nwe review some of the most prominent LLMs, including three\\npopular LLM families (GPT, LLaMA, PaLM), and discuss their\\ncharacteristics, contributions and limitations. We also give an\\noverview of techniques developed to build, and augment LLMs.\\nWe then survey popular datasets prepared for LLM training,\\nfine-tuning, and evaluation, review widely used LLM evaluation\\nmetrics, and compare the performance of several popular LLMs\\non a set of representative benchmarks. Finally, we conclude\\nthe paper by discussing open challenges and future research\\ndirections.\\nI. I NTRODUCTION\\nLanguage modeling is a long-standing research topic, dat-\\ning back to the 1950s with Shannon‚Äôs application of informa-\\ntion theory to human language, where he measured how well\\nsimple n-gram language models predict or compress natural\\nlanguage text [3]. Since then, statistical language modeling\\nbecame fundamental to many natural language understanding\\nand generation tasks, ranging from speech recognition, ma-\\nchine translation, to information retrieval [4], [5], [6].\\nThe recent advances on transformer-based large language\\nmodels (LLMs), pretrained on Web-scale text corpora, signif-\\nicantly extended the capabilities of language models (LLMs).\\nFor example, OpenAI‚Äôs ChatGPT and GPT-4 can be used not\\nonly for natural language processing, but also as general task\\nsolvers to power Microsoft‚Äôs Co-Pilot systems, for instance,\\ncan follow human instructions of complex new tasks per-\\nforming multi-step reasoning when needed. LLMs are thus\\nbecoming the basic building block for the development of\\ngeneral-purpose AI agents or artificial general intelligence\\n(AGI).\\nAs the field of LLMs is moving fast, with new findings,\\nmodels and techniques being published in a matter of months\\nor weeks [7], [8], [9], [10], [11], AI researchers and practi-\\ntioners often find it challenging to figure out the best recipes\\nto build LLM-powered AI systems for their tasks. This paper\\ngives a timely survey of the recent advances on LLMs. We\\nhope this survey will prove a valuable and accessible resource\\nfor students, researchers and developers.\\nLLMs are large-scale, pre-trained, statistical language mod-\\nels based on neural networks. The recent success of LLMs is\\nan accumulation of decades of research and development of\\nlanguage models, which can be categorized into four wavesthat have different starting points and velocity: statistical lan-\\nguage models, neural language models, pre-trained language\\nmodels and LLMs.\\nStatistical language models (SLMs) view text as a sequence\\nof words, and estimate the probability of text as the product\\nof their word probabilities. The dominating form of SLMs\\nare Markov chain models known as the n-gram models,\\nwhich compute the probability of a word conditioned on its\\nimmediate proceeding n‚àí1words. Since word probabilities\\nare estimated using word and n-gram counts collected from\\ntext corpora, the model needs to deal with data sparsity (i.e.,\\nassigning zero probabilities to unseen words or n-grams) by\\nusing smoothing , where some probability mass of the model\\nis reserved for unseen n-grams [12]. N-gram models are\\nwidely used in many NLP systems. However, these models\\nare incomplete in that they cannot fully capture the diversity\\nand variability of natural language due to data sparsity.\\nEarly neural language models (NLMs) [13], [14], [15], [16]\\ndeal with data sparsity by mapping words to low-dimensional\\ncontinuous vectors (embedding vectors) and predict the next\\nword based on the aggregation of the embedding vectors of\\nits proceeding words using neural networks. The embedding\\nvectors learned by NLMs define a hidden space where the\\nsemantic similarity between vectors can be readily computed\\nas their distance. This opens the door to computing semantic\\nsimilarity of any two inputs regardless their forms (e.g., queries\\nvs. documents in Web search [17], [18], sentences in different\\nlanguages in machine translation [19], [20]) or modalities (e.g.,\\nimage and text in image captioning [21], [22]). Early NLMs are\\ntask-specific models, in that they are trained on task-specific\\ndata and their learned hidden space is task-specific.\\nPre-trained language models (PLMs), unlike early NLMs,\\nare task-agnostic. This generality also extends to the learned\\nhidden embedding space. The training and inference of PLMs\\nfollows the pre-training and fine-tuning paradigm, where lan-\\nguage models with recurrent neural networks [23] or trans-\\nformers [24], [25], [26] are pre-trained on Web-scale unlabeled\\ntext corpora for general tasks such as word prediction, and then\\nfinetuned to specific tasks using small amounts of (labeled)\\ntask-specific data. Recent surveys on PLMs include [8], [27],\\n[28].\\nLarge language models (LLMs) mainly refer to\\ntransformer-based neural language models1that contain\\ntens to hundreds of billions of parameters, which are pre-\\ntrained on massive text data, such as PaLM [31], LLaMA\\n[32], and GPT-4 [33], as summarized in Table III. Compared\\n1Recently, several very promising non-transformer LLMs have been pro-\\nposed, such as the LLMs based on structured state space models [29], [30].\\nSee Section VII for more details.arXiv:2402.06196v2  [cs.CL]  20 Feb 2024'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 1}, page_content='Emerging\\n Basic\\n Augmented\\nLLM Capabilities\\nReasoning\\nCoding\\nComprehension\\nMultilingual\\nTool\\nutilization\\nWorld\\nknowledge\\nInstruction\\nfollowing\\nIn-context\\nlearning\\nInteracting\\nwith users\\nSelf-improvement\\nMulti choice QA\\n Wikipedia QA\\nXNLI\\nCrosslingual QA\\nCrosslingual T asks\\nTranslation\\nReading Comprehension\\nMulti choice QA\\nBoolean QA\\nSimplification\\nSummarization\\nFunction Calling\\nAPI calling\\nLogical\\nSymbolic\\nCommon Sense\\nArithmetic\\nTurn based\\nCompletion\\nTask definition\\nFew-shot\\nSymbolic\\nreference\\nPos/Neg example\\nStep by step\\nsolving\\nTool planning\\nTask\\ndecomposition\\nVirtual acting\\nPhysical acting\\nKnowledge base\\nutilization\\nAssignment\\nplanning\\nSelf-cirtisim\\nSelf-refinementFig. 1: LLM Capabilities.\\nto PLMs, LLMs are not only much larger in model size, but\\nalso exhibit stronger language understanding and generation\\nabilities, and more importantly, emergent abilities that are\\nnot present in smaller-scale language models. As illustrated\\nin Fig. 1, these emergent abilities include (1) in-context\\nlearning, where LLMs learn a new task from a small set\\nof examples presented in the prompt at inference time, (2)\\ninstruction following, where LLMs, after instruction tuning,\\ncan follow the instructions for new types of tasks without\\nusing explicit examples, and (3) multi-step reasoning, where\\nLLMs can solve a complex task by breaking down that task\\ninto intermediate reasoning steps as demonstrated in the\\nchain-of-thought prompt [34]. LLMs can also be augmented\\nby using external knowledge and tools [35], [36] so that\\nthey can effectively interact with users and environment [37],\\nand continually improve itself using feedback data collected\\nthrough interactions (e.g. via reinforcement learning with\\nhuman feedback (RLHF)).\\nThrough advanced usage and augmentation techniques,\\nLLMs can be deployed as so-called AI agents: artificial entities\\nthat sense their environment, make decisions, and take actions.\\nPrevious research has focused on developing agents for specific\\ntasks and domains. The emergent abilities demonstrated by\\nLLMs make it possible to build general-purpose AI agents\\nbased on LLMs. While LLMs are trained to produce responses\\nin static settings, AI agents need to take actions to interact with\\ndynamic environment. Therefore, LLM-based agents often\\nneed to augment LLMs to e.g., obtain updated information\\nfrom external knowledge bases, verify whether a system action\\nproduces the expected result, and cope with when things do\\nnot go as expected, etc. We will discuss in detail LLM-based\\nagents in Section IV.\\nIn the rest of this paper, Section II presents an overview of\\nstate of the art of LLMs, focusing on three LLM families (GPT,\\nLLaMA and PaLM) and other representative models. Section\\nIII discusses how LLMs are built. Section IV discusses howLLMs are used, and augmented for real-world applications\\nSections V and VI review popular datasets and benchmarks for\\nevaluating LLMs, and summarize the reported LLM evaluation\\nresults. Finally, Section VII concludes the paper by summa-\\nrizing the challenges and future research directions.\\nII. L ARGE LANGUAGE MODELS\\nIn this section we start with a review of early pre-trained\\nneural language models as they are the base of LLMs, and\\nthen focus our discussion on three families of LLMs: GPT,\\nLlaMA, and PaLM. Table I provides an overview of some of\\nthese models and their characteristics.\\nA. Early Pre-trained Neural Language Models\\nLanguage modeling using neural networks was pioneered\\nby [38], [39], [40]. Bengio et al. [13] developed one of the first\\nneural language models (NLMs) that are comparable to n-gram\\nmodels. Then, [14] successfully applied NLMs to machine\\ntranslation. The release of RNNLM (an open source NLM\\ntoolkit) by Mikolov [41], [42] helped significantly popularize\\nNLMs. Afterwards, NLMs based on recurrent neural networks\\n(RNNs) and their variants, such as long short-term memory\\n(LSTM) [19] and gated recurrent unit (GRU) [20], were widely\\nused for many natural language applications including machine\\ntranslation, text generation and text classification [43].\\nThen, the invention of the Transformer architecture [44]\\nmarks another milestone in the development of NLMs. By\\napplying self-attention to compute in parallel for every word\\nin a sentence or document an ‚Äúattention score‚Äù to model the\\ninfluence each word has on another, Transformers allow for\\nmuch more parallelization than RNNs, which makes it possible\\nto efficiently pre-train very big language models on large\\namounts of data on GPUs. These pre-trained language models\\n(PLMs) can be fine-tuned for many downstream tasks.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 2}, page_content='Paper Strcuture\\nEarly Pre-trained\\nLanguage ModelsIILarge Language Models\\nAIII HOW LLMS ARE BUIL T\\nA\\nData Cleaning BLarge Language\\nModel FamiliesB\\nOther Representative\\nLLMsCDominant LLM\\nArchitectures\\nTokenizations C\\nPositional Encoding DModel Pre-training E\\nFine-tuning and\\nInstruction T uningF\\nAlignment G\\nDecoding StrategiesH\\nI HOW LLMS ARE USED AND AUGMENTED\\nA\\nBLLM limitationsCost-Effective T raining/Inference,\\nAdaptation & CompressionI\\nUsing LLMs: Prompt Design\\nand Engineering\\nCAugmenting LLMs through\\nexternal knowledge - RAG\\nD Using External T ools\\nE LLM AgentsV  POPULAR DA TASETS FOR LLMS\\nADatasets for Basic T asks: language\\nmodeling/understanding/generation\\nB Datasets for Emergent: ICL, reasoning,\\ninstruction following\\nCDatasets for Augmented: using\\nexternal knowledge/tools\\nVI PROMINENT LLMS‚Äô  PERFORMANCE\\nON BENCHMARKS\\nA\\nBVII CHALLENGES AND FUTURE DIRECTIONS\\nASmaller and more efficient\\nLanguage ModelsLLMs‚Äô  Performance on Different T asksPopular Metrics for Evaluating LLMs\\nBNew Post-attention\\nArchitectural Paradigms\\nC Multi-modal Models\\nDImproved LLM Usage and\\nAugmentation techniques\\nDSecurity and\\nEthical/Responsible AIFig. 2: The paper structure.\\nWe group early popular Transformer-based PLMs, based on\\ntheir neural architectures, into three main categories: encoder-\\nonly, decoder-only, and encoder-decoder models. Comprehen-\\nsive surveys of early PLMs are provided in [43], [28].\\n1) Encoder-only PLMs: As the name suggests, the encoder-\\nonly models only consist of an encoder network. These models\\nare originally developed for language understanding tasks,\\nsuch as text classification, where the models need to predict a\\nclass label for an input text. Representative encoder-only mod-\\nels include BERT and its variants, e.g., RoBERTa, ALBERT,\\nDeBERTa, XLM, XLNet, UNILM, as to be described below.BERT (Birectional Encoder Representations from Trans-\\nformers) [24] is one of the most widely used encoder-only\\nlanguage models. BERT consists of three modules: (1) an\\nembedding module that converts input text into a sequence\\nof embedding vectors, (2) a stack of Transformer encoders\\nthat converts embedding vectors into contextual representation\\nvectors, and (3) a fully connected layer that converts the\\nrepresentation vectors (at the final layer) to one-hot vectors.\\nBERT is pre-trained uses two objectives: masked language\\nmodeling (MLM) and next sentence prediction. The pre-trained\\nBERT model can be fine-tuned by adding a classifier layer\\nfor many language understanding tasks, ranging from text'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 3}, page_content='TABLE I: High-level Overview of Popular Language Models\\nType Model Name #Parameters Release Base Models Open\\nSource#Tokens Training dataset\\nBERT 110M, 340M 2018 - ‚úì 137B BooksCorpus, English Wikipedia\\nRoBERTa 355M 2019 - ‚úì 2.2T BooksCorpus, English Wikipedia, CC-NEWS,\\nSTORIES (a subset of Common Crawl), Reddit\\nEncoder-OnlyALBERT 12M, 18M, 60M,\\n235M2019 - ‚úì 137B BooksCorpus, English Wikipedia\\nDeBERTa - 2020 - ‚úì - BooksCorpus, English Wikipedia, STORIES, Red-\\ndit content\\nXLNet 110M, 340M 2019 - ‚úì 32.89B BooksCorpus, English Wikipedia, Giga5, Com-\\nmon Crawl, ClueWeb 2012-B\\nDecoder-onlyGPT-1 120M 2018 - ‚úì 1.3B BooksCorpus\\nGPT-2 1.5B 2019 - ‚úì 10B Reddit outbound\\nT5 (Base) 223M 2019 - ‚úì 156B Common Crawl\\nEncoder-DecoderMT5 (Base) 300M 2020 - ‚úì - New Common Crawl-based dataset in 101 lan-\\nguages (m Common Crawl)\\nBART (Base) 139M 2019 - ‚úì - Corrupting text\\nGPT-3 125M, 350M,\\n760M, 1.3B, 2.7B,\\n6.7B, 13B, 175B2020 √ó 300B Common Crawl (filtered), WebText2, Books1,\\nBooks2, Wikipedia\\nGPT FamilyCODEX 12B 2021 GPT ‚úì - Public GitHub software repositories\\nWebGPT 760M, 13B, 175B 2021 GPT-3 √ó - ELI5\\nGPT-4 1.76T 2023 - √ó 13T -\\nLLaMA1 7B, 13B, 33B, 65B 2023 - ‚úì 1T, 1.4T Online sources\\nLLaMA2 7B, 13B, 34B, 70B 2023 - ‚úì 2T Online sources\\nAlpaca 7B 2023 LLaMA1 ‚úì - GPT-3.5\\nVicuna-13B 13B 2023 LLaMA1 ‚úì - GPT-3.5\\nLLaMA FamilyKoala 13B 2023 LLaMA ‚úì - Dialogue data\\nMistral-7B 7.3B 2023 ‚úì - -\\nCode Llama 34 2023 LLaMA2 ‚úì 500B Publicly available code\\nLongLLaMA 3B, 7B 2023 OpenLLaMA ‚úì 1T -\\nLLaMA-Pro-8B 8.3B 2024 LLaMA2-7B ‚úì 80B Code and math corpora\\nTinyLlama-1.1B 1.1B 2024 LLaMA1.1B ‚úì 3T SlimPajama, Starcoderdata\\nPaLM 8B, 62B, 540B 2022 - √ó 780B Web documents, books, Wikipedia, conversations,\\nGitHub code\\nU-PaLM 8B, 62B, 540B 2022 - √ó 1.3B Web documents, books, Wikipedia, conversations,\\nGitHub code\\nPaLM FamilyPaLM-2 340B 2023 - ‚úì 3.6T Web documents, books, code, mathematics, con-\\nversational data\\nMed-PaLM 540B 2022 PaLM √ó 780B HealthSearchQA, MedicationQA, LiveQA\\nMed-PaLM 2 - 2023 PaLM 2 √ó - MedQA, MedMCQA, HealthSearchQA, LiveQA,\\nMedicationQA\\nFLAN 137B 2021 LaMDA-PT ‚úì - Web documents, code, dialog data, Wikipedia\\nGopher 280B 2021 - √ó 300B MassiveText\\nERNIE 4.0 10B 2023 - √ó 4TB Chinese text\\nRetro 7.5B 2021 - √ó 600B MassiveText\\nLaMDA 137B 2022 - √ó 168B public dialog data and web documents\\nChinChilla 70B 2022 - √ó 1.4T MassiveText\\nGalactia-120B 120B 2022 - 450B\\nOther Popular LLMsCodeGen 16.1B 2022 - ‚úì - THE PILE, BIGQUERY , BIGPYTHON\\nBLOOM 176B 2022 - ‚úì 366B ROOTS\\nZephyr 7.24B 2023 Mistral-7B ‚úì 800B Synthetic data\\nGrok-0 33B 2023 - √ó - Online source\\nORCA-2 13B 2023 LLaMA2 - 2001B -\\nStartCoder 15.5B 2023 - ‚úì 35B GitHub\\nMPT 7B 2023 - ‚úì 1T RedPajama, m Common Crawl, S2ORC, Common\\nCrawl\\nMixtral-8x7B 46.7B 2023 - ‚úì - Instruction dataset\\nFalcon 180B 180B 2023 - ‚úì 3.5T RefinedWeb\\nGemini 1.8B, 3.25B 2023 ‚úì - Web documents, books, and code, image data,\\naudio data, video data\\nDeepSeek-Coder 1.3B, 6.7B, 33B 2024 - ‚úì 2T GitHub‚Äôs Markdown and StackExchange\\nDocLLM 1B,7B 2024 - √ó 2T IIT-CDIP Test Collection 1.0, DocBank\\nclassification, question answering to language inference. A\\nhigh-level overview of BERT framework is shown in Fig 3. As\\nBERT significantly improved state of the art on a wide range\\nof language understanding tasks when it was published, the AI\\ncommunity was inspired to develop many similar encoder-only\\nlanguage models based on BERT.\\nRoBERTa [25] significantly improves the robustness of\\nBERT using a set of model design choices and training strate-\\ngies, such as modifying a few key hyperparameters, removing\\nthe next-sentence pre-training objective and training with muchlarger mini-batches and learning rates. ALBERT [45] uses two\\nparameter-reduction techniques to lower memory consumption\\nand increase the training speed of BERT: (1) splitting the\\nembedding matrix into two smaller matrices, and (2) using\\nrepeating layers split among groups. DeBERTa (Decoding-\\nenhanced BERT with disentangled attention) [26] improves the\\nBERT and RoBERTa models using two novel techniques. The\\nfirst is the disentangled attention mechanism, where each word\\nis represented using two vectors that encode its content and\\nposition, respectively, and the attention weights among words'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 4}, page_content='Fig. 3: Overall pre-training and fine-tuning procedures for\\nBERT. Courtesy of [24]\\nare computed using disentangled matrices on their contents and\\nrelative positions, respectively. Second, an enhanced mask de-\\ncoder is used to incorporate absolute positions in the decoding\\nlayer to predict the masked tokens in model pre-training. In\\naddition, a novel virtual adversarial training method is used for\\nfine-tuning to improve models‚Äô generalization. ELECTRA [46]\\nuses a new pre-training task, known as replaced token detection\\n(RTD), which is empirically proven to be more sample-efficient\\nthan MLM. Instead of masking the input, RTD corrupts it by\\nreplacing some tokens with plausible alternatives sampled from\\na small generator network. Then, instead of training a model\\nthat predicts the original identities of the corrupted tokens, a\\ndiscriminative model is trained to predict whether a token in\\nthe corrupted input was replaced by a generated sample or not.\\nRTD is more sample-efficient than MLM because the former\\nis defined over all input tokens rather than just the small subset\\nbeing masked out, as illustrated in Fig 4.\\nFig. 4: A comparison between replaced token detection and\\nmasked language modeling. Courtesy of [46].\\nXLMs [47] extended BERT to cross-lingual language\\nmodels using two methods: (1) a unsupervised method that\\nonly relies on monolingual data, and (2) a supervised method\\nthat leverages parallel data with a new cross-lingual language\\nmodel objective, as illustrated in Fig 5. XLMs had obtained\\nstate-of-the-art results on cross-lingual classification, unsuper-\\nvised and supervised machine translation, at the time they were\\nproposed.\\nThere are also encoder-only language models that leverage\\nthe advantages of auto-regressive (decoder) models for model\\ntraining and inference. Two examples are XLNet and UNILM.\\nXLNet [48] is based on Transformer-XL, pre-trained using a\\ngeneralized autoregressive method that enables learning bidi-\\nrectional contexts by maximizing the expected likelihood over\\nFig. 5: Cross-lingual language model pretraining. The MLM\\nobjective is similar to BERT, but with continuous streams\\nof text as opposed to sentence pairs. The TLM objective\\nextends MLM to pairs of parallel sentences. To predict a\\nmasked English word, the model can attend to both the English\\nsentence and its French translation, and is encouraged to align\\nEnglish and French representations. Courtesy of [47].\\nall permutations of the factorization order. UNILM (UNIfied\\npre-trained Language Model) [49] is pre-trained using three\\ntypes of language modeling tasks: unidirectional, bidirectional,\\nand sequence-to-sequence prediction. This is achieved by\\nemploying a shared Transformer network and utilizing specific\\nself-attention masks to control what context the prediction is\\nconditioned on, as illustrated in Fig 6. The pre-trained model\\ncan be fine-tuned for both natural language understanding and\\ngeneration tasks.\\nFig. 6: Overview of unified LM pre-training. The model\\nparameters are shared across the LM objectives (i.e., bidirec-\\ntional LM, unidirectional LM, and sequence-to-sequence LM).\\nCourtesy of [49].\\n2) Decoder-only PLMs: Two of the most widely used\\ndecoder-only PLMs are GPT-1 and GPT-2, developed by\\nOpenAI. These models lay the foundation to more powerful\\nLLMs subsequently, i.e., GPT-3 and GPT-4.\\nGPT-1 [50] demonstrates for the first time that good\\nperformance over a wide range of natural language tasks can be\\nobtained by Generative Pre-Training (GPT) of a decoder-only\\nTransformer model on a diverse corpus of unlabeled text in a\\nself-supervised learning fashion (i.e., next word/token predic-'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 5}, page_content='tion), followed by discriminative fine-tuning on each specific\\ndownstream task (with much fewer samples), as illustrated in\\nFig 7. GPT-1 paves the way for subsequent GPT models, with\\neach version improving upon the architecture and achieving\\nbetter performance on various language tasks.\\nFig. 7: High-level overview of GPT pretraining, and fine-tuning\\nsteps. Courtesy of OpenAI.\\nGPT-2 [51] shows that language models are able to learn\\nto perform specific natural language tasks without any explicit\\nsupervision when trained on a large WebText dataset consisting\\nof millions of webpages. The GPT-2 model follows the model\\ndesigns of GPT-1 with a few modifications: Layer normal-\\nization is moved to the input of each sub-block, additional\\nlayer normalization is added after the final self-attention block,\\ninitialization is modified to account for the accumulation on\\nthe residual path and scaling the weights of residual layers,\\nvocabulary size is expanded to 50,25, and context size is\\nincreased from 512 to 1024 tokens.\\n3) Encoder-Decoder PLMs: In [52], Raffle et al. shows that\\nalmost all NLP tasks can be cast as a sequence-to-sequence\\ngeneration task. Thus, an encoder-decoder language model, by\\ndesign, is a unified model in that it can perform all natural\\nlanguage understanding and generation tasks. Representative\\nencoder-decoder PLMs we will review below are T5, mT5,\\nMASS, and BART.\\nT5 [52] is a Text-to-Text Transfer Transformer (T5) model,\\nwhere transfer learning is effectively exploited for NLP via an\\nintroduction of a unified framework in which all NLP tasks are\\ncast as a text-to-text generation task. mT5 [53] is a multilingual\\nvariant of T5, which is pre-trained on a new Common Crawl-\\nbased dataset consisting of texts in 101 languages.\\nMASS (MAsked Sequence to Sequence pre-training) [54]\\nadopts the encoder-decoder framework to reconstruct a sen-\\ntence fragment given the remaining part of the sentence. The\\nencoder takes a sentence with randomly masked fragment\\n(several consecutive tokens) as input, and the decoder predicts\\nthe masked fragment. In this way, MASS jointly trains the\\nencoder and decoder for language embedding and generation,\\nrespectively.\\nBART [55] uses a standard sequence-to-sequence transla-\\ntion model architecture. It is pre-trained by corrupting text with\\nan arbitrary noising function, and then learning to reconstruct\\nthe original text.B. Large Language Model Families\\nLarge language models (LLMs) mainly refer to\\ntransformer-based PLMs that contain tens to hundreds\\nof billions of parameters. Compared to PLMs reviewed above,\\nLLMs are not only much larger in model size, but also exhibit\\nstronger language understanding and generation and emergent\\nabilities that are not present in smaller-scale models. In what\\nfollows, we review three LLM families: GPT, LLaMA, and\\nPaLM, as illustrated in Fig 8.\\n1)The GPT Family :Generative Pre-trained Transform-\\ners (GPT) are a family of decoder-only Transformer-based\\nlanguage models, developed by OpenAI. This family con-\\nsists of GPT-1, GPT-2, GPT-3, InstrucGPT, ChatGPT, GPT-4,\\nCODEX, and WebGPT. Although early GPT models, such as\\nGPT-1 and GPT-2, are open-source, recent models, such as\\nGPT-3 and GPT-4, are close-source and can only be accessed\\nvia APIs. GPT-1 and GPT-2 models have been discussed in\\nthe early PLM subsection. We start with GPT-3 below.\\nGPT-3 [56] is a pre-trained autoregressive language model\\nwith 175 billion parameters. GPT-3 is widely considered as\\nthe first LLM in that it not only is much larger than previous\\nPLMs, but also for the first time demonstrates emergent\\nabilities that are not observed in previous smaller PLMs. GPT-\\n3 shows the emergent ability of in-context learning, which\\nmeans GPT-3 can be applied to any downstream tasks without\\nany gradient updates or fine-tuning, with tasks and few-shot\\ndemonstrations specified purely via text interaction with the\\nmodel. GPT-3 achieved strong performance on many NLP\\ntasks, including translation, question-answering, and the cloze\\ntasks, as well as several ones that require on-the-fly reasoning\\nor domain adaptation, such as unscrambling words, using a\\nnovel word in a sentence, 3-digit arithmetic. Fig 9 plots the\\nperformance of GPT-3 as a function of the number of examples\\nin in-context prompts.\\nCODEX [57], released by OpenAI in March 2023, is a\\ngeneral-purpose programming model that can parse natural\\nlanguage and generate code in response. CODEX is a de-\\nscendant of GPT-3, fine-tuned for programming applications\\non code corpora collected from GitHub. CODEX powers\\nMicrosoft‚Äôs GitHub Copilot.\\nWebGPT [58] is another descendant of GPT-3, fine-tuned to\\nanswer open-ended questions using a text-based web browser,\\nfacilitating users to search and navigate the web. Specifically,\\nWebGPT is trained in three steps. The first is for WebGPT\\nto learn to mimic human browsing behaviors using human\\ndemonstration data. Then, a reward function is learned to\\npredict human preferences. Finally, WebGPT is refined to\\noptimize the reward function via reinforcement learning and\\nrejection sampling.\\nTo enable LLMs to follow expected human instructions,\\nInstructGPT [59] is proposed to align language models with\\nuser intent on a wide range of tasks by fine-tuning with\\nhuman feedback. Starting with a set of labeler-written prompts\\nand prompts submitted through the OpenAI API, a dataset\\nof labeler demonstrations of the desired model behavior is\\ncollected. Then GPT-3 is fine-tuned on this dataset. Then, a\\ndataset of human-ranked model outputs is collected to further\\nfine-tune the model using reinforcement learning. The method\\nis known Reinforcement Learning from Human Feedback'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 6}, page_content='GPT Family PaLM Family    LLaMA  1/2 Family\\nGPT\\nGPT1\\nGPT2\\nGPT3\\nGPT4\\nGPT3.5 Turbo\\ntext-davinci\\ncode-davinci\\nCODEX\\nInstructGPT\\nWebGPT\\nGPT4 V ision\\nGPT4 Turbo\\nGorilla\\nMistral\\nVigogne\\nStable Beluga2\\nKoala\\nCode LLaMA\\nVicuna\\n Alpaca\\nBaize\\nLong LLaMA\\nGiraf fe\\nGuanaco\\nTulu\\nWizardLM\\nMed-PaLM\\nPaLM-E\\n Med-PaLM2\\nFLAN-PaLM\\n U-PaLM\\nPaLM2\\nPaLM\\nFig. 8: Popular LLM Families.\\nFig. 9: GPT-3 shows that larger models make increasingly\\nefficient use of in-context information. It shows in-context\\nlearning performance on a simple task requiring the model to\\nremove random symbols from a word, both with and without\\na natural language task description. Courtesy of [56].\\n(RLHF), as shown in 10. The resultant InstructGPT models\\nhave shown improvements in truthfulness and reductions in\\ntoxic output generation while having minimal performance\\nregressions on public NLP datasets.\\nFig. 10: The high-level overview of RLHF. Courtesy of [59].\\nThe most important milestone of LLM development is thelaunch of ChatGPT (Chat Generative Pre-trained Transformer)\\n[60] on November 30, 2022. ChatGPT is chatbot that enables\\nusers to steer a conversation to complete a wide range of\\ntasks such as question answering, information seeking, text\\nsummarization, and more. ChatGPT is powered by GPT-3.5\\n(and later by GPT-4), a sibling model to InstructGPT, which\\nis trained to follow an instruction in a prompt and provide a\\ndetailed response.\\nGPT-4 [33] is the latest and most powerful LLM in the\\nGPT family. Launched in March, 2023, GPT-4 is a multi-\\nmodal LLM in that it can take image and text as inputs and\\nproduce text outputs. While still less capable than humans\\nin some of the most challenging real-world scenarios, GPT-4\\nexhibits human-level performance on various professional and\\nacademic benchmarks, including passing a simulated bar exam\\nwith a score around the top 10% of test takers, as shown in\\nFig 11. Like early GPT models, GPT-4 was first pre-trained to\\npredict next tokens on large text corpora, and then fine-tuned\\nwith RLHF to align model behaviors with human-desired ones.\\n2)The LLaMA Family :LLaMA is a collection of founda-\\ntion language models, released by Meta. Unlike GPT models,\\nLLaMA models are open-source, i.e., model weights are\\nreleased to the research community under a noncommercial\\nlicense. Thus, the LLaMA family grows rapidly as these\\nmodels are widely used by many research groups to develop\\nbetter open-source LLMs to compete the closed-source ones or\\nto develop task-specific LLMs for mission-critical applications.\\nThe first set of LLaMA models [32] was released in Febru-\\nary 2023, ranging from 7B to 65B parameters. These models\\nare pre-trained on trillions of tokens, collected from publicly\\navailable datasets. LLaMA uses the transformer architecture of\\nGPT-3, with a few minor architectural modifications, including\\n(1) using a SwiGLU activation function instead of ReLU,\\n(2) using rotary positional embeddings instead of absolute\\npositional embedding, and (3) using root-mean-squared layer-\\nnormalization instead of standard layer-normalization. The\\nopen-source LLaMA-13B model outperforms the proprietary\\nGPT-3 (175B) model on most benchmarks, making it a good\\nbaseline for LLM research.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 7}, page_content='Fig. 11: GPT-4 performance on academic and professional\\nexams, compared with GPT 3.5. Courtesy of [33].\\nIn July 2023, Meta, in partnership with Microsoft, released\\nthe LLaMA-2 collection [61], which include both foundation\\nlanguage models and Chat models finetuned for dialog, known\\nas LLaMA-2 Chat. The LLaMA-2 Chat models were reported\\nto outperform other open-source models on many public\\nbenchmarks. Fig 12 shows the training process of LLaMA-2\\nChat. The process begins with pre-training LLaMA-2 using\\npublicly available online data. Then, an initial version of\\nLLaMA-2 Chat is built via supervised fine-tuning. Subse-\\nquently, the model is iteratively refined using RLHF, rejection\\nsampling and proximal policy optimization. In the RLHF stage,\\nthe accumulation of human feedback for revising the reward\\nmodel is crucial to prevent the reward model from being\\nchanged too much, which could hurt the stability of LLaMA\\nmodel training.\\nFig. 12: Training of LLaMA-2 Chat. Courtesy of [61].\\nAlpaca [62] is fine-tuned from the LLaMA-7B model using\\n52K instruction-following demonstrations generated in the\\nstyle of self-instruct using GPT-3.5 (text-davinci-003). Alpaca\\nis very cost-effective for training, especially for academic\\nresearch. On the self-instruct evaluation set, Alpaca performs\\nsimilarly to GPT-3.5, despite that Alpaca is much smaller.\\nThe Vicuna team has developed a 13B chat model, Vicuna-\\n13B, by fine-tuning LLaMA on user-shared conversationscollected from ShareGPT. Preliminary evaluation using GPT-\\n4 as a evaluator shows that Vicuna-13B achieves more than\\n90% quality of OpenAI‚Äôs ChatGPT, and Google‚Äôs Bard while\\noutperforming other models like LLaMA and Stanford Alpaca\\nin more than 90% of cases. 13 shows the relative response\\nquality of Vicuna and a few other well-known models by\\nGPT-4. Another advantage of Vicuna-13B is its relative limited\\ncomputational demand for model training. The training cost of\\nVicuna-13B is merely $300.\\nFig. 13: Relative Response Quality of Vicuna and a few other\\nwell-known models by GPT-4. Courtesy of Vicuna Team.\\nLike Alpaca and Vicuna, the Guanaco models [63] are also\\nfinetuned LLaMA models using instruction-following data. But\\nthe finetuning is done very efficiently using QLoRA such\\nthat finetuning a 65B parameter model can be done on a\\nsingle 48GB GPU. QLoRA back-propagates gradients through\\na frozen, 4-bit quantized pre-trained language model into Low\\nRank Adapters (LoRA). The best Guanaco model outperforms\\nall previously released models on the Vicuna benchmark,\\nreaching 99.3% of the performance level of ChatGPT while\\nonly requiring 24 hours of fine-tuning on a single GPU.\\nKoala [64] is yet another instruction-following language\\nmodel built on LLaMA, but with a specific focus on interaction\\ndata that include user inputs and responses generated by highly\\ncapable closed-source chat models such as ChatGPT. The\\nKoala-13B model performs competitively with state-of-the-art\\nchat models according to human evaluation based on real-\\nworld user prompts.\\nMistral-7B [65] is a 7B-parameter language model engi-\\nneered for superior performance and efficiency. Mistral-7B\\noutperforms the best open-source 13B model (LLaMA-2-13B)\\nacross all evaluated benchmarks, and the best open-source\\n34B model (LLaMA-34B) in reasoning, mathematics, and code\\ngeneration. This model leverages grouped-query attention for\\nfaster inference, coupled with sliding window attention to\\neffectively handle sequences of arbitrary length with a reduced\\ninference cost.\\nThe LLaMA family is growing rapidly, as more instruction-\\nfollowing models have been built on LLaMA or LLaMA-\\n2, including Code LLaMA [66], Gorilla [67], Giraffe [68],\\nVigogne [69], Tulu 65B [70], Long LLaMA [71], and Stable\\nBeluga2 [72], just to name a few.\\n3)The PaLM Family :The PaLM (Pathways Language\\nModel) family are developed by Google. The first PaLM\\nmodel [31] was announced in April 2022 and remained private\\nuntil March 2023. It is a 540B parameter transformer-based\\nLLM. The model is pre-trained on a high-quality text corpus\\nconsisting of 780 billion tokens that comprise a wide range\\nof natural language tasks and use cases. PaLM is pre-trained'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 8}, page_content='on 6144 TPU v4 chips using the Pathways system, which\\nenables highly efficient training across multiple TPU Pods.\\nPaLM demonstrates continued benefits of scaling by achiev-\\ning state-of-the-art few-shot learning results on hundreds of\\nlanguage understanding and generation benchmarks. PaLM-\\n540B outperforms not only state-of-the-art fine-tuned models\\non a suite of multi-step reasoning tasks, but also on par with\\nhumans on the recently released BIG-bench benchmark.\\nThe U-PaLM models of 8B, 62B, and 540B scales are\\ncontinually trained on PaLM with UL2R, a method of continue\\ntraining LLMs on a few steps with UL2‚Äôs mixture-of-denoiser\\nobjective [73]. An approximately 2x computational savings\\nrate is reported.\\nU-PaLM is later instruction-finetuned as Flan-PaLM [74].\\nCompared to other instruction finetuning work mentioned\\nabove, Flan-PaLM‚Äôs finetuning is performed using a much\\nlarger number of tasks, larger model sizes, and chain-of-\\nthought data. As a result, Flan-PaLM substantially outperforms\\nprevious instruction-following models. For instance, Flan-\\nPaLM-540B, which is instruction-finetuned on 1.8K tasks,\\noutperforms PaLM-540B by a large margin (+9.4% on av-\\nerage). The finetuning data comprises 473 datasets, 146 task\\ncategories, and 1,836 total tasks, as illustrated in Fig 14.\\nFig. 14: Flan-PaLM finetuning consist of 473 datasets in above\\ntask categories. Courtesy of [74].\\nPaLM-2 [75] is a more compute-efficient LLM with bet-\\nter multilingual and reasoning capabilities, compared to its\\npredecessor PaLM. PaLM-2 is trained using a mixture of\\nobjectives. Through extensive evaluations on English, multi-\\nlingual, and reasoning tasks, PaLM-2 significantly improves\\nthe model performance on downstream tasks across different\\nmodel sizes, while simultaneously exhibiting faster and more\\nefficient inference than PaLM.\\nMed-PaLM [76] is a domain-specific PaLM, and is de-\\nsigned to provide high-quality answers to medical questions.\\nMed-PaLM is finetuned on PaLM using instruction prompt\\ntuning, a parameter-efficient method for aligning LLMs to\\nnew domains using a few exemplars. Med-PaLM obtains very\\nencouraging results on many healthcare tasks, although it is\\nstill inferior to human clinicians. Med-PaLM 2 improves Med-\\nPaLM via med-domain finetuning and ensemble prompting[77]. Med-PaLM 2 scored up to 86.5% on the MedQA\\ndataset (i.e., a benchmark combining six existing open ques-\\ntion answering datasets spanning professional medical exams,\\nresearch, and consumer queries), improving upon Med-PaLM\\nby over 19% and setting a new state-of-the-art.\\nC. Other Representative LLMs\\nIn addition to the models discussed in the previous sub-\\nsections, there are other popular LLMs which do not belong\\nto those three model families, yet they have achieved great\\nperformance and have pushed the LLMs field forward. We\\nbriefly describe these LLMs in this subsection.\\nFLAN: In [78], Wei et al. explored a simple method for\\nimproving the zero-shot learning abilities of language models.\\nThey showed that instruction tuning language models on a\\ncollection of datasets described via instructions substantially\\nimproves zero-shot performance on unseen tasks. They take\\na 137B parameter pretrained language model and instruction\\ntune it on over 60 NLP datasets verbalized via natural language\\ninstruction templates. They call this instruction-tuned model\\nFLAN. Fig 15 provides a comparison of instruction tuning\\nwith pretrain‚Äìfinetune and prompting.\\nFig. 15: comparison of instruction tuning with pre-\\ntrain‚Äìfinetune and prompting. Courtesy of [78].\\nGopher: In [79], Rae et al. presented an analysis of\\nTransformer-based language model performance across a wide\\nrange of model scales ‚Äî from models with tens of millions of\\nparameters up to a 280 billion parameter model called Gopher.\\nThese models were evaluated on 152 diverse tasks, achieving\\nstate-of-the-art performance across the majority. The number\\nof layers, the key/value size, and other hyper-parameters of\\ndifferent model sizes are shown in Fig 16.\\nFig. 16: Model architecture details of Gopher with different\\nnumber of parameters. Courtesy of [78].\\nT0:In [80], Sanh et al. developed T0, a system for easily\\nmapping any natural language tasks into a human-readable\\nprompted form. They converted a large set of supervised\\ndatasets, each with multiple prompts with diverse wording.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 9}, page_content='These prompted datasets allow for benchmarking the ability\\nof a model to perform completely held-out tasks. Then, a\\nT0 encoder-decoder model is developed to consume textual\\ninputs and produces target responses. The model is trained on\\na multitask mixture of NLP datasets partitioned into different\\ntasks.\\nERNIE 3.0: In [81], Sun et al. proposed a unified frame-\\nwork named ERNIE 3.0 for pre-training large-scale knowledge\\nenhanced models. It fuses auto-regressive network and auto-\\nencoding network, so that the trained model can be easily tai-\\nlored for both natural language understanding and generation\\ntasks using zero-shot learning, few-shot learning or fine-tuning.\\nThey have trained ERNIE 3.0 with 10 billion parameters\\non a 4TB corpus consisting of plain texts and a large-scale\\nknowledge graph. Fig 17 illustrates the model architecture of\\nErnie 3.0.\\nFig. 17: High-level model architecture of ERNIE 3.0. Courtesy\\nof [81].\\nRETRO: In [82], Borgeaud et al. enhanced auto-regressive\\nlanguage models by conditioning on document chunks re-\\ntrieved from a large corpus, based on local similarity with pre-\\nceding tokens. Using a 2-trillion-token database, the Retrieval-\\nEnhanced Transformer (Retro) obtains comparable perfor-\\nmance to GPT-3 and Jurassic-1 [83] on the Pile, despite using\\n25% fewer parameters. As shown in Fig 18, Retro combines\\na frozen Bert retriever, a differentiable encoder and a chunked\\ncross-attention mechanism to predict tokens based on an order\\nof magnitude more data than what is typically consumed\\nduring training.\\nGLaM: In [84], Du et al. proposed a family of LLMs\\nnamed GLaM (Generalist Language Model), which use a\\nsparsely activated mixture-of-experts architecture to scale the\\nmodel capacity while also incurring substantially less training\\ncost compared to dense variants. The largest GLaM has 1.2\\ntrillion parameters, which is approximately 7x larger than GPT-\\n3. It consumes only 1/3 of the energy used to train GPT-3 and\\nrequires half of the computation flops for inference, while still\\nachieving better overall zero, one and few-shot performance\\nacross 29 NLP tasks. Fig 19 shows the high-level architecture\\nof GLAM.\\nLaMDA: In [85], Thoppilan et al. presented LaMDA, a\\nfamily of Transformer-based neural language models special-\\nized for dialog, which have up to 137B parameters and are\\npre-trained on 1.56T words of public dialog data and web text.\\nFig. 18: Retro architecture. Left: simplified version where a\\nsequence of length n = 12 is split into l = 3 chunks of size\\nm = 4. For each chunk, we retrieve k = 2 neighbours of r =\\n5 tokens each. The retrieval pathway is shown on top. Right:\\nDetails of the interactions in the CCA operator. Causality is\\nmaintained as neighbours of the first chunk only affect the last\\ntoken of the first chunk and tokens from the second chunk.\\nCourtesy of [82].\\nFig. 19: GLaM model architecture. Each MoE layer (the\\nbottom block) is interleaved with a Transformer layer (the\\nupper block). Courtesy of [84].\\nThey showed that fine-tuning with annotated data and enabling\\nthe model to consult external knowledge sources can lead to\\nsignificant improvements towards the two key challenges of\\nsafety and factual grounding.\\nOPT: In [86], Zhang et al. presented Open Pre-trained\\nTransformers (OPT), a suite of decoder-only pre-trained trans-\\nformers ranging from 125M to 175B parameters, which they\\nshare with researchers. The OPT models‚Äô parameters are\\nshown in 20\\nFig. 20: Different OPT Models‚Äô architecture details. Courtesy\\nof [86].'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 10}, page_content='Chinchilla: In [2], Hoffmann et al. investigated the optimal\\nmodel size and number of tokens for training a transformer\\nlanguage model under a given compute budget. By training\\nover 400 language models ranging from 70 million to over\\n16 billion parameters on 5 to 500 billion tokens, they found\\nthat for compute-optimal training, the model size and the\\nnumber of training tokens should be scaled equally: for every\\ndoubling of model size the number of training tokens should\\nalso be doubled. They tested this hypothesis by training a\\npredicted compute-optimal model, Chinchilla, that uses the\\nsame compute budget as Gopher but with 70B parameters and\\n4% more more data.\\nGalactica: In [87], Taylor et al. introduced Galactica, a\\nlarge language model that can store, combine and reason about\\nscientific knowledge. They trained on a large scientific corpus\\nof papers, reference material, knowledge bases and many other\\nsources. Galactica performed well on reasoning, outperforming\\nChinchilla on mathematical MMLU by 41.3% to 35.7%, and\\nPaLM 540B on MATH with a score of 20.4% versus 8.8%.\\nCodeGen: In [88], Nijkamp et al. trained and released\\na family of large language models up to 16.1B parameters,\\ncalled CODEGEN, on natural language and programming\\nlanguage data, and open sourced the training library JAX-\\nFORMER. They showed the utility of the trained model by\\ndemonstrating that it is competitive with the previous state-of-\\nthe-art on zero-shot Python code generation on HumanEval.\\nThey further investigated the multi-step paradigm for program\\nsynthesis, where a single program is factorized into multi-\\nple prompts specifying sub-problems. They also constructed\\nan open benchmark, Multi-Turn Programming Benchmark\\n(MTPB), consisting of 115 diverse problem sets that are\\nfactorized into multi-turn prompts.\\nAlexaTM: In [89], Soltan et al. demonstrated that mul-\\ntilingual large-scale sequence-to-sequence (seq2seq) models,\\npre-trained on a mixture of denoising and Causal Language\\nModeling (CLM) tasks, are more efficient few-shot learners\\nthan decoder-only models on various task. They trained a\\n20 billion parameter multilingual seq2seq model called Alexa\\nTeacher Model (AlexaTM 20B) and showed that it achieves\\nstate-of-the-art (SOTA) performance on 1-shot summarization\\ntasks, outperforming a much larger 540B PaLM decoder\\nmodel. AlexaTM consist of 46 encoder layers, 32 decoder\\nlayers, 32 attention heads, and dmodel = 4096 .\\nSparrow: In [90], Glaese et al. presented Sparrow, an\\ninformation-seeking dialogue agent trained to be more helpful,\\ncorrect, and harmless compared to prompted language model\\nbaselines. They used reinforcement learning from human feed-\\nback to train their models with two new additions to help\\nhuman raters judge agent behaviour. The high-level pipeline\\nof Sparrow model is shown in Fig 21.\\nMinerva: In [91], Lewkowycz et al. introduced Minerva,\\na large language model pretrained on general natural language\\ndata and further trained on technical content, to tackle previous\\nLLM struggle with quantitative reasoning (such as solving\\nmathematics, science, and engineering problems).\\nMoD: In [92], Tay et al. presented a generalized and\\nunified perspective for self-supervision in NLP and show how\\ndifferent pre-training objectives can be cast as one another\\nand how interpolating between different objectives can be\\nFig. 21: Sparrow pipeline relies on human participation to\\ncontinually expand a training set. Courtesy of [90].\\neffective. They proposed Mixture-of-Denoisers (MoD), a pre-\\ntraining objective that combines diverse pre-training paradigms\\ntogether. This framework is known as Unifying Language\\nLearning (UL2). An overview of UL2 pretraining paradigm\\nis shown in Fig 21.\\nFig. 22: An overview of UL2 pretraining paradigm. Courtesy\\nof [92].\\nBLOOM: In [93], Scao et al. presented BLOOM, a 176B-\\nparameter open-access language model designed and built\\nthanks to a collaboration of hundreds of researchers. BLOOM\\nis a decoder-only Transformer language model trained on the\\nROOTS corpus, a dataset comprising hundreds of sources in\\n46 natural and 13 programming languages (59 in total). An\\noverview of BLOOM architecture is shown in Fig 23.\\nFig. 23: An overview of BLOOM architecture. Courtesy of\\n[93].\\nGLM: In [94], Zeng et al. introduced GLM-130B, a'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 11}, page_content='bilingual (English and Chinese) pre-trained language model\\nwith 130 billion parameters. It was an attempt to open-source\\na 100B-scale model at least as good as GPT-3 (davinci) and\\nunveil how models of such a scale can be successfully pre-\\ntrained.\\nPythia: In [95], Biderman et al. introduced Pythia, a suite\\nof 16 LLMs all trained on public data seen in the exact same\\norder and ranging in size from 70M to 12B parameters. We\\nprovide public access to 154 checkpoints for each one of the\\n16 models, alongside tools to download and reconstruct their\\nexact training dataloaders for further study.\\nOrca: In [96], Mukherjee et al. develop Orca, a 13-billion\\nparameter model that learns to imitate the reasoning process\\nof large foundation models. Orca learns from rich signals\\nfrom GPT-4 including explanation traces; step-by-step thought\\nprocesses; and other complex instructions, guided by teacher\\nassistance from ChatGPT.\\nStarCoder: In [97], Li et al. introduced StarCoder and\\nStarCoderBase. They are 15.5B parameter models with 8K\\ncontext length, infilling capabilities and fast large-batch in-\\nference enabled by multi-query attention. StarCoderBase is\\ntrained on one trillion tokens sourced from The Stack, a\\nlarge collection of permissively licensed GitHub repositories\\nwith inspection tools and an opt-out process. They fine-tuned\\nStarCoderBase on 35B Python tokens, resulting in the creation\\nof StarCoder. They performed the most comprehensive evalu-\\nation of Code LLMs to date and showed that StarCoderBase\\noutperforms every open Code LLM that supports multiple pro-\\ngramming languages and matches or outperforms the OpenAI\\ncode-cushman-001 model.\\nKOSMOS: In [98], Huang et al. introduced KOSMOS-1,\\na Multimodal Large Language Model (MLLM) that can per-\\nceive general modalities, learn in context (i.e., few-shot), and\\nfollow instructions (i.e. zero-shot). Specifically, they trained\\nKOSMOS-1 from scratch on web-scale multi-modal corpora,\\nincluding arbitrarily interleaved text and images, image-caption\\npairs, and text data. Experimental results show that KOSMOS-\\n1 achieves impressive performance on (i) language understand-\\ning, generation, and even OCR-free NLP (directly fed with\\ndocument images), (ii) perception-language tasks, including\\nmultimodal dialogue, image captioning, visual question an-\\nswering, and (iii) vision tasks, such as image recognition with\\ndescriptions (specifying classification via text instructions).\\nGemini: In [99], Gemini team introduced a new family of\\nmultimodal models, that exhibit promising capabilities across\\nimage, audio, video, and text understanding. Gemini family\\nincludes three versions: Ultra for highly-complex tasks, Pro\\nfor enhanced performance and deployability at scale, and Nano\\nfor on-device applications. Gemini architecture is built on top\\nof Transformer decoders, and is trained to support 32k context\\nlength (via using efficient attention mechanisms).\\nSome of the other popular LLM frameworks (or techniques\\nused for efficient developments of LLMs) includes Inner-\\nMonologue [100], Megatron-Turing NLG [101], LongFormer\\n[102], OPT-IML [103], MeTaLM [104], Dromedary [105],\\nPalmyra [106], Camel [107], Yalm [108], MPT [109], ORCA-\\n2 [110], Gorilla [67], PAL [111], Claude [112], CodeGen 2\\n[113], Zephyr [114], Grok [115], Qwen [116], Mamba [30],\\nMixtral-8x7B [117], DocLLM [118], DeepSeek-Coder [119],FuseLLM-7B [120], TinyLlama-1.1B [121], LLaMA-Pro-8B\\n[122].\\nFig 24 provides an overview of some of the most repre-\\nsentative LLM frameworks, and the relevant works that have\\ncontributed to the success of LLMs and helped to push the\\nlimits of LLMs.\\nIII. H OWLLM SAREBUILT\\nIn this section, we first review the popular architectures\\nused for LLMs, and then discuss data and modeling techniques\\nranging from data preparation, tokenization, to pre-training,\\ninstruction tuning, and alignment.\\nOnce the model architecture is chosen, the major steps\\ninvolved in training an LLM includes: data preparation (col-\\nlection, cleaning, deduping, etc.), tokenization, model pre-\\ntraining (in a self-supervised learning fashion), instruction\\ntuning, and alignment. We will explain each of them in a\\nseparate subsection below. These steps are also illustrated in\\nFig 25.\\nA. Dominant LLM Architectures\\nThe most widely used LLM architectures are encoder-only,\\ndecoder-only, and encoder-decoder. Most of them are based on\\nTransformer (as the building block). Therefore we also review\\nthe Transformer architecture here.\\n1)Transformer :in a ground-breaking work [44], Vaswani\\net al. proposed the Transformer framework, which was orig-\\ninally designed for effective parallel computing using GPUs.\\nThe heart of Transformer is the (self-)attention mechanism,\\nwhich can capture long-term contextual information much\\nmore effectively using GPUs than the recurrence and convo-\\nlution mechanisms. Fig 26 provides a high-level overview of\\ntransformer work. In this section we provide an overview of the\\nmain elements and variants, see [44], [123] for more details.\\nThe Transformer language model architecture, originally\\nproposed for machine translation, consists of an encoder and\\na decoder. The encoder is composed of a stack of N = 6\\nidentical Transformer layers. Each layer has two sub-layers.\\nThe first one is a multi-head self-attention layer, and the other\\none is a simple position-wise fully connected feed-forward\\nnetwork. The decoder is composed of a stack of 6 identical\\nlayers. In addition to the two sub-layers in each encoder layer,\\nthe decoder has a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. The attention\\nfunction can be described as mapping a query and a set of key-\\nvalue pairs to an output, where the query, keys, values, and\\noutput are all vectors. The output is computed as a weighted\\nsum of the values, where the weight assigned to each value\\nis computed by a compatibility function of the query with the\\ncorresponding key. Instead of performing a single attention\\nfunction with dmodel dimensional keys, values and queries,\\nit is found to be beneficial to linearly project the queries,\\nkeys and values hwith different, learned linear projections to\\ndk,dkanddvdimensions, respectively. Positional encoding is\\nincorporated to fuse information about the relative or absolute\\nposition of the tokens in the sequence.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 12}, page_content='Fig. 24: Timeline of some of the most representative LLM frameworks (so far). In addition to large language models with our\\n#parameters threshold, we included a few representative works, which pushed the limits of language models, and paved the way\\nfor their success (e.g. vanilla Transformer, BERT, GPT-1), as well as some small language models. ‚ô£shows entities that serve\\nnot only as models but also as approaches. ‚ô¶shows only approaches.\\n2)Encoder-Only :For this family, at each stage, the atten-\\ntion layers can access all the words in the initial sentence.\\nThe pre-training of these models usually consist of some-\\nhow corrupting a given sentence (for instance, by masking\\nrandom words in it) and tasking the model with finding or\\nreconstructing the initial sentence. Encoder models are great\\nfor tasks requiring an understanding of the full sequence,\\nsuch as sentence classification, named entity recognition, and\\nextractive question answering. One prominent encoder only\\nmodel is BERT (Bidirectional Encoder Representations from\\nTransformers), proposed in [24].\\n3)Decoder-Only :For these models, at each stage, for any\\nword, the attention layers can only access the words positioned\\nbefore that in the sentence. These models are also sometimes\\ncalled auto-regressive models. The pretraining of these models\\nis usually formulated as predicting the next word (or token)\\nin the sequence. The decoder-only models are best suited for\\ntasks involving text generation. GPT models are prominent\\nexample of this model category.\\n4)Encoder-Decoder :These models use both encoder and\\ndecoder, and are sometimes called sequence-to-sequence mod-\\nels. At each stage, the attention layers of the encoder can access\\nall the words in the initial sentence, whereas the attention\\nlayers of the decoder only accesses the words positioned before\\na given word in the input. These models are usually pre-\\ntrained using the objectives of encoder or decoder models, but\\nusually involve something a bit more complex. For instance,\\nsome models are pretrained by replacing random spans of text\\n(that can contain several words) with a single mask special\\nword, and the objective is then to predict the text that thismask word replaces. Encoder-decoder models are best suited\\nfor tasks about generating new sentences conditioned on a\\ngiven input, such as summarization, translation, or generative\\nquestion answering.\\nB. Data Cleaning\\nData quality is crucial to the performance of language\\nmodels trained on them. Data cleaning techniques such as\\nfiltering, deduplication, are shown to have a big impact on\\nthe model performance.\\nAs an example, in Falcon40B [124], Penedo et al. showed\\nthat properly filtered and deduplicated web data alone can lead\\nto powerful models; even significantly outperforming models\\nfrom the state-of-the-art trained on The Pile. Despite extensive\\nfiltering, they were able to obtain five trillion tokens from\\nCommonCrawl. They also released an extract of 600 billion\\ntokens from our REFINEDWEB dataset, and 1.3/7.5B param-\\neters language models trained on it. 27 shows the Refinement\\nprocess of CommonCrawl data by this work.\\n1) Data Filtering: Data filtering aims to enhance the qual-\\nity of training data and the effectiveness of the trained LLMs.\\nCommon data filtering techniques include:\\nRemoving Noise: refers to eliminating irrelevant or noisy\\ndata that might impact the model‚Äôs ability to generalize well.\\nAs an example, one can think of removing false information\\nfrom the training data, to lower the chance of model generating\\nfalse responses. Two mainstream approaches for quality filter-\\ning includes: classifier-based, and heuristic-based frameworks.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 13}, page_content='How LLMs Are Built?\\nData Cleaning\\nTokenizationsBytePairEncoding\\nWordPieceEncoding\\nSentencePieceEncoding\\nPositional EncodingAbsolute Positional Embeddings\\nRelative Positional Embeddings\\nRotary Position Embeddings\\nRelative Positional Bias\\nModel Pre-trainingMasked Language Modeling\\nCausal Language Modeling\\nNext Sentence Prediction\\nMixture of Experts\\nFine-tuning and Instruction T uning\\nAlignmentSupervised learning\\nReinforcement Learning from Human Feedback\\nDirect Preference Optimization\\nKahneman-Tversky Optimization\\nDecoding StrategiesGreedy Search\\nBeam Search\\nTop-k Sampling\\nTop-p Sampling\\nCost-Effective T raining/Inference,\\nAdaptation & CompressionOptimized T raining\\nZero Redundancy Optimizer\\nReceptance W eighted Key V alue\\nLow-Rank Adaption\\nKnowledge Distillation\\nQuantizationData Filtering\\nRemoving Noise\\nHandling Outliers\\nAddressing Imbalances\\nText Preprocessing\\nDeduplication\\nLLM ArchitecturesEncoder-Only\\nDecoder-Only\\nEncoder-Decoder\\n...\\nSupervised Fine-tuning\\nGeneral Fine-tuning\\nMulti- turn I nstructions\\nInstruction FollowingFig. 25: This figure shows different components of LLMs.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 14}, page_content='Fig. 26: High-level overview of transformer work. Courtesy of\\n[44].\\nFig. 27: Subsequent stages of Macrodata Refinement remove\\nnearly 90% of the documents originally in CommonCrawl.\\nCourtesy of [124].\\nHandling Outliers: Identifying and handling outliers or\\nanomalies in the data to prevent them from disproportionately\\ninfluencing the model.\\nAddressing Imbalances: Balancing the distribution of\\nclasses or categories in the dataset to avoid biases and ensure\\nfair representation. This is specially useful for responsible\\nmodel training and evaluation.\\nText Preprocessing: Cleaning and standardizing text data\\nby removing stop words, punctuation, or other elements that\\nmay not contribute significantly to the model‚Äôs learning.\\nDealing with Ambiguities: Resolving or excluding am-\\nbiguous or contradictory data that might confuse the model\\nduring training. This can help the model to provide more\\ndefinite and reliable answers.\\n2) Deduplication: De-duplication refers to the process of\\nremoving duplicate instances or repeated occurrences of the\\nsame data in a dataset. Duplicate data points can introducebiases in the model training process and reduce the diversity, as\\nthe model may learn from the same examples multiple times,\\npotentially leading to overfitting on those particular instances.\\nSome works [125] have shown that de-duplication improves\\nmodels‚Äô ability to generalize to new, unseen data.\\nThe de-duplication process is particularly important when\\ndealing with large datasets, as duplicates can unintentionally\\ninflate the importance of certain patterns or characteristics.\\nThis is especially relevant in NLP tasks, where diverse and\\nrepresentative training data is crucial for building robust lan-\\nguage models.\\nThe specific de-duplication method can vary based on\\nthe nature of the data and the requirements of the particular\\nlanguage model being trained. It may involve comparing entire\\ndata points or specific features to identify and eliminate du-\\nplicates. At the document level, existing works mainly rely on\\nthe overlap ratio of high-level features (e.g. n-grams overlap)\\nbetween documents to detect duplicate samples.\\nC. Tokenizations\\nTokenization referes to the process of converting a se-\\nquence of text into smaller parts, known as tokens. While\\nthe simplest tokenization tool simply chops text into tokens\\nbased on white space, most tokenization tools rely on a word\\ndictionary. However, out-of-vocabulary (OOV) is a problem\\nin this case because the tokenizer only knows words in its\\ndictionary. To increase the coverage of dictionaries, popular\\ntokenizers used for LLMs are based on sub-words, which can\\nbe combined to form a large number of words, including the\\nwords unseen in training data or words in different languages.\\nIn what follows, we describe three popular tokenizers.\\n1)BytePairEncoding : BytePairEncoding is originally a\\ntype of data compression algorithm that uses frequent patterns\\nat byte level to compress the data. By definition, this algorithm\\nmainly tries to keep the frequent words in their original form\\nand break down ones that are not common. This simple\\nparadigm keeps the vocabulary not very large, but also good\\nenough to represent common words at the same time. Also\\nmorphological forms of the frequent words can be represented\\nvery well if suffix or prefix is also commonly presented in the\\ntraining data of the algorithm.\\n2)WordPieceEncoding :This algorithm is mainly used for\\nvery well-known models such as BERT and Electra. At the\\nbeginning of training, the algorithm takes all the alphabet from\\nthe training data to make sure that nothing will be left as UNK\\norunknown from the training dataset. This case happens when\\nthe model is given an input that can not be tokenized by the\\ntokenizer. It mostly happens in cases where some characters are\\nnot tokenizable by it. Similar to BytePairEncoding, it tries to\\nmaximize the likelihood of putting all the tokens in vocabulary\\nbased on their frequency.\\n3)SentencePieceEncoding :Although both tokenizers de-\\nscribed before are strong and have many advantages compared\\nto white-space tokenization, they still take assumption of\\nwords being always separated by white-space as granted. This\\nassumption is not always true, in fact in some languages, words\\ncan be corrupted by many noisy elements such as unwanted\\nspaces or even invented words. SentencePieceEncoding tries\\nto address this issue.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 15}, page_content='D.Positional Encoding\\n1)Absolute Positional Embeddings :(APE) [44] has been\\nused in the original Transformer model to preserve the infor-\\nmation of sequence order. Therefore, the positional information\\nof words is added to the input embeddings at the bottom of\\nboth the encoder and decoder stacks. There are various options\\nfor positional encodings, either learned or fixed. In the vanilla\\nTransformer, sine and cosine functions are employed for this\\npurpose. The main drawback of using APE in Transformers\\nis the restriction to a certain number of tokens. Additionally,\\nAPE fails to account for the relative distances between tokens.\\n2)Relative Positional Embeddings :(RPE) [126] involves\\nextending self-attention to take into account the pairwise links\\nbetween input elements. RPE is added to the model at two\\nlevels: first as an additional component to the keys, and\\nsubsequently as a sub-component of the values matrix. This\\napproach looks at the input as a fully-connected graph with\\nlabels and directed edges. In the case of linear sequences, edges\\ncan capture information about the relative position differences\\nbetween input elements. A clipping distance, represented as k\\n2‚â§k‚â§n‚àí4, specifies the maximum limit on relative lo-\\ncations. This allows the model to make reasonable predictions\\nfor sequence lengths that are not part of the training data.\\n3)Rotary Position Embeddings :Rotary Positional Em-\\nbedding (RoPE) [127] tackles problems with existing ap-\\nproaches. Learned absolute positional encodings can lack gen-\\neralizability and meaningfulness, particularly when sentences\\nare short. Moreover, current methods like T5‚Äôs positional\\nembedding face challenges with constructing a full attention\\nmatrix between positions. RoPE uses a rotation matrix to\\nencode the absolute position of words and simultaneously in-\\ncludes explicit relative position details in self-attention. RoPE\\nbrings useful features like flexibility with sentence lengths, a\\ndecrease in word dependency as relative distances increase,\\nand the ability to improve linear self-attention with relative\\nposition encoding. GPT-NeoX-20B, PaLM, CODEGEN, and\\nLLaMA are among models that take advantage of RoPE in\\ntheir architectures.\\n4)Relative Positional Bias :The concept behind this type\\nof positional embedding is to facilitate extrapolation during\\ninference for sequences longer than those encountered in train-\\ning. In [128] Press et al. proposed Attention with Linear Biases\\n(ALiBi). Instead of simply adding positional embeddings to\\nword embeddings, they introduced a bias to the attention scores\\nof query-key pairs, imposing a penalty proportional to their\\ndistance. In the BLOOM model, ALiBi is leveraged.\\nE. Model Pre-training\\nPre-training is the very first step in large language model\\ntraining pipeline, and it helps LLMs to acquire fundamental\\nlanguage understanding capabilities, which can be useful in a\\nwide range of language related tasks. During pre-training, the\\nLLM is trained on a massive amount of (usually) unlabeled\\ntexts, usually in a self-supervised manner. There are different\\napproaches used for pre-training like next sentence prediction\\n[24], two most common ones include, next token prediction\\n(autoregressive language modeling), and masked language\\nmodeling.InAutoregressive Language Modeling framework, given\\na sequence of ntokens x1, ...,xn, the model tries to predict\\nnext token xn+1(and sometimes next sequence of tokens) in\\nan auto-regressive fashion. One popular loss function in this\\ncase is the log-likelihood of predicted tokens as shown in Eq\\n2\\nLALM(x) =NX\\ni=1p(xi+n|xi, ..., x i+n‚àí1) (1)\\nGiven the auto-regressive nature of this framework, the\\ndecoder-only models are naturally better suited to learn how\\nto accomplish these task.\\nInMasked Language Modeling , some words are masked\\nin a sequence and the model is trained to predict the masked\\nwords based on the surrounding context. Sometimes people\\nrefer to this approach as denoising autoencoding, too. If we\\ndenote the masked/corrupted samples in the sequence x, asÀúx,\\nthen the training objective of this approach can be written as:\\nLMLM (x) =NX\\ni=1p(Àúx|x\\\\Àúx) (2)\\nAnd more recently, Mixture of Experts (MoE) [130],\\n[131] have become very popular in LLM space too. MoEs\\nenable models to be pre-trained with much less compute,\\nwhich means one can dramatically scale up the model or\\ndataset size with the same compute budget as a dense model.\\nMoE consists of two main elements: Sparse MoE layers ,\\nwhich are used instead of dense feed-forward network (FFN)\\nlayers, and have a certain number of ‚Äúexperts‚Äù (e.g. 8), in\\nwhich each expert is a neural network. In practice, the experts\\nare FFNs, but they can also be more complex networks. A gate\\nnetwork or router , that determines which tokens are sent to\\nwhich expert. It is worth noting that, one can send a token\\nto more than one expert. How to route a token to an expert\\nis one of the big decisions when working with MoEs - the\\nrouter is composed of learned parameters and is pretrained at\\nthe same time as the rest of the network. Fig 29 provides an\\nillustration of a Switch Transformer encoder block, which are\\nused in MoE.\\nF . Fine-tuning and Instruction Tuning\\nEarly language models such as BERT trained using self-\\nsupervision as explained in section III-E were not able to\\nperform specific tasks. In order for the foundation model to be\\nuseful it needed to be fine-tuned to a specific task with labeled\\ndata (so-called supervised fine-tuning or SFT for short). For\\nexample, in the original BERT paper [24], the model was fine-\\ntuned to 11 different tasks. While more recent LLMs no longer\\nrequire fine-tuning to be used, they can still benefit from task\\nor data-specific fine-tuning. For example, OpenAI reports that\\nthe much smaller GPT-3.5 Turbo model can outperform GPT-4\\nwhen fine-tuned with task specific data2.\\nFine-tuning does not need to be performed to a single\\ntask though, and there are different approaches to multi-task\\nfine-tuning (see e.g. Mahabi et al. [132]). Fine-tuning to one\\nor more tasks is known to improve results and reduce the\\ncomplexity of prompt engineering, and it can serve as an\\n2https://platform.openai.com/docs/guides/fine-tuning'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 16}, page_content='(a) Absolute Positional Embeddings [129]\\n (b) Relative Positional Embeddings\\n(c) Rotary Positional Embedding [127]\\n (d) Relative Positional Bias [128]\\nFig. 28: Various positional encodings are employed in LLMs.\\nFig. 29: : Illustration of a Switch Transformer encoder block.\\nThey replaced the dense feed forward network (FFN) layer\\npresent in the Transformer with a sparse Switch FFN layer\\n(light blue). . Courtesy of [131].\\nalternative to retrieval augmented generation. Furthermore,\\nthere are other reasons why it might be advisable to fine-tune.\\nFor example, one might want to fine-tune to expose the model\\nto new or proprietary data that it has not been exposed to\\nduring pre-training.\\nAn important reason to fine-tune LLMs is to align the\\nresponses to the expectations humans will have when providing\\ninstructions through prompts. This is the so-called instruction\\ntuning [133]. We dive into the details of how to design\\nand engineer prompts in section IV-B, but in the context\\nof instruction tuning, it is important to understand that the\\ninstruction is a prompt that specifies the task that the LLM\\nshould accomplish. Instruction tuning datasets such as NaturalInstructions [134] include not only the task definition but other\\ncomponents such as positive/negative examples or things to\\navoid.\\nThe specific approach and instruction datasets used to\\ninstruction-tune an LLM varies, but, generally speaking, in-\\nstruction tuned models outperform their original foundation\\nmodels they are based on. For example, InstructGPT [59]\\noutperforms GPT-3 on most benchmarks. The same is true\\nfor Alpaca [62] when compared to LLaMA.\\nSelf-Instruct [135], proposed by Wang et al. is also a\\npopular approach along this line, in which they introduced a\\nframework for improving the instruction-following capabilities\\nof pre-trained language models by bootstrapping their own\\ngenerations. Their pipeline generates instructions, input, and\\noutput samples from a language model, then filters invalid or\\nsimilar ones before using them to fine tune the original model.\\nG. Alignment\\nAI Alignment is the process of steering AI systems towards\\nhuman goals, preferences, and principles. LLMs, pre-trained\\nfor word prediction, often exhibit unintended behaviors. For\\nexample, they might generate contents that are toxic, harmful,\\nmisleading and biased.\\nInstruction tuning, discussed above, gets LLMs a step\\ncloser to being aligned. However, in many cases, it is important\\nto include further steps to improve the alignment of the model\\nand avoid unintended behaviors3. We review the most popular\\n3According to very recent research by Ethayarajh et al. [136], further\\nalignment besides SFT mainly improves models of at least 7B parameters.\\nFor smaller models, SFT is sufficient.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 17}, page_content='approaches to alignment in this subsection.\\nRLHF (reinforcement learning from human feedback) and\\nRLAIF (reinforcement learning from AI feedback) are two\\npopular approaches. RLHF uses a reward model to learn\\nalignment from human feedback. This reward model, after\\nbeing tuned, is able to rate different outputs and score them\\naccording to their alignment preferences given by humans. The\\nreward model gives feedback to the original LLM and this\\nfeedback is used to tune the LLM further [137]. Reinforcement\\nlearning from AI feedback on the other hand, directly connects\\na pretrained and well-aligned model to the LLM and helps it\\nto learn from larger and more aligned models [138].\\nIn another recent work (known as DPO ) [139], Rafailov\\net al. discussed that RLHF is a complex and often unstable\\nprocedure, and tried to address this with a new approach. They\\nleveraged a mapping between reward functions and optimal\\npolicies to show that this constrained reward maximization\\nproblem can be optimized exactly with a single stage of policy\\ntraining, essentially solving a classification problem on the\\nhuman preference data. The resulting algorithm, which they\\ncalled Direct Preference Optimization (DPO), is stable, per-\\nformant, and computationally lightweight, eliminating the need\\nfor fitting a reward model, sampling from the LM during fine-\\ntuning, or performing significant hyperparameter tuning. They\\nobserved that fine-tuning with DPO exceeds RLHF‚Äôs ability to\\ncontrol sentiment of generations and improves response quality\\nin summarization. Fig 30 shows the high-level comparison\\nbetween DPO vs RLHF.\\nFig. 30: DPO optimizes for human preferences while avoiding\\nreinforcement learning. Existing methods for fine-tuning lan-\\nguage models with human feedback first fit a reward model\\nto a dataset of prompts and human preferences over pairs of\\nresponses, and then use RL to find a policy that maximizes\\nthe learned reward. In contrast, DPO directly optimizes for\\nthe policy best satisfying the preferences with a simple classi-\\nfication objective, without an explicit reward function or RL.\\nCourtesy of [139].\\nEven more recently Ethayarajh et al. proposed a new align-\\nment approach called the Kahneman-Tversky Optimization\\n(KTO) [136]. Unlike existing state-of-the-art approaches, KTO\\ndoes not require paired preference data ( x,yw,yl), and it\\nonly needs (x,y) and knowledge of whether yis desirable or\\nundesirable. KTO-aligned models are shown to be good or\\nbetter than DPO-aligned models at scales from 1B to 30B,\\ndespite not using paired preferences. KTO is also far easier to\\nuse in the real world than preference optimization methods, as\\nthe kind of data it needs is far more abundant. As an example,\\nevery retail company has a lot of customer interaction data and\\nwhether that interaction was successful (e.g., purchase made)\\nor unsuccessful (e.g., no purchase made). However, They have\\nlittle to no counterfactual data (i.e., what would have made\\nan unsuccessful customer interaction ylinto a successful oneyw). Fig 31 shows a high-level comparison between KTO and\\nother alignment approaches discussed above.\\nFig. 31: LLM alignment involves supervised finetuning fol-\\nlowed by optimizing a human-centered loss (HALO). How-\\never, the paired preferences that existing approaches need are\\nhard-to-obtain. In contrast, KTO uses a far more abundant\\nkind of data, making it much easier to use in the real world.\\nCourtesy of [136].\\nH. Decoding Strategies\\nDecoding refers to the process of text generation using pre-\\ntrained LLMs. Given an input prompt, the tokenizer translates\\neach token in the input text into a corresponding token ID.\\nThen, the language model uses these token IDs as input and\\npredicts the next most likely token (or a sequence of tokens).\\nFinally, the model generates logits, which are converted to\\nprobabilities using a softmax function. Different decoding\\nstrategies have been proposed. Some of the most popular ones\\nare greedy search, beam search, as well as different sample\\ntechniques such as top-K, top-P (Nucleus sampling).\\n1)Greedy Search :Greedy search takes the most probable\\ntoken at each step as the next token in the sequence, discarding\\nall other potential options. As you can imagine, this is a simple\\napproach and can loose a lot of temporal consistency and\\ncoherency. It only considers the most probable token at each\\nstep, without considering the overall effect on the sequence.\\nThis property makes it fast, but it also means that it can miss\\nout on better sequences that might have appeared with slightly\\nless probable next tokens.\\n2)Beam Search :Unlike greedy search that only considers\\nthe next most probable token, beam search takes into account\\ntheNmost likely tokens, where Ndenotes the number of\\nbeams. This procedure is repeated until a predefined maxi-\\nmum sequence length is reached or an end-of-sequence token\\nappears. At this point, the sequence of tokens (AKA ‚Äúbeam‚Äù)\\nwith the highest overall score is chosen as the output. For\\nexample for beam size of 2 and maximum length of 5,\\nthe beam search needs to keep track of 25= 32 possible\\nsequences. So it is more computationally intensive than greedy\\nsearch.\\n3)Top-k Sampling :Top-k sampling is a technique that\\nuses the probability distribution generated by the language\\nmodel to select a token randomly from the k most likely\\noptions.\\nSuppose we have 6 tokens (A, B, C, D, E, F) and k=2,\\nand P(A)= 30%, and P(B)= 20%, P(C)= P(D)= P(E)= P(F)='),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 18}, page_content='12.5%. In top-k sampling, tokens C, D, E, F are disregarded,\\nand the model outputs A 60% of the time, and B, 40% of\\nthe time. This approach ensures that we prioritize the most\\nprobable tokens while introducing an element of randomness\\nin the selection process.\\nThe randomness is usually introduced via the concept of\\ntemperature. The temperature T is a parameter that ranges from\\n0 to 1, which affects the probabilities generated by the softmax\\nfunction, making the most likely tokens more influential. In\\npractice, it simply consists of dividing the input logits by\\ntemperature value:\\nsoftmax (xi) =exi/T\\nP\\njexj/T(3)\\nA low temperature setting significantly alters the proba-\\nbility distribution (and is commonly used in text generation\\nto control the level of ‚Äúcreativity‚Äù in the generated output),\\nwhile a large temperature prioritizes the tokens with higher\\nprobabilities. Top-k is a creative way of sampling, and can be\\nused along with beam search. The sequence chosen by top-\\nk sampling may not be the sequence with highest probability\\nin beam search. But it‚Äôs important to remember that highest\\nscores do not always lead to more realistic or meaningful\\nsequences.\\n4)Top-p Sampling :Top-p sampling, also known as Nu-\\ncleus sampling, takes a slightly different approach from top-k\\nsampling. Instead of selecting the top k most probable tokens,\\nnucleus sampling chooses a cutoff value p such that the sum of\\nthe probabilities of the selected tokens exceeds p. This forms\\na ‚Äúnucleus‚Äù of tokens from which to randomly choose the next\\ntoken. In other words, in top-p sampling the language model\\nexamines the most probable tokens in descending order and\\nkeeps adding them to the list until the sum of probabilities\\nsurpasses the threshold p. As you can imagine, this could be\\nbetter specially for scenarios in which top-k tokens do not have\\na large probability mass. Unlike top-k sampling, the number\\nof tokens included in the nucleus sampling is not fixed. This\\nvariability often results in a more diverse and creative output,\\nmaking nucleus sampling popular for text generation related\\ntasks.\\nI. Cost-Effective Training/Inference/Adaptation/Compression\\nIn this part, we review some of the popular approaches\\nused for more cost-friendly (and compute-friendly) training\\nand usage of LLMs.\\n1)Optimized Training :There are many frameworks de-\\nveloped for optimized training of LLMs, here we introduce\\nsome of the prominent ones.\\nZeRO: In [140], Rajbhandari et al. developed a novel\\nsolution, Zero Redundancy Optimizer (ZeRO), to optimize\\nmemory, vastly improving training speed of LLMs while\\nincreasing the model size that can be efficiently trained. ZeRO\\neliminates memory redundancies in data- and model-parallel\\ntraining while retaining low communication volume and high\\ncomputational granularity, allowing one to scale the model\\nsize proportional to the number of devices with sustained high\\nefficiency.RWKV: In [141], Peng et al. proposed a novel model\\narchitecture, Receptance Weighted Key Value (RWKV), that\\ncombines the efficient parallelizable training of Transformers\\nwith the efficient inference of RNNs. Their approach leverages\\na linear attention mechanism and allows them to formulate the\\nmodel as either a Transformer or an RNN, which parallelizes\\ncomputations during training and maintains constant compu-\\ntational and memory complexity during inference, leading to\\nthe first non-transformer architecture to be scaled to tens of\\nbillions of parameters. RWKV architecture is shown in Fig\\n32. The Time Complexity comparison of RWKV with different\\nFig. 32: RWKV architecture. Courtesy of [141].\\nTransformers are provided in Fig 33.\\nFig. 33: Time Complexity comparison of RWKV with different\\nTransformers. Here T denotes the sequence length, d the\\nfeature dimension, and c is MEGA‚Äôs chunk size of quadratic\\nattention. Courtesy of [141].\\n2)Low-Rank Adaption (LoRA) :Low-Rank Adaptation is\\na popular and lightweight training technique that significantly\\nreduces the number of trainable parameters, and is based\\non a crucial insight that the difference between the fine-\\ntuned weights for a specialized task and the initial pre-trained\\nweights often exhibits ‚Äúlow intrinsic rank‚Äù - meaning that\\nit can be approximated well by a low rank matrix [142].'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 19}, page_content='Fig. 34: An illustration of LoRA reparametrizan. Only Aand\\nBtrained during this process. Courtesy of [142].\\nTraining with LoRA is much faster, memory-efficient, and\\nproduces smaller model weights (a few hundred MBs), that are\\neasier to store and share. One property of low-rank matrices\\nis that they can be represented as the product of two smaller\\nmatrices. This realization leads to the hypothesis that this delta\\nbetween fine-tuned weights and initial pre-trained weights can\\nbe represented as the matrix product of two much smaller\\nmatrices. By focusing on updating these two smaller matrices\\nrather than the entire original weight matrix, computational\\nefficiency can be substantially improved.\\nSpecifically, for a pre-trained weight matrix W0‚ààRd√ók,\\nLoRA constrains its update by representing the latter with\\na low-rank decomposition W0+ ‚àÜW=W0+BA, where\\nB‚ààRd√ór,A‚ààRr√ók, and the rank r‚â™min(d, k). During\\ntraining, W0is frozen and does not receive gradient updates,\\nwhile AandBcontain trainable parameters. It is worth\\nmentioning that both W0and‚àÜW=BA are multiplied with\\nthe same input, and their respective output vectors are summed\\ncoordinate-wise. For h=W0x, their modified forward pass\\nyields: h=W0x+ ‚àÜWx=W0x+BAx . Usually a random\\nGaussian initialization is used for A, and zero initialization\\nforB, so‚àÜW=BA is zero at the beginning of training.\\nThey then scale ‚àÜWx byŒ±r, where Œ±is a constant in r. This\\nreparametrization is illustrated in Figure 34\\nIt is worth mentioning that LoRA can be applied to any a\\nsubset of weight matrices in a neural network to reduce the\\nnumber of trainable parameters. In the Transformer architec-\\nture, there are four weight matrices in the self-attention module\\n(Wq,Wk,Wv,Wo), and two in the MLP module. Most of\\nthe time, LoRA is focused on adapting the attention weights\\nonly for downstream tasks, and freezes the MLP modules, so\\nthey are not trained in downstream tasks both for simplicity\\nand parameter-efficiency.\\n3)Knowledge Distillation :Knowledge distillation is the\\nprocess of learning from a larger model [143]. Earlier days of\\nbest-performing models release have proven that this approach\\nis very useful even if it is used in an API distillation approach.\\nIt is also referred to as an approach to distill the knowledge of\\nnot a single model but in fact multiple models into a smaller\\none. Creating smaller models by this approach yields smaller\\nmodel sizes that can be used even on edge devices. Knowledge\\ndistillation as shown in Fig 35, illustrates a general setup of\\nthis training scheme.\\nFig. 35: A generic knowledge distillation framework with\\nstudent and teacher (Courtesy of [144]).\\nKnowledge can be transferred by different forms of learn-\\ning: response distillation, feature distillation, and API distilla-\\ntion. Response distillation is concerned only with the outputs\\nof the teacher model and tries to teach the student model\\nhow to exactly or at least similarly perform (in the sense of\\nprediction) as the teacher. Feature distillation not only uses\\nthe last layer but also intermediate layers as well to create a\\nbetter inner representation for the student model. This helps the\\nsmaller model to have a similar representation as the teacher\\nmodel.\\nAPI distillation is the process of using an API (typically\\nfrom an LLM provider such as OpenAI) to train smaller\\nmodels. In the case of LLMs, it is used to train the model\\nfrom the direct output of the larger model which makes it very\\nsimilar to response distillation. Many concerns are raised by\\nthis type of distillation because in cases where the model itself\\nis not openly available, a (usually) paid API is exposed for end\\nusers. On the other hand, while users pay for each call, how to\\nuse the predictions is limited, for example, OpenAI prohibits\\nusage of its API to create LLMs that later will be used to\\ncompete with it. The main value in such case is training data.\\n4)Quantization :deep learning in its core, is a set of\\nmathematical functions applied to matrices, with a specific\\nprecision for model weights. Reducing the precision of the\\nweights can be used to reduce the size of the model and also\\nmake it faster. As an example, Float-32 operations compared\\nto Int-8 operations are slower. This process, which is called\\nquantization, can be applied in different phases. Main ap-\\nproaches for model quantization can be categorized as: post\\ntraining quantization and quantization-aware training. Post-\\ntraining quantization is concerned with quantized trained mod-\\nels in two well-known methods: dynamic and static. Dynamic\\npost-training quantization computes the range of quantization\\non the runtime and is slower compared to static. Quantization-\\naware training adds quantization criteria into training, and\\na quantized model is trained and optimized during training\\nprocess. This approach ensures that the end model will have\\ngood performance and also does not need to be quantized after\\ntraining.\\nIV. H OWLLM SAREUSED AND AUGMENTED\\nOnce the LLMs are trained, we can use them to generate\\ndesired outputs for a variety of tasks. LLMs can be used\\ndirectly through basic prompting. However, in order to exploit\\ntheir full potential or to address some of the shortcomings,'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 20}, page_content='we need to augment the models through some external means.\\nIn this section we first provide a brief overview of the main\\nshortcoming of LLMs, with a deeper look at the issue of\\nhallucination. We then describe how prompting and some aug-\\nmentation approaches can not only address those limitations\\nbut also be used to augment the capabilities of LLMs going\\nas far as turning an LLM into a full-blown AI agent with the\\nability to interface with the external world.\\nA. LLM limitations\\nIt is important to remember that LLMs are trained to predict\\na token. While fine-tuning and alignment improves their per-\\nformance and adds different dimensions to their abilities, there\\nare still some important limitations that come up, particularly\\nif they are used naively. Some of them include the following:\\n‚Ä¢ They don‚Äôt have state/memory. LLMs on their own\\ncannot remember even what was sent to them in the\\nprevious prompt. That is an important limitation for\\nmany of the uses cases that require some form of state.\\n‚Ä¢ They are stochastic/probabilistic. If you send the same\\nprompt to an LLM several times, you are likely to get\\ndifferent responses. While there are parameters, and\\nin particular the temperature, to limit the variability\\nin the response, this is an inherent property of their\\ntraining that can create issues.\\n‚Ä¢ They have stale information and, on their own, don‚Äôt\\nhave access to external data. An LLM on its own does\\nnot even know about the current time or day and does\\nnot have access to any information that was not present\\nin its training set.\\n‚Ä¢ They are generally very large. This means that many\\ncostly GPU machines are needed for training and\\nserving. In some cases, largest models have poor\\nSLAs, particularly in terms of latency.\\n‚Ä¢ They hallucinate. LLMs do not have a notion of\\n‚Äùtruth‚Äù and they have usually been trained on a mix\\nof good and bad content. They can produce very\\nplausible but untruthful answers.\\nWhile the previous limitations can all become important\\nfor some applications, it is worth for us to dive a bit into the\\nlast one, hallucinations, since it has gathered a lot of interest\\nover the past few months and it has also sparked many of the\\nprompt approaches and LLM augmentation methods we will\\nlater describe.\\nHallucination: In the realm of Large Language Models\\n(LLMs), the phenomenon of ‚Äùhallucinations‚Äù has garnered\\nsignificant attention. Defined in the literature, notably in the\\n‚ÄùSurvey of Hallucination in Natural Language Generation‚Äù\\npaper [145], hallucination in an LLM is characterized as\\n‚Äùthe generation of content that is nonsensical or unfaithful\\nto the provided source.‚Äù This terminology, although rooted in\\npsychological parlance, has been appropriated within the field\\nof artificial intelligence.\\nHallucinations in LLMs can be broadly categorized into\\ntwo types:1) Intrinsic Hallucinations : These directly conflict with\\nthe source material, introducing factual inaccuracies\\nor logical inconsistencies.\\n2) Extrinsic Hallucinations : These, while not contra-\\ndicting, are unverifiable against the source, encom-\\npassing speculative or unconfirmable elements.\\nThe definition of ‚Äôsource‚Äô in LLM contexts varies with the\\ntask. In dialogue-based tasks, it refers to ‚Äôworld knowledge‚Äô,\\nwhereas in text summarization, it pertains to the input text\\nitself. This distinction plays a crucial role in evaluating and\\ninterpreting hallucinations. The impact of hallucinations is also\\nhighly context-dependent. For instance, in creative endeavors\\nlike poem writing, hallucinations might be deemed acceptable\\nor even beneficial.\\nLLMs, trained on diverse datasets including the internet,\\nbooks, and Wikipedia, generate text based on probabilistic\\nmodels without an inherent understanding of truth or falsity.\\nRecent advancements like instruct tuning and Reinforcement\\nLearning from Human Feedback (RLHF) have attempted to\\nsteer LLMs towards more factual outputs, but the fundamental\\nprobabilistic nature and its inherent limitations remain. A\\nrecent study, ‚ÄúSources of Hallucination by Large Language\\nModels on Inference Tasks‚Äù [146], highlights two key aspects\\ncontributing to hallucinations in LLMs: the veracity prior and\\nthe relative frequency heuristic, underscoring the complexities\\ninherent in LLM training and output generation.\\nEffective automated measurement of hallucinations in\\nLLMs requires a combination of statistical and model-based\\nmetrics.\\nStatistical Metrics :\\n‚Ä¢ Metrics like ROUGE [147] and BLEU [148] are com-\\nmon for assessing text similarity, focusing on intrinsic\\nhallucinations.\\n‚Ä¢ Advanced metrics such as PARENT [149], PARENT-\\nT [150], and Knowledge F1 [151] are utilized when\\nstructured knowledge sources are available. These\\nmetrics, while effective, have limitations in capturing\\nsyntactic and semantic nuances.\\nModel-Based Metrics :\\n‚Ä¢ IE-Based Metrics : Utilize Information Extraction\\nmodels to simplify knowledge into relational tuples,\\nthen compare these with the source.\\n‚Ä¢ QA-Based Metrics : Assess the overlap between gen-\\nerated content and the source through a question-\\nanswering framework (see [152]).\\n‚Ä¢ NLI-Based Metrics : Use Natural Language Inference\\ndatasets to evaluate the truthfulness of a generated\\nhypothesis based on a given premise (see [153]).\\n‚Ä¢ Faithfulness Classification Metrics : Offer a refined\\nassessment by creating task-specific datasets for a\\nnuanced evaluation (see [154]).\\nDespite advances in automated metrics, human judgment\\nremains a vital piece. It typically involves two methodologies:'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 21}, page_content='B) Augmenting LLMs through\\nexternal knowledge - RAG\\nHow LLMs Are Used and Augmented\\nC) Using External T ools\\nD) LLM AgentsFunctionality of an LLM-based agent\\nTool Access and Utilization\\nDecision MakingPrompt engineering techniques for agents\\nReasoning without Observation\\nReason and Act\\nDialog-Enabled Resolving Agentsa) RAG-aware prompting techniques\\na) Tool-aware prompting techniques\\nA) LLM limitationsHallucination\\nHallucination QuantificationAutomated metrics\\nHuman judgmentStatistical Metrics\\nModel-Based Metrics\\nScoring\\nComparative AnalysisIE-Based Metrics\\nQA-Based Metrics\\nNLI-Based Metrics\\nB) Using LLMs Prompt Design and Engineering\\n1) Chain of Thought\\nZero-Shot CoT\\nManual CoT5) Expert Prompting\\n6) Chains2) Tree of Thought 7) Rails\\nTopical Rails\\nFact-Checking Rails\\nJailbreaking Rails8) Automatic Prompt Engineering\\nPrompt Generation\\nPrompt Scoring\\nRefinement and Iteration3) Self-Consistency\\n4) Reflection\\nComponents of a RAG\\nRetrieval \\nGeneration \\nAugmentationRAG T ools\\nLangChain \\nLlamaIndex\\nHayStackMeltano\\nCohere Coral\\nFlowise AIFig. 36: How LLMs Are Used and Augmented.\\n1) Scoring : Human evaluators rate the level of halluci-\\nnation within a predefined scale.\\n2) Comparative Analysis : Evaluators compare gener-\\nated content against baseline or ground-truth refer-\\nences, adding an essential layer of subjective assess-\\nment.\\nFactScore [155] is a recent example of a metric that can be\\nused both for human and model-based evaluation. The metric\\nbreaks an LLM generation into ‚Äúatomic facts‚Äù. The final score\\nis computed as the sum of the accuracy of each atomic fact,\\ngiving each of them equal weight. Accuracy is a binary number\\nthat simply states whether the atomic fact is supported by the\\nsource. The authors implement different automation strategies\\nthat use LLMs to estimate this metric.\\nFinally, mitigating hallucinations in LLMs is a multifaceted\\nchallenge, requiring tailored strategies to suit various applica-\\ntions. Those include:\\n‚Ä¢ Product Design and User Interaction Strategies such\\nas use case design, structuring the input/output, or\\nproviding mechanisms for user feedback.\\n‚Ä¢ Data Management and Continuous Improvement.Maintaining and analyzing a tracking set of hallucina-\\ntions is essential for ongoing model improvement.\\n‚Ä¢ Prompt Engineering and Metaprompt Design. Many\\nof the advanced prompt techniques described in IV-B\\nsuch as Retrieval Augmented Generation directly ad-\\ndress hallucination risks.\\n‚Ä¢ Model Selection and Configuration for Hallucination\\nMitigation. For exemple, larger models with lower\\ntemperature settings usually perform better. Also,\\ntechniques such as RLHF or domain-sepcific fine-\\ntuning can mitigate hallucination risks.\\nB. Using LLMs: Prompt Design and Engineering\\nA prompt in generative AI models is the textual input\\nprovided by users to guide the model‚Äôs output. This could\\nrange from simple questions to detailed descriptions or specific\\ntasks. Prompts generally consist of instructions, questions,\\ninput data, and examples. In practice, to elicit a desired\\nresponse from an AI model, a prompt must contain either\\ninstructions or questions, with other elements being optional.\\nAdvanced prompts involve more complex structures, such as'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 22}, page_content='‚Äùchain of thought‚Äù prompting, where the model is guided to\\nfollow a logical reasoning process to arrive at an answer.\\nPrompt engineering is a rapidly evolving discipline that\\nshapes the interactions and outputs of LLMs and other gen-\\nerative AI models. The essence of prompt engineering lies in\\ncrafting the optimal prompt to achieve a specific goal with\\na generative model. This process is not only about instructing\\nthe model but also involves some understanding of the model‚Äôs\\ncapabilities and limitations, and the context within which it\\noperates.\\nPrompt engineering transcends the mere construction of\\nprompts; it requires a blend of domain knowledge, understand-\\ning of the AI model, and a methodical approach to tailor\\nprompts for different contexts. This might involve creating\\ntemplates that can be programmatically modified based on a\\ngiven dataset or context. For example, generating personalized\\nresponses based on user data might use a template that is\\ndynamically filled with relevant user information.\\nFurthermore, prompt engineering is an iterative and ex-\\nploratory process, akin to traditional machine learning prac-\\ntices such as model evaluation or hyperparameter tuning. The\\nrapid growth of this field suggests its potential to revolutionize\\ncertain aspects of machine learning, moving beyond traditional\\nmethods like feature or architecture engineering. On the other\\nhand, traditional engineering practices such as version con-\\ntrol and regression testing need to be adapted to this new\\nparadigm just like they were adapted to other machine learning\\napproaches [156].\\nIn the following paragraphs we detail some of the most\\ninteresting and popular prompt engineering approaches.\\n1) Chain of Thought (CoT): The Chain of Thought (CoT)\\ntechnique, initially described in the paper ‚ÄúChain-of-Thought\\nPrompting Elicits Reasoning in Large Language Models‚Äù[34]\\nby Google researchers, represents a pivotal advancement in\\nprompt engineering for Large Language Models (LLMs).\\nThis approach hinges on the understanding that LLMs, while\\nproficient in token prediction, are not inherently designed for\\nexplicit reasoning. CoT addresses this by guiding the model\\nthrough essential reasoning steps.\\nCoT is based on making the implicit reasoning process of\\nLLMs explicit. By outlining the steps required for reasoning,\\nthe model is directed closer to a logical and reasoned output,\\nespecially in scenarios demanding more than simple informa-\\ntion retrieval or pattern recognition.\\nCoT prompting manifests in two primary forms:\\n1) Zero-Shot CoT: This form involves instructing the\\nLLM to ‚Äúthink step by step‚Äù, prompting it to de-\\nconstruct the problem and articulate each stage of\\nreasoning.\\n2) Manual CoT: A more complex variant, it requires\\nproviding step-by-step reasoning examples as tem-\\nplates for the model. While yielding more effective\\nresults, it poses challenges in scalability and mainte-\\nnance.\\nManual CoT is more effective than zero-shot. However,\\nthe effectiveness of this example-based CoT depends on the\\nchoice of diverse examples, and constructing prompts withsuch examples of step by step reasoning by hand is hard and\\nerror prone. That is where automatic CoT [157] comes into\\nplay.\\n2) Tree of Thought (ToT): The Tree of Thought (ToT)\\n[158] prompting technique is inspired by the concept of\\nconsidering various alternative solutions or thought processes\\nbefore converging on the most plausible one. ToT is based\\non the idea of branching out into multiple ‚Äùthought trees‚Äù\\nwhere each branch represents a different line of reasoning.\\nThis method allows the LLM to explore various possibilities\\nand hypotheses, much like human cognitive processes where\\nmultiple scenarios are considered before determining the most\\nlikely one.\\nA critical aspect of ToT is the evaluation of these reasoning\\npaths. As the LLM generates different branches of thought,\\neach is assessed for its validity and relevance to the query.\\nThis process involves real-time analysis and comparison of\\nthe branches, leading to a selection of the most coherent and\\nlogical outcome.\\nToT is particularly useful in complex problem-solving\\nscenarios where a single line of reasoning might not suffice.\\nIt allows LLMs to mimic a more human-like problem-solving\\napproach, considering a range of possibilities before arriving\\nat a conclusion. This technique enhances the model‚Äôs ability\\nto handle ambiguity, complexity, and nuanced tasks, making it\\na valuable tool in advanced AI applications.\\n3) Self-Consistency: Self-Consistency [159] utilizes an\\nensemble-based method, where the LLM is prompted to gen-\\nerate multiple responses to the same query. The consistency\\namong these responses serves as an indicator of their accuracy\\nand reliability.\\nThe Self-Consistency approach is grounded in the principle\\nthat if an LLM generates multiple, similar responses to the\\nsame prompt, it is more likely that the response is accurate.\\nThis method involves asking the LLM to tackle a query mul-\\ntiple times, each time analyzing the response for consistency.\\nThis technique is especially useful in scenarios where factual\\naccuracy and precision are paramount.\\nThe consistency of responses can be measured using vari-\\nous methods. One common approach is to analyze the overlap\\nin the content of the responses. Other methods may include\\ncomparing the semantic similarity of responses or employing\\nmore sophisticated techniques like BERT-scores or n-gram\\noverlaps. These measures help in quantifying the level of\\nagreement among the responses generated by the LLM.\\nSelf-Consistency has significant applications in fields\\nwhere the veracity of information is critical. It is particularly\\nrelevant in scenarios like fact-checking, where ensuring the\\naccuracy of information provided by AI models is essential.\\nBy employing this technique, prompt engineers can enhance\\nthe trustworthiness of LLMs, making them more reliable for\\ntasks that require high levels of factual accuracy.\\n4) Reflection: Reflection [160] involves prompting LLMs\\nto assess and potentially revise their own outputs based on\\nreasoning about the correctness and coherence of their re-\\nsponses. The concept of Reflection centers on the ability of\\nLLMs to engage in a form of self-evaluation. After generating'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 23}, page_content='an initial response, the model is prompted to reflect on its\\nown output, considering factors like factual accuracy, logical\\nconsistency, and relevance. This introspective process can lead\\nto the generation of revised or improved responses.\\nA key aspect of Reflection is the LLM‚Äôs capacity for\\nself-editing. By evaluating its initial response, the model can\\nidentify potential errors or areas of improvement. This iterative\\nprocess of generation, reflection, and revision enables the LLM\\nto refine its output, enhancing the overall quality and reliability\\nof its responses.\\n5) Expert Prompting: Expert Prompting [161] enhances the\\ncapabilities of Large Language Models (LLMs) by simulating\\nthe responses of experts in various fields. This method involves\\nprompting the LLMs to assume the role of an expert and re-\\nspond accordingly, providing high-quality, informed answers.\\nA key strategy within Expert Prompting is the multi-expert\\napproach. The LLM is prompted to consider responses from\\nmultiple expert perspectives, which are then synthesized to\\nform a comprehensive and well-rounded answer. This tech-\\nnique not only enhances the depth of the response but also\\nincorporates a range of viewpoints, reflecting a more holistic\\nunderstanding of the subject matter.\\n6) Chains: Chains refer to the method of linking multiple\\ncomponents in a sequence to handle complex tasks with Large\\nLanguage Models (LLMs). This approach involves creating a\\nseries of interconnected steps or processes, each contributing\\nto the final outcome. The concept of Chains is based on\\nthe idea of constructing a workflow where different stages\\nor components are sequentially arranged. Each component in\\na Chain performs a specific function, and the output of one\\nserves as the input for the next. This end-to-end arrangement\\nallows for more complex and nuanced processing, as each\\nstage can be tailored to handle a specific aspect of the task.\\nChains can vary in complexity and structure, depending on\\nthe requirements. In ‚ÄúPromptChainer: Chaining Large Lan-\\nguage Model Prompts through Visual Programming‚Äù [162],\\nthe authors not only describe the main challenges in designing\\nchains, but also describe a visual tool to support those tasks.\\n7) Rails: Rails in advanced prompt engineering refer to\\na method of guiding and controlling the output of Large\\nLanguage Models (LLMs) through predefined rules or tem-\\nplates. This approach is designed to ensure that the model‚Äôs\\nresponses adhere to certain standards or criteria, enhancing the\\nrelevance, safety, and accuracy of the output. The concept of\\nRails involves setting up a framework or a set of guidelines\\nthat the LLM must follow while generating responses. These\\nguidelines are typically defined using a modeling language or\\ntemplates known as Canonical Forms, which standardize the\\nway natural language sentences are structured and delivered.\\nRails can be designed for various purposes, depending on\\nthe specific needs of the application:\\n‚Ä¢ Topical Rails: Ensure that the LLM sticks to a\\nparticular topic or domain.\\n‚Ä¢ Fact-Checking Rails: Aimed at minimizing the gen-\\neration of false or misleading information.\\n‚Ä¢ Jailbreaking Rails: Prevent the LLM from generating\\nresponses that attempt to bypass its own operational\\nconstraints or guidelines.8) Automatic Prompt Engineering (APE): Automatic\\nPrompt Engineering (APE) [163] focuses on automating the\\nprocess of prompt creation for Large Language Models\\n(LLMs). APE seeks to streamline and optimize the prompt\\ndesign process, leveraging the capabilities of LLMs themselves\\nto generate and evaluate prompts. APE involves using LLMs\\nin a self-referential manner where the model is employed\\nto generate, score, and refine prompts. This recursive use of\\nLLMs enables the creation of high-quality prompts that are\\nmore likely to elicit the desired response or outcome.\\nThe methodology of APE can be broken down into several\\nkey steps:\\n‚Ä¢ Prompt Generation: The LLM generates a range of\\npotential prompts based on a given task or objective.\\n‚Ä¢ Prompt Scoring: Each generated prompt is then\\nevaluated for its effectiveness, often using criteria\\nlike clarity, specificity, and likelihood of eliciting the\\ndesired response.\\n‚Ä¢ Refinement and Iteration: Based on these evalua-\\ntions, prompts can be refined and iterated upon, further\\nenhancing their quality and effectiveness.\\nC.Augmenting LLMs through external knowledge - RAG\\nOne of the main limitations of pre-trained LLMs is their\\nlack of up-to-date knowledge or access to private or use-\\ncase-specific information. This is where retrieval augmented\\ngeneration (RAG) comes into the picture [164]. RAG, illus-\\ntrated in figure 37, involves extracting a query from the input\\nprompt and using that query to retrieve relevant information\\nfrom an external knowledge source (e.g. a search engine or a\\nknowledge graph, see figure 38 ). The relevant information is\\nthen added to the original prompt and fed to the LLM in order\\nfor the model to generate the final response. A RAG system\\nincludes three important components: Retrieval, Generation,\\nAugmentation [165].\\na) RAG-aware prompting techniques: Because of the\\nimportance of RAG to build advanced LLM systems, several\\nRAG-aware prompting techniques have been developed re-\\ncently. One such technique is Forward-looking Active Retrieval\\nAugmented Generation (FLARE)\\nForward-looking Active Retrieval Augmented Generation\\n(FLARE) [168] enhances the capabilities of Large Language\\nModels (LLMs) by iteratively combining prediction and in-\\nformation retrieval. FLARE represents an evolution in the\\nuse of retrieval-augmented generation, aimed at improving the\\naccuracy and relevance of LLM responses.\\nFLARE involves an iterative process where the LLM\\nactively predicts upcoming content and uses these predictions\\nas queries to retrieve relevant information. This method con-\\ntrasts with traditional retrieval-augmented models that typically\\nretrieve information once and then proceed with generation. In\\nFLARE, this process is dynamic and ongoing throughout the\\ngeneration phase. In FLARE, each sentence or segment gener-\\nated by the LLM is evaluated for confidence. If the confidence\\nlevel is below a certain threshold, the model uses the generated\\ncontent as a query to retrieve relevant information, which is\\nthen used to regenerate or refine the sentence. This iterative'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 24}, page_content='Fig. 37: An example of synthesizing RAG with LLMs for question answering application [166].\\nFig. 38: This is one example of synthesizing the KG as a\\nretriever with LLMs [167].\\nprocess ensures that each part of the response is informed by\\nthe most relevant and current information available.\\nFor more details on RAG framework and its relevant works,\\nwe refer the readers to this survey of retrieval augmented\\ngenerations [165].\\nD.Using External Tools\\nRetrieving information from an external knowledge source\\nas described above is only one of the potential ways to augment\\nan LLM. More generally, an LLM can access any number\\nof external tools (e.g. an API to a service) to augment its\\nfunctionality. In that regards, RAG can be seen as a specific\\ninstance of the broader category of the so called ‚Äùtools‚Äù.\\nTools in this context are external functions or services that\\nLLMs can utilize. These tools extend the range of tasks an\\nLLM can perform, from basic information retrieval to complex\\ninteractions with external databases or APIs.\\nIn the paper ‚ÄùToolformer: Language Models Can Teach\\nThemselves to Use Tools‚Äù [169], the authors go beyond simple\\ntool usage by training an LLM to decide what tool to use\\nwhen, and even what parameters the API needs. Tools include\\ntwo different search engines, or a calculator. In the followingexamples, the LLM decides to call an external Q&A tool,\\na calculator, and a Wikipedia Search Engine More recently,\\nresearchers at Berkeley have trained a new LLM called Gorilla\\n[67] that beats GPT-4 at the use of APIs, a specific but quite\\ngeneral tool.\\na) Tool-aware prompting techniques: Similarly to what\\nwas described with RAG, several tool-aware prompting ap-\\nproaches have been developed to make usage of tools more\\nscalable. A popular technique is the so called Automatic Multi-\\nstep Reasoning and Tool-use (ART).\\nAutomatic Multi-step Reasoning and Tool-use (ART) [170]\\nis a prompt engineering technique that combines automated\\nchain of thought prompting with the use of external tools.\\nART represents a convergence of multiple prompt engineering\\nstrategies, enhancing the ability of Large Language Models\\n(LLMs) to handle complex tasks that require both reasoning\\nand interaction with external data sources or tools.\\nART involves a systematic approach where, given a task\\nand input, the system first identifies similar tasks from a task\\nlibrary. These tasks are then used as examples in the prompt,\\nguiding the LLM on how to approach and execute the current\\ntask. This method is particularly effective when tasks require a\\ncombination of internal reasoning and external data processing\\nor retrieval.\\nE.LLM Agents\\nThe idea of AI agents has been well-explored in the history\\nof AI. An agent is typically an autonomous entity that can\\nperceive the environment using its sensors, make a judgment\\nbased on the state it currently is, and accordingly act based on\\nthe actions that are available to it.\\nIn the context of LLMs, an agent refers to a system based\\non a specialized instantiation of an (augmented) LLM that\\nis capable of performing specific tasks autonomously. These\\nagents are designed to interact with users and environment to\\nmake decisions based on the input and the intended goal of\\nthe interaction. Agents are based on LLMs equipped with the'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 25}, page_content='ability to access and use tools, and to make decisions based on\\nthe given input. They are designed to handle tasks that require\\na degree of autonomy and decision-making, typically beyond\\nsimple response generation.\\nThe functionalities of a generic LLM-based agent include:\\n‚Ä¢ Tool Access and Utilization: Agents have the capabil-\\nity to access external tools and services, and to utilize\\nthese resources effectively to accomplish tasks.\\n‚Ä¢ Decision Making: They can make decisions based on\\nthe input, context, and the tools available to them,\\noften employing complex reasoning processes.\\nAs an example, an LLM that has access to a function (or\\nan API) such as weather API, can answer any question related\\nto the weather of the specific place. In other words, it can use\\nAPIs to solve problems. Furthermore, if that LLM has access\\nto an API that allows to make purchases, a purchasing agent\\ncan be built to not only have capabilities to read information\\nfrom the external world, but also act on it [171].\\nFig. 40 shows another example of LLM-based agents for\\nconversational information seeking [36], where an LLM is\\naugmented with a set of plug-and-play modules, including\\naworking memory that tracks the dialog state, a policy that\\nmakes an execution plan for the task and selects next system\\naction, an action executor that performs an action selected by\\nthe policy (consolidating evidence from external knowledge,\\nor prompting the LLM to generate responses), and a utility\\nthat accesses the alignment of the LLM‚Äôs responses with user\\nexpectations or specific business requirements, and generate\\nfeedback to improve agent performance.\\nFor more details on LLM-based AI agents see recent survey\\n[172], [173], [174].\\na) Prompt engineering techniques for agents: Like\\nRAG and Tools, prompt engineering techniques that specif-\\nically address the needs of LLM-based agents have been\\ndeveloped. Three such examples are Reasoning without Ob-\\nservation (ReWOO), Reason and Act (ReAct), and Dialog-\\nEnabled Resolving Agents (DERA).\\nReasoning without Observation (ReWOO) [175] aims to\\ndecouple reasoning from direct observations. ReWOO operates\\nby enabling LLMs to formulate comprehensive reasoning plans\\nor meta-plans without immediate reliance on external data\\nor tools. This approach allows the agent to create a struc-\\ntured framework for reasoning that can be executed once the\\nnecessary data or observations are available. In ReWOO, the\\nLLM initially develops a plan (a series of steps) that outlines\\nhow to approach and solve a given problem. This meta-\\nplanning phase is crucial as it sets the stage for the agent to\\nprocess information once it becomes available. The execution\\nphase then involves integrating actual data or observations into\\nthe pre-specified plan, leading to coherent and contextually\\nrelevant responses. ReWOO offers significant advantages in\\nterms of token efficiency and robustness to tool failure. It\\nenables LLMs to handle tasks where immediate access to\\nexternal data is not available, relying instead on a well-\\nstructured reasoning framework. This method is particularly\\nadvantageous in scenarios where data retrieval is costly, slow,or uncertain, allowing the LLM-based agent to maintain a high\\nlevel of performance and reliability.\\nReason and Act (ReAct)[176] prompts LLMs to generate\\nnot only verbal reasoning but also actionable steps, thus\\nenhancing the model‚Äôs dynamic problem-solving capabilities.\\nReAct is grounded in the principle of integrating reasoning\\nwith action. In this approach, the LLM is prompted to alternate\\nbetween generating reasoning traces (explanations) and taking\\nactions (steps or commands) in an interleaved manner. This\\napproach allows the model to dynamically reason about a prob-\\nlem, and propose and take concrete actions simultaneously.\\nDialog-Enabled Resolving Agents (DERA) [177] are spe-\\ncialized AI agents that can engage in dialogue, resolve queries,\\nand make decisions based on interactive exchanges. DERA\\nis developed based on the idea of utilizing multiple agents\\nwithin a dialog context, each with specific roles and functions.\\nThese agents can include Researchers, who gather and analyze\\ninformation, and Deciders, who make final judgments based\\non the information provided. This division of roles allows for\\na well-organized and efficient approach to problem-solving\\nand decision-making. DERA is particularly advantageous in\\nscenarios requiring complex decision-making and problem-\\nsolving, such as those in medical diagnostics or customer ser-\\nvice. The collaborative and interactive nature of DERA agents\\nallows them to handle intricate queries with a level of depth\\nand nuance that single-agent systems might struggle with.\\nMoreover, this approach aligns well with human decision-\\nmaking processes, making AI reasoning more relatable and\\ntrustworthy.\\nV. P OPULAR DATASETS FOR LLM S\\nLarge language models exhibit promising accomplish-\\nments, but the main question that arises is how effectively\\nthey function and how their performance can be assessed in\\nspecific tasks or applications.\\nThe evaluation of LLMs poses particular challenges due\\nto the evolving landscape of their applications. The original\\nintent behind developing LLMs was to boost the performance\\nof NLP tasks such as translation, summarization, question-\\nanswering, and so on [178]. However, it is evident today\\nthat these models are finding utility across diverse domains\\nincluding code generation and finance. Moreover, the eval-\\nuation of LLMs encompasses several critical considerations\\nsuch as fairness and bias, fact-checking, and reasoning. In\\nthis section, we outline the commonly used benchmarks for\\nassessing LLMs. These benchmarks are categorized based on\\ntraining or evaluating the LLM Capabilities.\\nA. Datasets for Basic Tasks: language model-\\ning/understanding/generation\\nThis section provides an overview of the benchmarks and\\ndatasets suited to evaluate the basic abilities of LLMs.\\n‚Ä¢ Natural Questions [179] is a QA dataset that consists\\nof real anonymized, aggregated queries submitted to\\nthe Google search engine as questions. An annotator\\nis presented with a question along with a Wikipedia\\npage from the top 5search results, and annotates a\\nlong answer (typically a paragraph) and a short answer'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 26}, page_content='Fig. 39: HuggingGPT: An agent-based approach to use tools and planning [image courtesy of [171]]\\nFig. 40: A LLM-based agent for conversational information\\nseeking. Courtesy of [36].\\n(one or more entities) if present on the page, or marks\\nnull if no long/short answer is present.\\n‚Ä¢ MMLU [180] is intended to evaluate the knowl-\\nedge gained in zero-shot and few-shot scenarios. That\\nmeans that MMLU assesses both the general knowl-\\nedge and problem-solving ability of a model. It covers\\n57 subjects in STEM, humanities, social sciences,\\nand other areas. The benchmark varies in complexity,\\nranging from elementary to advanced professional.\\nIt is worth mentioning that the main contribution of\\nthis dataset is for multi-task language understanding,\\nquestion answering, and arithmetic reasoning.\\n‚Ä¢ MBPP [181] stands for ‚ÄúMostly Basic Python Prob-\\nlems‚Äù and provides a benchmark for evaluating the\\nperformance of models designed for code generation.\\nThe benchmark encompasses 974 short Python pro-\\ngrams including a wide range of topics, including\\nfundamental programming concepts and standard li-\\nbrary usage, and more. Each challenge comprises atask description, a code solution, and three automated\\ntest cases.\\n‚Ä¢ HumanEval [182] is a dataset for code generation\\ntask. This dataset consists of 164 hand-crafted pro-\\ngramming challenges. Each challenge is accompanied\\nby a function signature, docstring, code body, and mul-\\ntiple unit tests. The main intuition behind developing\\nthis dataset is to guarantee the exclusion of its contents\\nfrom training datasets for code generation models.\\n‚Ä¢ APPS [183] is designed for code generation task\\nfocusing on the Python programming language. The\\nAPPS dataset contains a collection of 232,444Python\\nprograms. Each program in the dataset has an average\\nof18lines of Python code. Additionally, APPS offers\\naccess to a repository of 10,000unique programming\\nexercises, each with text-based problem descriptions.\\nThe final aspect to highlight is that the it includes test\\ncases.\\n‚Ä¢ WikiSQL [184] is crafted for code generation task and\\nit has 87,726 carefully labeled pairs of SQL queries\\nand corresponding natural language questions from\\nWikipedia tables. The SQL queries comprise three\\nsubsets: test sets ( 17,284 examples), development\\n(9,145examples), and training ( 61,297examples).\\n‚Ä¢ TriviaQA [185] is designed for QA task. This\\ndataset comprises more than 650,000 question-\\nanswer-evidence triples. There are 95,000 question-\\nanswer pairs in this dataset, each authored by trivia en-\\nthusiasts and supported by an average of six indepen-\\ndently sourced evidence documents. These documents\\nare automatically acquired from Wikipedia or broader\\nweb search results. The dataset is categorized into\\ntwo segments, including those with authentic answers\\nfrom Wikipedia and web domains, and verified sets\\nembody the accurately answered questions along with\\ntheir associated documents from both Wikipedia and\\nonline.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 27}, page_content='Fig. 41: Dataset applications.\\n‚Ä¢ RACE [186] suits for reading comprehension task.\\nThis dataset is based on English tests completed by\\nChinese students from middle school and high school,\\naged 12to18, and it contains roughly 28,000 texts\\nand100,000questions rigorously prepared by human\\nspecialists, primarily English instructors. This dataset\\ncontains a wide range of subjects that were purpose-\\nfully chosen to assess students‚Äô comprehension and\\nreasoning abilities. This dataset is available in three\\nsubgroups: RACE-M, RACE-H, and RACE. RACE-\\nM refers to the middle school examinations, whereas\\nRACE-H denotes the high school tests. Finally, RACEis the synthesis of RACE-M and RACE-H.\\n‚Ä¢ SQuAD [187] stands for ‚ÄúStanford Question Answer-\\ning Dataset‚Äù and is a crowdsourced reading compre-\\nhension dataset based on Wikipedia articles. It has\\napproximately 100,000 question-answer pairs con-\\nnected to more than 500 articles. The answers to\\nthese questions are typically text fragments or spans\\ntaken from the corresponding reading passages. The\\nquestions may be unanswerable in some cases. The\\ndataset is divided into three sets: an 80% training set,\\na10% development set, and a 10% hidden test set.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 28}, page_content='Fig. 42: Datasets licensed under different licenses.\\n‚Ä¢ BoolQ [188] is a yes/no question-answering dataset\\nwhere the goal is reading comprehension task. BoolQ\\nincludes 15,942 examples. Each example is a triplet\\nthat includes a question, a relevant paragraph, and\\nthe solution. Although the main intuition behind\\nthis dataset is for reading comprehension, it can be\\nused for reasoning, natural language inference, and\\nquestion-answering tasks.\\n‚Ä¢ MultiRC [189] is another dataset that fits reading\\ncomprehension task. MultiRC contains brief para-\\ngraphs as well as multi-sentence questions that can\\nbe answered using the information in the paragraph.\\nThe paragraphs in this dataset come from a variety\\nof sources, including news, fiction, historical texts,\\nWikipedia articles, discussions on society and law,\\nelementary school science textbooks, and 9/11 re-\\nports. Each question has many response choices, with\\none or more of them being correct. Answering the\\nquestions requires reasoning across several sentences.\\nMultiRC dataset encompasses around 6,000 multi-\\nsentence questions gathered from over 800 paragraphs.\\nOn average, each question offers about two valid\\nanswer alternatives out of a total of five.\\nB. Datasets for Emergent: ICL, reasoning (CoT), instruction\\nfollowing\\nThis section centers on the benchmarks and datasets em-\\nployed to evaluate the emergent abilities of LLMs.‚Ä¢ GSM8K [190] is designed to evaluate the model‚Äôs\\nability for multi-step mathematical reasoning. GSM8K\\nincludes 8.5K linguistically diverse grade school math\\nword problems written by humans. The dataset is split\\ninto two sets: a training set with 7.5Kproblems,\\nand a test set with 1K problems. These problems\\nneed 2to8steps to be solved. Solutions mainly\\nare a series of elementary calculations using basic\\narithmetic operations.\\n‚Ä¢ MATH [191] enables to assess how well models can\\nsolve math problems. MATH dataset hast 12,500\\nproblems from high school math competitions. Each\\nproblem in the dataset has a step-by-step solution and\\na final answer enclosed in a box. The problems cover\\na wide range of topics and have different levels of\\ncomplexity. There are seven subjects in total. Further-\\nmore, the difficulty of each problem is rated based\\non the AoPS standards on a scale from‚Ä≤1‚Ä≤to‚Ä≤5‚Ä≤. A\\n‚Ä≤1‚Ä≤shows the easiest problems in a subject, while‚Ä≤5‚Ä≤\\nrepresents the most difficult. In terms of formatting,\\nall problems and solutions are presented using LATEX\\nand the Asymptote vector graphics language.\\n‚Ä¢ HellaSwag [192] is designed to assess commonsense\\nreasoning in LLMs. This benchmark includes 70,000\\nmultiple-choice questions. Each question is derived\\nfrom one of two domains: ActivityNet or WikiHow,\\nand presents four answer choices regarding what\\nmight happen in the following situation. The correct\\nanswer provides an actual statement describing the'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 29}, page_content='upcoming event, but the three wrong answers are\\ncreated to confuse machines.\\n‚Ä¢ AI2 Reasoning Challenge (ARC) [193] is used\\nfor commonsense reasoning. This benchmark encom-\\npasses 7,787 science examination questions. These\\nquestions are in English, and most of them are set\\nup in a multiple-choice format. The questions have\\nbeen divided into two groups: a Challenge Set with\\n2,590difficult questions and an Easy Set with 5,197\\nquestions. Each collection has also been pre-divided\\ninto Train, Development, and Test subsets.\\n‚Ä¢ PIQA [194] is intended to evaluate the language\\nrepresentations on their knowledge of physical com-\\nmonsense. In this dataset, the focus is on everyday\\nsituations with a preference for uncommon solutions.\\nThe central task is a multiple-choice question answer-\\ning, where a question (q)is provided along with two\\npotential solutions (s1, s2). Then, the best solution is\\nchosen by whether a model or a human. For each\\nquestion, only one of the solutions is the correct\\nanswer.\\n‚Ä¢ SIQA [195] provides a framework for evaluating mod-\\nels‚Äô ability for commonsense reasoning about social\\nsituations. SIQA dataset has 38,000 multiple-choice\\nquestions designed to assess emotional and social\\nintelligence in everyday circumstances. This dataset\\ncovers a wide variety of social scenarios. In SIQA,\\nthe potential answers is a mixture of human-selected\\nresponses and machine-generated ones that have been\\nfiltered through adversarial processes.\\n‚Ä¢ OpenBookQA (OBQA) [196] is a new kind of\\nquestion-answering dataset where answering its ques-\\ntions requires additional common and commonsense\\nknowledge not contained in the book and rich text\\ncomprehension. This dataset includes around 6,000\\nmultiple-choice questions. Each question is linked to\\none core fact, as well as an additional collection\\nof over 6000 facts. The questions were developed\\nusing a multi-stage crowdsourcing and expert filter-\\ning procedure. OpenBookQA questions are difficult\\nbecause they need multi-hop reasoning with limited\\nbackground.\\n‚Ä¢ TruthfulQA [197] is designed specifically to eval-\\nuate the truthfulness of language models in gen-\\nerating answers to questions. This dataset includes\\n817 questions, written by authors, from 38different\\ncategories, including health, law, finance, and politics.\\nThese questions are purposefully designed to chal-\\nlenge human responders, as they may contain common\\nmisunderstandings that lead to incorrect answers.\\n‚Ä¢ OPT-IML Bench [103] is a comprehensive bench-\\nmark for Instruction Meta-Learning. It covers 2000\\nNLP tasks from 8 existing benchmarks. The OPT-IML\\nBench consists of a training set with 17.9 M examples,\\na dev set with 145K samples, and a test set with 321K\\nsamples.C. Datasets for Augmented: using external knowledge/tools\\nThis section focuses on datasets designed for the aug-\\nmented abilities of LLMs.\\n‚Ä¢ HotpotQA [198] is designed to cover a diverse and\\nexplainable question-answering dataset that necessi-\\ntates multi-hop reasoning. This dataset is derived from\\nthe English Wikipedia. It consists of roughly 113,000\\nquestions. Each question in the dataset comes with\\ntwo paragraphs, called gold paragraphs, from two\\nWikipedia articles. Also, there is a list of sentences\\nin those paragraphs that crowdworkers have picked as\\nimportant for answering the question.\\n‚Ä¢ ToolQA [199] is a question answering benchmark\\nto evaluate LLMs‚Äô ability to use external tools for\\nanswering questions.\\n‚Ä¢ GPT4Tools serves as an instructional dataset, gener-\\nated by instructing advanced teachers (such as Chat-\\nGPT), with instructions conditioned on visual content\\nand tool descriptions. This process results in the\\ngeneration of instructions related to the use of tools.\\nThere are three versions of this dataset. The first\\nversion comprises 71,000 instruction-following data\\npoints utilized to fine-tune the GPT4Tools model. The\\nnext version consists of manually cleaned instruction\\ndata used for validation, covering instructions related\\nto the tools from the first version. The last version is\\ncleaned instruction data used for testing and includes\\ninstructions related to some tools that are not present\\nin the first version.\\nVI. P ROMINENT LLM S‚Äô PERFORMANCE ON\\nBENCHMARKS\\nIn this section we first provide an overview of some of\\npopular metrics used for evaluating the performance of LLMs\\nunder different scenarios. We then look at the performance\\nof prominent large language models on some of the popular\\ndatasets and benchmarks.\\nA. Popular Metrics for Evaluating LLMs\\nEvaluating the performance of generative language models\\ndepends on the underlying task they are going to be used for.\\nTasks that are mostly about selecting a choice out of given\\nones (such as sentiment analysis), can be seen as simple as\\nclassification and their performance can be evaluated using\\nclassification metrics. Metrics such as accuracy, precision,\\nrecall, F1, etc are applicable in this case. It is also important to\\nnote that the answers generated by the model for specific tasks\\nsuch as multi-choice question answering are always either True\\nor False. If the answer is not in a set of options, it can be seen\\nas False as well.\\nHowever, some tasks that are purely open-ended text gener-\\nation cannot be evaluated in the same way as for categorization.\\nDifferent metrics are required for the specific purpose of the\\nevaluation. Code generation is a very different case in open-\\nended generative evaluations. The generated code must pass\\nthe test suite but on the other hand, it is also important\\nto understand if a model is capable of generating different'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 30}, page_content='TABLE II: LLM Datasets Overview.\\nBenchmark Name Evaluation Metric Leaderboard Source paperswithcode\\nHumanEval PASS@k Link Link Link\\nMBPP PASS@k, Accuracy - Link Link\\nAPPS PASS@k, Accuracy - Link Link\\nWikiSQL Accuracy - Link Link\\nCoNaLa BLEU Link Link\\nCodeParrot PASS@k - Link -\\nHellaSwag Accuracy Link Link Link\\nAI2 Reasoning\\nChallenge (ARC)Accuracy Link Link Link\\nBoolQ Accuracy - Link Link\\nMultiRC F1-score, Accuracy - Link Link\\nCNN/Daily Mail [200] Accuracy - Link -\\nSQuAD F1-score, EM Link Link Link\\nRACE Accuracy - Link Link\\nCNN/Daily Mail [201] ROUGE - Link Link\\nDrop F1-score, EM Link Link Link\\nQuAC F1-score, HEQ-Q, HEQ-D Link Link Link\\nTriviaQA EM, F1-score, Accuracy Link Link Link\\nNatural Questions EM, F1-score, Accuracy Link Link Link\\nStrategyQA Accuracy, Recall@10, SARI Link Link Link\\nCoQA F1-score Link Link Link\\nXSum ROUGE - Link Link\\nSAMSum ROUGE - - Link\\nWikiSum ROUGE - Link -\\nDialogSum ROUGE - Link Link\\nTruthfulQA MC1 , MC2, % true, % info, BLEURT Link Link Link\\nMMLU Accuracy Link Link Link\\nGSM8K Accuracy Link Link Link\\nPIQA Accuracy Link Link Link\\nSIQA Accuracy Link Link Link\\nOpenBookQA (OBQA) Accuracy Link Link Link\\nHotpotQA EM, F1-score, Joint EM, Joint F1-score, Link Link Link\\nMATH Accuracy - Link Link\\nCommonsenseQA Accuracy Link Link Link\\nNatural Instructions ROUGE-L, Human Link Link Link\\nBIG-bench Accuracy, Average - Link Link\\nToolTalkSuccess rate, Precision, Recall, Incorrect\\naction rate, Percent of failing error types- Link Link\\nMetaTool Accuracy, Precision, Recall, F1-score - Link Link\\nGPT4ToolsSuccessful Rate of Thought, Successful\\nRate of Action, Successful Rate of Ar-\\nguments, Success Rate- Link Link\\nAPI-BankCorrectness, ROUGE, Error(API Hallu-\\ncination, Has Exception, Invalid Input\\nParameters, False API Call Format, API\\nCall, Miss Input Parameters)- Link Link\\nAlpaca-CoT - - Link Link\\nsolutions as a code, what is the probability of selecting the\\ncorrect one among them. Pass@k is a very good metric in this\\ncase. It works in this manner that given a problem, different\\nsolutions as code are generated. They are tested for correctness\\nusing different functionality tests. Afterward, from generated\\nn solutions, and the respective c number of them being correct\\nequation 4 provides the final value.\\npass@ k:=E\\nProblems\"\\n1‚àí\\x00n‚àíc\\nk\\x01\\n\\x00n\\nk\\x01#\\n(4)\\nExact match (EM) is another metric that is mostly con-\\ncerned with exact matches from (pre-defined) answers. It\\ncounts a prediction as correct if it exactly matches one of\\nmore than one desired reference text token by token. In some\\ncases, it can be the same as accuracy and the equation 5 shows\\nthe mathematical definition. Here M is total number of correct\\nanswers and N is the total number of questions [202].EM =M\\nN(5)\\nHuman equivalence score (HEQ) on the other hand, is an\\nalternative to F1 score [203]. HEQ-Q represents the precision\\nof individual questions, wherein an answer is deemed correct\\nif the model‚Äôs F1 score surpasses the average human F1 score.\\nLikewise, HEQ-D denotes the precision of each dialogue; it is\\ndeemed accurate when all questions within the dialogue meet\\nthe criteria of HEQ [182].\\nEvaluation of other generative tasks such as machine trans-\\nlation are based on metrics such as Rouge and BLEU. These\\nscores work well when there is a reference text as ground\\ntruth (such as translation) and a hypothesis that is generated\\nby the generative model, in our case the LLM. These scores\\nare mostly used for cases where the goal is to detect the\\nsimilarity of the answer and ground truth in a computation\\nmanner. In a computation manner, it meant that nothing more\\nthan N-Grams would be used. However, metrics such as BERT-\\nScore are also good for these cases but they are also heavily'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 31}, page_content='TABLE III: LLM categories and respective definitions.\\nClassification Category Description\\nSizeSmall Number of parameters ‚â§1B\\nMedium 1B<Number of parameters ‚â§10B\\nLarge 10B<Number of parameters ‚â§100B\\nVery Large 100B<Number of parameters\\nTypeFoundation model Pretrained language model\\nInstruction model Pretrained and instruction fine-tuned language model\\nChat model Pretrained, instruction fine-tuned, and chat fine-tuned language model\\nOriginOriginal model An original model released with either Foundation, Instruction, or Chat model\\nTuned model Fine-tuned version of an original model\\nAvailabilityPublicly available Model and weights are available due to request to without request\\nPublicly unavailable Model and weights are not publicly available\\nTABLE IV: Different LLM categorization.\\nModel Size #Params (B) Type Availability Origin\\nDavinci-002 Very Large 175 Instruction Unavailable Tuned\\nDavinci-003 Very Large 175 Instruction Unavailable Tuned\\nGPT 3.5-turbo Large 20 Chat Unavailable Tuned\\nFalcon 7B Medium 7 Foundation Public Original\\nAlpaca Large 13 Chat Public Tuned\\nPythia 7B Medium 7 Foundation Public Original\\nPythia 12B Large 12 Foundation Public Original\\nLLAMA 7B Medium 7 Chat Public Original\\nLLAMA 2 7B Medium 7 Chat Public Tuned\\nLLAMA 2 7B Medium 7 Foundation Public Original\\nVicuna 13B Large 13 Foundation Public Tuned\\nVicuna 7B Medium 7 Foundation Public Tuned\\nClaude Large 93 Chat Unavailable Original\\nClaude 2 Very Large 137 Chat Unavailable Original\\nerroneous because another model is used to judge. Still, even\\ntoday, evaluating purely generated content is very hard and\\nno completely fitting metric is not found, metrics are either\\nlooking for simplistic features such as N-Gram, SkipGram,\\netc, or they are models with unknown accuracy and preciseness\\n[204].\\nGenerative evaluation metrics are also another type of eval-\\nuation metric for LLMs that use another LLM for evaluating\\nthe answer. However, depending on the task itself, evaluation\\ncan be possible in this way or not. Another dependency\\nthat makes generative evaluation error-prone is reliance on\\nthe prompt itself. RAGAS is one of the good examples that\\nincorporate the usage of generative evaluation.\\nVarious benchmarks and leaderboards have been proposed\\nto address the most challenging question in the world of\\nlarge language models: Which one is better? However not\\na simple answer can address this question. The answer de-\\npends on various aspects of large language models. Section V\\nshows the categorical presentation of different tasks and the\\nmost important datasets in each category. We will follow the\\nsame categorization and provide a comparison based on each\\ncategory. After providing comparison for each category, we\\nwill provide a broad overview of aggregated performance by\\naveraging the reported performance metric on different tasks.\\nEvaluating different LLMs can be seen also from different\\nperspectives. For example, a LLM with a drastically fewer\\nnumber of parameters is not completely comparable to one\\nwith a larger number of parameters. From this perspective, we\\nwill categorize LLMs in four categories as well: small (less\\nthan or equal to 1 billion parameters), medium (between 1 and\\n10 billion), large (between 10 and 100 billion), and very large\\n(more than 100 billion). Another classification for the LLMswe use is their primary use case. We consider each LLM to\\nbe either: Foundation model (pretrained language model with\\nno instruction fine-tuning and chat fine-tuning), Instruction\\nmodel (pretrained language model with only instruction fine-\\ntuning), and Chat model (pretrained language model with\\ninstruction and chat fine-tuning). Apart from all the catego-\\nrization described, another category is required to distinguish\\nbetween original models and tuned ones. Original models are\\nthose that have been released as a foundation model or a fine-\\ntuned one. Tuned models are those that grasped the original\\nmodel and tuned it with different datasets or even different\\ntraining approaches. It is also good to note that original models\\nare usually foundation models that have been fine-tuned on\\nspecific datasets or even different approaches. Availability of\\nthe model weights regardless of the license is another category\\nin our classification. Models that have their weights publicly\\navailable (even through request) are noted as Public models\\nwhile others are noted as Private . Table III shows all of these\\ndefinitions and abbreviations used in the rest of the article.\\nFigure 43 illustrate these visually.\\nAccording to the provided categorizations, we can catego-\\nrize and label each notable LLM as shown in table IV. As can\\nbe seen from this table, models categorized as very large are\\nalso unavailable as well.\\nB. LLMs‚Äô Performance on Different Tasks\\nCommonsense reasoning is one of the important capabili-\\nties each model can obtain. This capability denotes the ability\\nof the model to use prior knowledge in combination with\\nreasoning skills. In the case of HellaSwag for example, finding\\nthe continuation of text is challenging because the given text\\ncontains a partial part of the story while the given choices\\nas continuation are tricky to select, and without having prior'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 32}, page_content='Large\\nLanguage\\nModelsParameters\\nAvailabilityOriginalityTypeSmall LM\\n# of params <1BMedium LM\\n1B < # of params <10BLarge LM\\n10B < # of params <100B\\nVery Large LM\\n100B < # of params\\nTuned\\nFine tuning\\nOriginal\\nPublic PrivateFoundation\\nInstruction\\nChatFine tuned models that are originally\\nbased on original models.\\nExample: Alpaca  (based on LLaMA)\\nOriginal models that are not fine\\ntuned or based on any other\\npretrained model.\\nExample: LLaMA\\nModel weights are publicly released\\nand is available.\\nExample: LLaMAModel weights are NOT  publicly\\nreleased and is NOT  available.\\nExample: GPT -4Pretrained model with no instruction\\nor chat fine-tuning.\\nExample: MPT -7B\\nPretrained model that is\\nalso fine-tuned on\\ninstruction following.\\nExample: MPT -7B-instruct\\nPretrained model that is\\nalso fine-tuned on chat.\\nExample: MPT -7B-chatFig. 43: LLM categorizations.\\nknowledge about the world it is not possible. This specific kind\\nof reasoning deserves high attention because it is related to\\nutilizing previous knowledge with open text-described scenes\\nor facts. As can be seen from table V not just Unavailable\\nmodels but also Public ones can achieve good results on\\nvarious tests.\\nTABLE V: Commonsense reasoning comparison.\\nModel OBQA HellaSwag\\nDavinci-003 51 83.4\\nFalcon 7B 44.4 76.3\\nAlpaca 43.4 73.9\\nPythia 7B 37.2 64\\nPythia 12B 43.2 68.1\\nLLAMA 7B 42.4 73\\nDolly 6B 41.2 67.6\\nDolly 12B 40.4 71\\nAlpaca 7B 43.4 73.9\\nAlpaca Lora 7B 42.6 74\\nGPT-J 6.7B 38.2 66.2\\nLLama 7B 42.4 73\\nLLama 13B 42.2 76.2\\nPythia 6.7B 37.2 64\\nPythia 12B 38 67.3\\nStableLM Tuned 33.4 53.6\\nKoala 13B 42.8 72.6\\nMosaic mpt-7B 42.6 76.3\\nLLAMA 2 70B - 87.33\\nLLAMA 65B - 86.09\\nFalcon 40B - 85.3\\nFalcon 180B - 88.86\\nMPT Instruct 30B - 84.31\\nMPT Instruct 7B - 77.91\\nYi 6B - 76.42\\nYi 34B - 85.69\\nGPT-4 - 95.3\\nGemini Ultra - 87.8From the results presented in Table V it is clear that GPT-4\\nachieves best results for HellaSwag while Davinci-003 is best\\nmodel for OBQA. It is also good to note that results for OBQA\\nare not reported for all of the models and possibly davinci-003\\nis not the best model achieving highest results on OBQA.\\nNot all models report their performance on all datasets, and\\nbecause of that, the number of models for which performance\\nis reported in different tables varies.\\nTABLE VI: Symbolic reasoning comparison.\\nModel Cobjects Penguins\\nGPT-NeoX 26 33.56\\nOPT 66B 31.2 28.08\\nBloomberg GPT 34.8 37.67\\nBLOOM 176B 36.8 40.41\\nPaLM 540B 38 44.5\\nGopher-280B 49.2 40.6\\nChinchilla-70B 59.7 48.7\\nPaLM 2 61.2 65.8\\nWorld knowledge is mostly about general knowledge ques-\\ntions, for example, in Wikifact dataset questions such as ‚ÄùWho\\nis the author of a specific well-known book‚Äù can be found and\\nreferences are also provided. Table VII shows the results.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 33}, page_content='TABLE VII: World knowledge comparison.\\nModel TriviaQA NaturalQ WebQ ARC\\nBLOOM - - - 32.9\\nBLOOM 176B - - - 50.85\\nBloomberg GPT - - - 48.63\\nChinchilla - 35.5 - -\\nCodex + REPLUG 76.8 44.7 - -\\nGAL 120B - - - 67.9\\nGLaM 62B/64E 75.8 32.5 15.5 50.3\\nGopher - 28.2 - -\\nGPT-3 175B 71.2 29.9 41.5 85.2\\nGPT-4 - - - 96.4\\nGPT-NeoX - - - 45.39\\nLLaMA 13B - - - 52.7\\nLLaMA 2 70B 85 33 - -\\nLLaMA 33B - 24.9 - 57.8\\nLLaMA 65B 72.6 39.9 - -\\nLLaMA 7B - - - 47.6\\nMistral 7B 69.9 28.8 - 55.5\\nNeo-6B - 13.7 - -\\nOPT - - - 31.1\\nOPT 66B - - - 44.54\\nOPT-175B - - - 43.94\\nOPT-175B - - - 25.6\\nPaLM 2-L 86.1 37.5 28.2 95.1\\nPaLM 2-M 81.7 32 26.9 64.9\\nPaLM 2-S 75.2 25.3 21.8 59.6\\nPaLM-540B 81.4 39.6 43.5 87.1\\nphi-1.5-web 1.3B - - - 44.9\\nSparseGPT - - - 38.99\\nSparseGPT - - - 39.85\\nSparseGPT - - - 41.3\\nFor some specific use-case models, it is highly demanded to\\nhave coding and code-generation capability. Table VIII shows\\nthe results of different models on coding capability.\\nTABLE VIII: Coding capability comparison.\\nModel HumanEval\\nGemini Ultra 74.4\\nGemini Pro 67.7\\nGPT-4 67\\nWizardCoder 15B 57.3\\nphi-1 1.3B 50.6\\nCode Llama 48.8\\nGPT-3.5 48.1\\nOctoCoder 46.2\\nphi-1-small 45\\nPaLM 2-S 37.6\\nInstructCodeT5+ 16B 35\\nMistral 7B 30.5\\nLLaMA 2 29.9\\nphi-1-base 29\\nCodex-12B 28.81\\nPaLM 540B 26.2\\nCodeT5+ 2B 24.2\\nLLaMA 65B 23.7\\nLLaMA 33B 21.7\\nPaLM 62B 15.9\\nLLaMA 13B 15.8\\nLaMDA 137B 14\\nMIM-350M 13.7\\nLLaMA 7B 10.5\\nPaLM 8B 3.6\\nArithmetic reasoning is another challenging reasoning ca-\\npability to achieve. GSM8K for example contains grade school\\nmathematical questions with respect to their answers. Table IX\\nprovides an insight for different model comparisons.TABLE IX: Arithmetic reasoning comparison.\\nModel GSM8k MATH\\nGemini Ultra 94.4 53.2\\nGPT-4 87.1 42.5\\nGemini Pro 86.5 32.6\\nToRA 70B 84.3 49.7\\nMathCoder-L-70B 83.9 -\\nMetaMath 70B 82.3 26\\nMuggleMATH 70B 82.3 -\\nMathCoder-CL-34B 81.7 45.2\\nToRA-Code 34B 80.7 50.8\\nMetaMath-Mistral-7B 77.7 -\\nArithmo2-Mistral-7B 76.4 -\\nToRA-Code 13B 75.8 48.1\\nArithmo-Mistral-7B 74.7 -\\nMathCoder-CL-13B 74.1 35.9\\nMuggleMATH 13B 74 -\\nCodeT5+ 73.8 -\\nKwaiYiiMath 13B 73.3 -\\nToRA-Code 7B 72.6 44.6\\nMathCoder-L-13B 72.6 29.9\\nMetaMath 13B 71 22.5\\nLLaMA 65B 69.7 10.6\\nMuggleMATH 7B 68.4 -\\nMathCoder-CL-7B 67.8 23.3\\nMetaMath 7B 66.4 19.4\\nRFT 70B 64.8 -\\nMathCoder-L-7B 64.2 -\\nOrca 2-13B 59.14 -\\nU-PaLM 58.5 -\\nPaLM-540B 58.1 8.8\\nLLaMA 2 70B 56.8 -\\nRFT 13B 55.3 -\\nLLaMA 33B 53.1 7.1\\nMistral 7B 52.2 13.1\\nRFT 7B 51.2 -\\nLLaMA 65B 50.9 20.5\\nOrca 2-7B 47.23 -\\nText-davinci-002 40.7 19.1\\nLLaMA 33B 35.6 3.9\\nGPT-Neo-2.7B 19.5 -\\nLLaMA 7B 18.1 2.9\\nPaLM 540B 17.9 8.8\\nLLaMA 13B 17.8 3.9\\nLLaMA 7B 11 2.9\\nGPT-Neo-125M 7.5 -\\nPaLM 8B 4.1 1.5\\nGPT-2 - 5.4\\nGPT-3 175B - 5.2\\nPaLM 62B - 4.4\\nGPT-3-13B - 3\\nLLaMA 7B 11 2.9\\nPaLM 8B - 1.5\\nLarge language models in some cases are hallucinating an-\\nswers simply because they are next-token prediction machines.\\nHallucination is one of the important factors in measuring\\nhow much a large language model is trustworthy and reliable.\\nMeasuring hallucination on the other hand is also not easy as it\\nseems because each fact can be written in different styles and\\neven the smallest changes in writing make it hard to detect.\\nIt is fair to assume if any particular LLM is more capable\\nto detect hallucination of false information in text, it is also\\nmore trustworthy. HaluEval is one of the datasets that aims to\\nmeasure hallucination in this field [205]. Evaluation can also be\\nperformed by another model judging the response with regard\\nto the actual answer [206]. Table X shows the evaluation of\\ndifferent models based on these datasets.\\nVII. C HALLENGES AND FUTURE DIRECTIONS\\nAs we have seen in the previous sections, large language\\nmodels have achieved impressive results in the past 1-2 years.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 34}, page_content='TABLE X: Hallucination evaluation\\nModel HHEM HaluEval QA HaluEval Dialogue HaluEval Sum. HaluEval General\\nGPT 4 97 - - - -\\nGPT 4 Turbo 97 - - - -\\nGPT 3.5 Turbo 96.5 62.59 72.4 58.53 79.44\\nDavinci002 - 60.05 60.81 47.77 80.42\\nDavinci003 - 49.65 68.37 48.07 80.4\\nGPT-3 - 49.21 50.02 51.23 72.72\\nGoogle Gemini Pro 95.2 - - - -\\nLlama 2 70B 94.9 - - - -\\nLlama 2 7B 94.4 49.6 43.99 49.55 20.46\\nLlama 2 13B 94.1 - - - -\\nCohere-Chat 92.5 - - - -\\nCohere 91.5 - - - -\\nClaude 2 91.5 69.78 64.73 57.75 75\\nClaude 1 67.6 64.83 53.76 73.88\\nMicrosoft Phi 2 91.5 - - - -\\nGoogle Palm 2 (beta) 91.4 - - - -\\nMixtral 8x7B 90.7 - - - -\\nAmazon Titan Express 90.6 - - - -\\nMistral 7B 90.6 - - - -\\nGoogle Palm 2 Chat (beta) 90 - - - -\\nGoogle Palm 2 87.9 - - - -\\nGoogle Palm 2 Chat 72.8 - - - -\\nChatGLM - 47.93 44.41 48.57 30.92\\nFalcon - 39.66 29.08 42.71 18.98\\nVicuna - 60.34 46.35 45.62 19.48\\nAlpaca - 6.68 17.55 20.63 9.54\\nAt the same time this is still a new and extremely active\\nresearch area where the pace of innovation is increasing rather\\nthan slowing down. As in any other evolving area though, there\\nare still numerous challenges ahead. Here we briefly mention\\nsome of the challenges and main active areas which are known\\nso far. It is worth noting that LLM challenges are discussed\\nin details in a work by Kaddour et al. [207].\\nA. Smaller and more efficient Language Models\\nThis is a survey on large language models, and there\\nhas been an initial push towards ‚Äùlarger is better‚Äù that has\\nclearly been rewarded with ever larger models like GPT-\\n4 getting better accuracy and performance in benchmarks.\\nHowever, those large models are costly and inefficient in\\nseveral dimensions (e.g. high latency). In response to all of\\nthis, there is a current research trend to come up with Small\\nLanguage Models (SLMs) as a cost-effective alternative to\\nLLMs, particularly when used on specific tasks that might not\\nrequire the full generality of larger models. Prominent works\\nin this direction include Phi-1 [208], Phi-1.5 [209], and Phi-2\\nfrom Microsoft.\\nMore generally, we should expect many research efforts in\\nthis area of how to train smaller and more efficient models.\\nTechniques such as parameter-efficient fine-tuning (PEFT),\\nteacher/student, and other forms of distillation ‚Äì see section\\nIII-I ‚Äì will continue to be used to build a smaller model out\\nof larger ones.\\nB. New Post-attention Architectural Paradigms\\nTransformer blocks have been a crucial and constant part of\\nmost of current LLM frameworks, and it‚Äôs a big question mark\\nhow much longer this architecture will be in vogue, and what\\nwill be the next big architectural break-through in the field of\\ndeep learning (and NLP). Since AlexNet in 2012, we have seen\\nmany architectures go in and out of fashion, including LSTM,GRU, seq2seq, but Transformers have been the dominant\\napproach since its inception. As described earlier, attention is\\nthe main mechanism driving transformers. More recently, there\\nhas been promising research in alternative approaches that are\\nbeing labelled as post-attention.\\nAn important class of such class of post-attention models\\nare the so called State Space Models (SSMs). While the notion\\nof State Space Models has a long history in machine learning,\\nit should be noted that in the context of language models, SSM\\nis usually used in reference to the newer Structure State Space\\nModel architecture or S4 for short (see Gu et al. [29]). Some\\nrecent models in this category are Mamba [30], Hyena [210],\\nand Striped Hyena [211].\\nWhile all of those models are very competitive in terms of\\nperformance in leaderboards and efficiency, they also address\\nan important challenge in more traditional attention-based\\narchitectures: the lack of support for larger context windows .\\nHaving a good answer to many prompts requires context.\\nFor example, the response to ‚ÄùRecommend some good movies\\nfor me‚Äù requires a lot of context about ‚Äùme‚Äù as well as what\\nmovies are available and which ones I have not watched.\\nContext length is especially important for RAG, where large\\nportions of text might be retrieved and injected into the prompt\\nfor generation (see section IV-C.\\nThe longer the context length, the more tokens we can\\nsqueeze into the context. The more information the model has\\naccess to, the better its response will be. But on the other\\nhand, with very long context, it would be hard for the model\\nto remember everything and efficiently process all the informa-\\ntion. Attention-based models are highly inefficient for longer\\ncontexts and that is why we should expect more research in\\ndifferent mechanisms that enable processing longer contexts\\nand generally come up with more efficient architectures.\\nThat being said, new architectures might not only propose'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 35}, page_content='alternatives for the attention mechanism but rather rethink the\\nwhole Transformer architecture. As an early example of this,\\nMonarch Mixer [212] proposes a new architecture that uses\\nthe same sub-quadratic primitive that achieves high hardware\\nefficiency on GPUs ‚Äì Monarch matrices ‚Äì along both sequence\\nlength and model dimension.\\nOn the other end of the spectrum, it is worth mentioning\\nthat there are some attention-compatible architectural mecha-\\nnisms that have been recently gaining steam and proving their\\nvalue in creating better and more powerful LLMs. Probably\\nthe best example of such mechanism is Mixture of Experts\\n(MoE). MoEs have been around in machine learning for years,\\neven before the Deep Learning Era [213], but they have been\\ngaining popularity since then, and particularly in the context\\nof Transformer models and LLMs.\\nIn LLMs, MoEs allow to train an extremely large model\\nthan is then only partially instantiated during inference\\nwhen some of the experts are turned off wherever the gat-\\ning/weighting function has a low weight assigned to them. As\\nan example, the GLaM model has 1.2 trillion parameters, but\\nduring inference only 2 out of the 64 experts are used [84].\\nMoEs are nowadays an important component of the so-\\ncalled frontier LLMs (i.e. the most advanced and capable\\nmodels). GPT-4 itself is rumored to be based on a MoE\\narchitecture, and some of the best performing LLMs such as\\nMixtral [117], are basically an MoE version of pre-existing\\nLLMs.\\nFinally, it is important to note that MoEs can be used as a\\ncomponent of any architecture regardless of whether it is based\\non attention or not. In fact, MoEs have also been applied to\\nSSM-based LLMs like Mamba citepioro2024moemamba. We\\nshould continue to see MoE-driven improvements in the future\\nregardless of the underlying architecture.\\nC. Multi-modal Models\\nFuture LLMs are expected to be multi-modal and handle\\na variety of data types, such as text, images, and videos,\\naudio, in a unified manner. This opens up possibilities for\\nmore diverse applications in fields like question answering,\\ncontent generation, creative arts, and healthcare, robotics, and\\nbeyond. There are already several prominent multi-modal\\nLLMs out there, including: LLA V A [214], LLA V A-Plus [215],\\nGPT-4 [33], Qwen-vl [116], Next-GPT [216], but the trend is\\nexpected to be continued. Evaluation of these models also is a\\nnew research topic, especially conversational generative vision\\nmodels [217]. Multi-modal LLMs can unlock huge potentials\\nin a variety of tasks, and there has already been a descent\\nprogress in this direction, which needs a dedicated paper to\\ndiscuss all its details.\\nD. Improved LLM Usage and Augmentation techniques\\nAs we described in sectionIV, many of the shortcomings\\nand limitations of LLMs such as hallucination can be ad-\\ndressed through advanced prompt engineering, use of tools,\\nor other augmentation techniques. We should expect not only\\ncontinued, but accelerated research in this area. It is worth\\nmentioning that, in the specific case of software engineering,\\nsome works ([218]) tried to automatically eliminate this issue\\nfrom the overall software engineering workflowLLM-based systems are already starting to replace ma-\\nchine learning systems that were until recently using other\\napproaches. As a clear example of this, LLMs are now being\\ndeployed to better understand people preference and interests,\\nand provide more personalized interactions, whether in cus-\\ntomer service, content recommendation, or other applications.\\nThis involves better understanding of user preferences, and\\nanalyzing their past interactions and using them as the context.\\nWe will continue to see research in the application and usage\\nof LLMs for not only personalization and recommendations ,\\nbut many other application areas using other machine learning\\ntechniques.\\nFinally, another important area of research we expect to\\ngather increased attention is that of LLM-based agents and\\nmulti-agent systems [172], [173], [174]. The development of\\nLLM systems with access to external tools and decision-\\nmaking capabilities is both exciting and challenging. We will\\nsee continued research and progress in this important area that\\nsome argue could lead to Artificial General Intelligence (AGI).\\nE. Security and Ethical/Responsible AI\\nEnsuring the robustness and security of LLMs against\\nadversarial attacks and other vulnerabilities is a critical area\\nof research [219]. As LLMs are increasingly deployed in real-\\nworld applications, they need to be protected from potential\\nthreats, to prevent them being used to manipulate people or\\nspread mis-information.\\nAddressing ethical concerns and biases in LLMs is another\\nactive area of research. Efforts are being made to ensure that\\nLLMs are fair, unbiased, and capable of handling sensitive\\ninformation responsibly. As LLMs are being used more and\\nmore by a large number of people on a daily basis, making\\nsure they are unbiased and behave responsibly is crucial.\\nVIII. C ONCLUSION\\nThis paper present a survey of LLMs developed in the\\npast few years. We first provide an overview of early pre-\\ntrained language models (e.g., as BERT), then review three\\npopular LLM families (GPT, LLaMA, PaLM), and other\\nrepresentative LLMs. We then survey methods and techniques\\nof building, augmenting, and using LLMs. We review popular\\nLLM datasets and benchmarks, and compare performance of\\na set of prominent models on public benchmarks. Finally, we\\npresent open challenges and future research directions.\\nREFERENCES\\n[1] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, ‚ÄúScaling laws\\nfor neural language models,‚Äù arXiv preprint arXiv:2001.08361 , 2020.\\n[2] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark\\net al. , ‚ÄúTraining compute-optimal large language models,‚Äù arXiv\\npreprint arXiv:2203.15556 , 2022.\\n[3] C. E. Shannon, ‚ÄúPrediction and entropy of printed english,‚Äù Bell system\\ntechnical journal , vol. 30, no. 1, pp. 50‚Äì64, 1951.\\n[4] F. Jelinek, Statistical methods for speech recognition . MIT press,\\n1998.\\n[5] C. Manning and H. Schutze, Foundations of statistical natural lan-\\nguage processing . MIT press, 1999.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 36}, page_content='[6] C. D. Manning, An introduction to information retrieval . Cambridge\\nuniversity press, 2009.\\n[7] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min,\\nB. Zhang, J. Zhang, Z. Dong et al. , ‚ÄúA survey of large language\\nmodels,‚Äù arXiv preprint arXiv:2303.18223 , 2023.\\n[8] C. Zhou, Q. Li, C. Li, J. Yu, Y . Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\\nL. He et al. , ‚ÄúA comprehensive survey on pretrained foundation mod-\\nels: A history from bert to chatgpt,‚Äù arXiv preprint arXiv:2302.09419 ,\\n2023.\\n[9] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, ‚ÄúPre-\\ntrain, prompt, and predict: A systematic survey of prompting methods\\nin natural language processing,‚Äù ACM Computing Surveys , vol. 55,\\nno. 9, pp. 1‚Äì35, 2023.\\n[10] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\\nJ. Xu, and Z. Sui, ‚ÄúA survey for in-context learning,‚Äù arXiv preprint\\narXiv:2301.00234 , 2022.\\n[11] J. Huang and K. C.-C. Chang, ‚ÄúTowards reasoning in large language\\nmodels: A survey,‚Äù arXiv preprint arXiv:2212.10403 , 2022.\\n[12] S. F. Chen and J. Goodman, ‚ÄúAn empirical study of smoothing\\ntechniques for language modeling,‚Äù Computer Speech & Language ,\\nvol. 13, no. 4, pp. 359‚Äì394, 1999.\\n[13] Y . Bengio, R. Ducharme, and P. Vincent, ‚ÄúA neural probabilistic\\nlanguage model,‚Äù Advances in neural information processing systems ,\\nvol. 13, 2000.\\n[14] H. Schwenk, D. D ¬¥echelotte, and J.-L. Gauvain, ‚ÄúContinuous space\\nlanguage models for statistical machine translation,‚Äù in Proceedings\\nof the COLING/ACL 2006 Main Conference Poster Sessions , 2006,\\npp. 723‚Äì730.\\n[15] T. Mikolov, M. Karafi ¬¥at, L. Burget, J. Cernock `y, and S. Khudanpur,\\n‚ÄúRecurrent neural network based language model.‚Äù in Interspeech ,\\nvol. 2, no. 3. Makuhari, 2010, pp. 1045‚Äì1048.\\n[16] A. Graves, ‚ÄúGenerating sequences with recurrent neural networks,‚Äù\\narXiv preprint arXiv:1308.0850 , 2013.\\n[17] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, ‚ÄúLearning\\ndeep structured semantic models for web search using clickthrough\\ndata,‚Äù in Proceedings of the 22nd ACM international conference on\\nInformation & Knowledge Management , 2013, pp. 2333‚Äì2338.\\n[18] J. Gao, C. Xiong, P. Bennett, and N. Craswell, Neural Approaches to\\nConversational Information Retrieval . Springer Nature, 2023, vol. 44.\\n[19] I. Sutskever, O. Vinyals, and Q. V . Le, ‚ÄúSequence to sequence learning\\nwith neural networks,‚Äù Advances in neural information processing\\nsystems , vol. 27, 2014.\\n[20] K. Cho, B. Van Merri ¬®enboer, D. Bahdanau, and Y . Bengio, ‚ÄúOn\\nthe properties of neural machine translation: Encoder-decoder ap-\\nproaches,‚Äù arXiv preprint arXiv:1409.1259 , 2014.\\n[21] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Doll ¬¥ar,\\nJ. Gao, X. He, M. Mitchell, J. C. Platt et al. , ‚ÄúFrom captions to\\nvisual concepts and back,‚Äù in Proceedings of the IEEE conference\\non computer vision and pattern recognition , 2015, pp. 1473‚Äì1482.\\n[22] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, ‚ÄúShow and tell:\\nA neural image caption generator,‚Äù in Proceedings of the IEEE\\nconference on computer vision and pattern recognition , 2015, pp.\\n3156‚Äì3164.\\n[23] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nand L. Zettlemoyer, ‚ÄúDeep contextualized word representations. corr\\nabs/1802.05365 (2018),‚Äù arXiv preprint arXiv:1802.05365 , 2018.\\n[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-training\\nof deep bidirectional transformers for language understanding,‚Äù arXiv\\npreprint arXiv:1810.04805 , 2018.\\n[25] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, and V . Stoyanov, ‚ÄúRoberta: A robustly optimized bert\\npretraining approach,‚Äù arXiv preprint arXiv:1907.11692 , 2019.\\n[26] P. He, X. Liu, J. Gao, and W. Chen, ‚ÄúDeberta: Decoding-enhanced bert\\nwith disentangled attention,‚Äù arXiv preprint arXiv:2006.03654 , 2020.\\n[27] X. Han, Z. Zhang, N. Ding, Y . Gu, X. Liu, Y . Huo, J. Qiu, Y . Yao,\\nA. Zhang, L. Zhang et al. , ‚ÄúPre-trained models: Past, present and\\nfuture,‚Äù AI Open , vol. 2, pp. 225‚Äì250, 2021.\\n[28] X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang, ‚ÄúPre-trainedmodels for natural language processing: A survey,‚Äù Science China\\nTechnological Sciences , vol. 63, no. 10, pp. 1872‚Äì1897, 2020.\\n[29] A. Gu, K. Goel, and C. R ¬¥e, ‚ÄúEfficiently modeling long sequences with\\nstructured state spaces,‚Äù 2022.\\n[30] A. Gu and T. Dao, ‚ÄúMamba: Linear-time sequence modeling with\\nselective state spaces,‚Äù arXiv preprint arXiv:2312.00752 , 2023.\\n[31] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\\nA. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al. ,\\n‚ÄúPalm: Scaling language modeling with pathways,‚Äù arXiv preprint\\narXiv:2204.02311 , 2022.\\n[32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al. , ‚ÄúLlama:\\nOpen and efficient foundation language models,‚Äù arXiv preprint\\narXiv:2302.13971 , 2023.\\n[33] OpenAI, ‚ÄúGPT-4 Technical Report,‚Äù https://arxiv.org/pdf/2303.\\n08774v3.pdf, 2023.\\n[34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter,\\nF. Xia, E. Chi, Q. V . Le, and D. Zhou, ‚ÄúChain-of-thought\\nprompting elicits reasoning in large language models,‚Äù in\\nAdvances in Neural Information Processing Systems , S. Koyejo,\\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\\nEds., vol. 35. Curran Associates, Inc., 2022, pp. 24 824‚Äì24 837.\\n[Online]. Available: https://proceedings.neurips.cc/paper files/paper/\\n2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\\n[35] G. Mialon, R. Dess `ƒ±, M. Lomeli, C. Nalmpantis, R. Pasunuru,\\nR. Raileanu, B. Rozi `ere, T. Schick, J. Dwivedi-Yu, A. Celikyil-\\nmaz et al. , ‚ÄúAugmented language models: a survey,‚Äù arXiv preprint\\narXiv:2302.07842 , 2023.\\n[36] B. Peng, M. Galley, P. He, H. Cheng, Y . Xie, Y . Hu, Q. Huang,\\nL. Liden, Z. Yu, W. Chen, and J. Gao, ‚ÄúCheck your facts and try\\nagain: Improving large language models with external knowledge and\\nautomated feedback,‚Äù arXiv preprint arXiv:2302.12813 , 2023.\\n[37] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao,\\n‚ÄúReact: Synergizing reasoning and acting in language models,‚Äù arXiv\\npreprint arXiv:2210.03629 , 2022.\\n[38] D. E. Rumelhart, G. E. Hinton, R. J. Williams et al. , ‚ÄúLearning internal\\nrepresentations by error propagation,‚Äù 1985.\\n[39] J. L. Elman, ‚ÄúFinding structure in time,‚Äù Cognitive science , vol. 14,\\nno. 2, pp. 179‚Äì211, 1990.\\n[40] M. V . Mahoney, ‚ÄúFast text compression with neural networks.‚Äù in\\nFLAIRS conference , 2000, pp. 230‚Äì234.\\n[41] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. ÀáCernock `y, ‚ÄúStrate-\\ngies for training large scale neural network language models,‚Äù in 2011\\nIEEE Workshop on Automatic Speech Recognition & Understanding .\\nIEEE, 2011, pp. 196‚Äì201.\\n[42] tmikolov. rnnlm. [Online]. Available: https://www.fit.vutbr.cz/\\n‚àºimikolov/rnnlm/\\n[43] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu,\\nand J. Gao, ‚ÄúDeep learning‚Äìbased text classification: a comprehensive\\nreview,‚Äù ACM computing surveys (CSUR) , vol. 54, no. 3, pp. 1‚Äì40,\\n2021.\\n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\\nAdvances in neural information processing systems , vol. 30, 2017.\\n[45] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\\n‚ÄúAlbert: A lite bert for self-supervised learning of language represen-\\ntations,‚Äù arXiv preprint arXiv:1909.11942 , 2019.\\n[46] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning, ‚ÄúElectra: Pre-\\ntraining text encoders as discriminators rather than generators,‚Äù arXiv\\npreprint arXiv:2003.10555 , 2020.\\n[47] G. Lample and A. Conneau, ‚ÄúCross-lingual language model pretrain-\\ning,‚Äù arXiv preprint arXiv:1901.07291 , 2019.\\n[48] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and\\nQ. V . Le, ‚ÄúXlnet: Generalized autoregressive pretraining for language\\nunderstanding,‚Äù Advances in neural information processing systems ,\\nvol. 32, 2019.\\n[49] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao,\\nM. Zhou, and H.-W. Hon, ‚ÄúUnified language model pre-training for'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 37}, page_content='natural language understanding and generation,‚Äù Advances in neural\\ninformation processing systems , vol. 32, 2019.\\n[50] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al. , ‚ÄúImprov-\\ning language understanding by generative pre-training,‚Äù 2018.\\n[51] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. ,\\n‚ÄúLanguage models are unsupervised multitask learners,‚Äù OpenAI blog ,\\nvol. 1, no. 8, p. 9, 2019.\\n[52] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY . Zhou, W. Li, and P. J. Liu, ‚ÄúExploring the limits of transfer learning\\nwith a unified text-to-text transformer,‚Äù The Journal of Machine\\nLearning Research , vol. 21, no. 1, pp. 5485‚Äì5551, 2020.\\n[53] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\\nA. Barua, and C. Raffel, ‚Äúmt5: A massively multilingual pre-trained\\ntext-to-text transformer,‚Äù arXiv preprint arXiv:2010.11934 , 2020.\\n[54] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y . Liu, ‚ÄúMass: Masked\\nsequence to sequence pre-training for language generation,‚Äù arXiv\\npreprint arXiv:1905.02450 , 2019.\\n[55] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\\nV . Stoyanov, and L. Zettlemoyer, ‚ÄúBart: Denoising sequence-to-\\nsequence pre-training for natural language generation, translation, and\\ncomprehension,‚Äù arXiv preprint arXiv:1910.13461 , 2019.\\n[56] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al. , ‚ÄúLanguage mod-\\nels are few-shot learners,‚Äù Advances in neural information processing\\nsystems , vol. 33, pp. 1877‚Äì1901, 2020.\\n[57] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Ka-\\nplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman et al. ,\\n‚ÄúEvaluating large language models trained on code,‚Äù arXiv preprint\\narXiv:2107.03374 , 2021.\\n[58] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V . Kosaraju, W. Saunders et al. , ‚ÄúWebgpt: Browser-\\nassisted question-answering with human feedback,‚Äù arXiv preprint\\narXiv:2112.09332 , 2021.\\n[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , ‚ÄúTraining language\\nmodels to follow instructions with human feedback,‚Äù Advances in\\nNeural Information Processing Systems , vol. 35, pp. 27 730‚Äì27 744,\\n2022.\\n[60] OpenAI. (2022) Introducing chatgpt. [Online]. Available: https:\\n//openai.com/blog/chatgpt\\n[61] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al. , ‚ÄúLlama\\n2: Open foundation and fine-tuned chat models,‚Äù arXiv preprint\\narXiv:2307.09288 , 2023.\\n[62] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang,\\nand T. B. Hashimoto, ‚ÄúAlpaca: A strong, replicable instruction-\\nfollowing model,‚Äù Stanford Center for Research on Foundation Mod-\\nels. https://crfm. stanford. edu/2023/03/13/alpaca. html , vol. 3, no. 6,\\np. 7, 2023.\\n[63] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, ‚ÄúQlora: Ef-\\nficient finetuning of quantized llms,‚Äù arXiv preprint arXiv:2305.14314 ,\\n2023.\\n[64] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\\nand D. Song, ‚ÄúKoala: A dialogue model for academic research,‚Äù Blog\\npost, April , vol. 1, 2023.\\n[65] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,\\nD. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al. ,\\n‚ÄúMistral 7b,‚Äù arXiv preprint arXiv:2310.06825 , 2023.\\n[66] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y . Adi,\\nJ. Liu, T. Remez, J. Rapin et al. , ‚ÄúCode llama: Open foundation models\\nfor code,‚Äù arXiv preprint arXiv:2308.12950 , 2023.\\n[67] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, ‚ÄúGorilla: Large\\nlanguage model connected with massive apis,‚Äù 2023.\\n[68] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, and\\nS. Naidu, ‚ÄúGiraffe: Adventures in expanding context lengths in llms,‚Äù\\narXiv preprint arXiv:2308.10882 , 2023.\\n[69] B. Huang, ‚ÄúVigogne: French instruction-following and chat models,‚Äù\\nhttps://github.com/bofenghuang/vigogne, 2023.[70] Y . Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu,\\nD. Wadden, K. MacMillan, N. A. Smith, I. Beltagy et al. , ‚ÄúHow far can\\ncamels go? exploring the state of instruction tuning on open resources,‚Äù\\narXiv preprint arXiv:2306.04751 , 2023.\\n[71] S. Tworkowski, K. Staniszewski, M. Pacek, Y . Wu, H. Michalewski,\\nand P. Mi≈Ço ¬¥s, ‚ÄúFocused transformer: Contrastive training for context\\nscaling,‚Äù arXiv preprint arXiv:2307.03170 , 2023.\\n[72] D. Mahan, R. Carlow, L. Castricato, N. Cooper,\\nand C. Laforte, ‚ÄúStable beluga models.‚Äù [Online].\\nAvailable: [https://huggingface.co/stabilityai/StableBeluga2](https://\\nhuggingface.co/stabilityai/StableBeluga2)\\n[73] Y . Tay, J. Wei, H. W. Chung, V . Q. Tran, D. R. So, S. Shakeri, X. Gar-\\ncia, H. S. Zheng, J. Rao, A. Chowdhery et al. , ‚ÄúTranscending scaling\\nlaws with 0.1% extra compute,‚Äù arXiv preprint arXiv:2210.11399 ,\\n2022.\\n[74] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus,\\nY . Li, X. Wang, M. Dehghani, S. Brahma et al. , ‚ÄúScaling instruction-\\nfinetuned language models,‚Äù arXiv preprint arXiv:2210.11416 , 2022.\\n[75] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen et al. , ‚ÄúPalm 2 technical\\nreport,‚Äù arXiv preprint arXiv:2305.10403 , 2023.\\n[76] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al. , ‚ÄúLarge language\\nmodels encode clinical knowledge,‚Äù arXiv preprint arXiv:2212.13138 ,\\n2022.\\n[77] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou,\\nK. Clark, S. Pfohl, H. Cole-Lewis, D. Neal et al. , ‚ÄúTowards expert-\\nlevel medical question answering with large language models,‚Äù arXiv\\npreprint arXiv:2305.09617 , 2023.\\n[78] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,\\nA. M. Dai, and Q. V . Le, ‚ÄúFinetuned language models are zero-shot\\nlearners,‚Äù arXiv preprint arXiv:2109.01652 , 2021.\\n[79] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\\nJ. Aslanides, S. Henderson, R. Ring, S. Young et al. , ‚ÄúScaling language\\nmodels: Methods, analysis & insights from training gopher,‚Äù arXiv\\npreprint arXiv:2112.11446 , 2021.\\n[80] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al. , ‚ÄúMulti-\\ntask prompted training enables zero-shot task generalization,‚Äù arXiv\\npreprint arXiv:2110.08207 , 2021.\\n[81] Y . Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\\nY . Zhao, Y . Lu et al. , ‚ÄúErnie 3.0: Large-scale knowledge enhanced pre-\\ntraining for language understanding and generation,‚Äù arXiv preprint\\narXiv:2107.02137 , 2021.\\n[82] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Mil-\\nlican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark\\net al. , ‚ÄúImproving language models by retrieving from trillions of\\ntokens,‚Äù in International conference on machine learning . PMLR,\\n2022, pp. 2206‚Äì2240.\\n[83] O. Lieber, O. Sharir, B. Lenz, and Y . Shoham, ‚ÄúJurassic-1: Technical\\ndetails and evaluation,‚Äù White Paper. AI21 Labs , vol. 1, p. 9, 2021.\\n[84] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun,\\nY . Zhou, A. W. Yu, O. Firat et al. , ‚ÄúGlam: Efficient scaling of\\nlanguage models with mixture-of-experts,‚Äù in International Conference\\non Machine Learning . PMLR, 2022, pp. 5547‚Äì5569.\\n[85] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-\\nT. Cheng, A. Jin, T. Bos, L. Baker, Y . Du et al. , ‚ÄúLamda: Language\\nmodels for dialog applications,‚Äù arXiv preprint arXiv:2201.08239 ,\\n2022.\\n[86] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,\\nC. Dewan, M. Diab, X. Li, X. V . Lin et al. , ‚ÄúOpt: Open pre-trained\\ntransformer language models,‚Äù arXiv preprint arXiv:2205.01068 , 2022.\\n[87] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Sar-\\navia, A. Poulton, V . Kerkez, and R. Stojnic, ‚ÄúGalactica: A large\\nlanguage model for science,‚Äù arXiv preprint arXiv:2211.09085 , 2022.\\n[88] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou,\\nS. Savarese, and C. Xiong, ‚ÄúCodegen: An open large language\\nmodel for code with multi-turn program synthesis,‚Äù arXiv preprint\\narXiv:2203.13474 , 2022.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 38}, page_content='[89] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky et al. ,\\n‚ÄúAlexatm 20b: Few-shot learning using a large-scale multilingual\\nseq2seq model,‚Äù arXiv preprint arXiv:2208.01448 , 2022.\\n[90] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V . Firoiu,\\nT. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al. ,\\n‚ÄúImproving alignment of dialogue agents via targeted human judge-\\nments,‚Äù arXiv preprint arXiv:2209.14375 , 2022.\\n[91] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski,\\nV . Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo et al. ,\\n‚ÄúSolving quantitative reasoning problems with language models,‚Äù\\nAdvances in Neural Information Processing Systems , vol. 35, pp.\\n3843‚Äì3857, 2022.\\n[92] Y . Tay, M. Dehghani, V . Q. Tran, X. Garcia, D. Bahri, T. Schuster,\\nH. S. Zheng, N. Houlsby, and D. Metzler, ‚ÄúUnifying language learning\\nparadigms,‚Äù arXiv preprint arXiv:2205.05131 , 2022.\\n[93] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ¬¥c, D. Hesslow,\\nR. Castagn ¬¥e, A. S. Luccioni, F. Yvon, M. Gall ¬¥eet al. , ‚ÄúBloom: A 176b-\\nparameter open-access multilingual language model,‚Äù arXiv preprint\\narXiv:2211.05100 , 2022.\\n[94] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu,\\nW. Zheng, X. Xia et al. , ‚ÄúGlm-130b: An open bilingual pre-trained\\nmodel,‚Äù arXiv preprint arXiv:2210.02414 , 2022.\\n[95] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O‚ÄôBrien,\\nE. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff et al. ,\\n‚ÄúPythia: A suite for analyzing large language models across train-\\ning and scaling,‚Äù in International Conference on Machine Learning .\\nPMLR, 2023, pp. 2397‚Äì2430.\\n[96] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and\\nA. Awadallah, ‚ÄúOrca: Progressive learning from complex explanation\\ntraces of gpt-4,‚Äù arXiv preprint arXiv:2306.02707 , 2023.\\n[97] R. Li, L. B. Allal, Y . Zi, N. Muennighoff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim et al. , ‚ÄúStarcoder: may the source\\nbe with you!‚Äù arXiv preprint arXiv:2305.06161 , 2023.\\n[98] S. Huang, L. Dong, W. Wang, Y . Hao, S. Singhal, S. Ma, T. Lv,\\nL. Cui, O. K. Mohammed, Q. Liu et al. , ‚ÄúLanguage is not all you\\nneed: Aligning perception with language models,‚Äù arXiv preprint\\narXiv:2302.14045 , 2023.\\n[99] G. Team, R. Anil, S. Borgeaud, Y . Wu, J.-B. Alayrac, J. Yu, R. Soricut,\\nJ. Schalkwyk, A. M. Dai, A. Hauth et al. , ‚ÄúGemini: a family of highly\\ncapable multimodal models,‚Äù arXiv preprint arXiv:2312.11805 , 2023.\\n[100] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\\nJ. Tompson, I. Mordatch, Y . Chebotar et al. , ‚ÄúInner monologue:\\nEmbodied reasoning through planning with language models,‚Äù arXiv\\npreprint arXiv:2207.05608 , 2022.\\n[101] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti\\net al. , ‚ÄúUsing deepspeed and megatron to train megatron-turing\\nnlg 530b, a large-scale generative language model,‚Äù arXiv preprint\\narXiv:2201.11990 , 2022.\\n[102] I. Beltagy, M. E. Peters, and A. Cohan, ‚ÄúLongformer: The long-\\ndocument transformer,‚Äù arXiv preprint arXiv:2004.05150 , 2020.\\n[103] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus-\\nter, T. Wang, Q. Liu, P. S. Koura et al. , ‚ÄúOpt-iml: Scaling language\\nmodel instruction meta learning through the lens of generalization,‚Äù\\narXiv preprint arXiv:2212.12017 , 2022.\\n[104] Y . Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma,\\nand F. Wei, ‚ÄúLanguage models are general-purpose interfaces,‚Äù arXiv\\npreprint arXiv:2206.06336 , 2022.\\n[105] Z. Sun, Y . Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y . Yang,\\nand C. Gan, ‚ÄúPrinciple-driven self-alignment of language mod-\\nels from scratch with minimal human supervision,‚Äù arXiv preprint\\narXiv:2305.03047 , 2023.\\n[106] W. E. team, ‚ÄúPalmyra-base Parameter Autoregressive Language\\nModel,‚Äù https://dev.writer.com, 2023.\\n[107] ‚Äî‚Äî, ‚ÄúCamel-5b instructgpt,‚Äù https://dev.writer.com, 2023.\\n[108] Yandex. Yalm. [Online]. Available: https://github.com/yandex/\\nYaLM-100B\\n[109] M. Team et al. , ‚ÄúIntroducing mpt-7b: a new standard for open-source,\\ncommercially usable llms,‚Äù 2023.[110] A. Mitra, L. D. Corro, S. Mahajan, A. Codas, C. Simoes, S. Agarwal,\\nX. Chen, A. Razdaibiedina, E. Jones, K. Aggarwal, H. Palangi,\\nG. Zheng, C. Rosset, H. Khanpour, and A. Awadallah, ‚ÄúOrca 2:\\nTeaching small language models how to reason,‚Äù 2023.\\n[111] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y . Yang, J. Callan, and\\nG. Neubig, ‚ÄúPal: Program-aided language models,‚Äù in International\\nConference on Machine Learning . PMLR, 2023, pp. 10 764‚Äì10 799.\\n[112] Anthropic. claude. [Online]. Available: https://www.anthropic.com/\\nnews/introducing-claude\\n[113] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y . Zhou,\\n‚ÄúCodegen2: Lessons for training llms on programming and natural\\nlanguages,‚Äù arXiv preprint arXiv:2305.02309 , 2023.\\n[114] L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y . Belkada,\\nS. Huang, L. von Werra, C. Fourrier, N. Habib et al. , ‚ÄúZephyr: Direct\\ndistillation of lm alignment,‚Äù arXiv preprint arXiv:2310.16944 , 2023.\\n[115] X. team. Grok. [Online]. Available: https://grok.x.ai/\\n[116] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou,\\nand J. Zhou, ‚ÄúQwen-vl: A frontier large vision-language model with\\nversatile abilities,‚Äù arXiv preprint arXiv:2308.12966 , 2023.\\n[117] mixtral. mixtral. [Online]. Available: https://mistral.ai/news/\\nmixtral-of-experts/\\n[118] D. Wang, N. Raman, M. Sibue, Z. Ma, P. Babkin, S. Kaur, Y . Pei,\\nA. Nourbakhsh, and X. Liu, ‚ÄúDocllm: A layout-aware generative\\nlanguage model for multimodal document understanding,‚Äù 2023.\\n[119] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi,\\nY . Wu, Y . K. Li, F. Luo, Y . Xiong, and W. Liang, ‚ÄúDeepseek-coder:\\nWhen the large language model meets programming ‚Äì the rise of code\\nintelligence,‚Äù 2024.\\n[120] F. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi, ‚ÄúKnowledge\\nfusion of large language models,‚Äù 2024.\\n[121] P. Zhang, G. Zeng, T. Wang, and W. Lu, ‚ÄúTinyllama: An open-source\\nsmall language model,‚Äù 2024.\\n[122] C. Wu, Y . Gan, Y . Ge, Z. Lu, J. Wang, Y . Feng, P. Luo, and Y . Shan,\\n‚ÄúLlama pro: Progressive llama with block expansion,‚Äù 2024.\\n[123] X. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and\\nM. Kazi, ‚ÄúTransformer models: an introduction and catalog,‚Äù 2023.\\n[124] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli,\\nH. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, ‚ÄúThe refined-\\nweb dataset for falcon llm: outperforming curated corpora with web\\ndata, and web data only,‚Äù arXiv preprint arXiv:2306.01116 , 2023.\\n[125] D. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-\\nShowk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume et al. ,\\n‚ÄúScaling laws and interpretability of learning from repeated data,‚Äù\\narXiv preprint arXiv:2205.10487 , 2022.\\n[126] P. Shaw, J. Uszkoreit, and A. Vaswani, ‚ÄúSelf-attention with relative\\nposition representations,‚Äù arXiv preprint arXiv:1803.02155 , 2018.\\n[127] J. Su, Y . Lu, S. Pan, B. Wen, and Y . Liu, ‚ÄúRoformer: En-\\nhanced transformer with rotary position embedding,‚Äù arXiv preprint\\narXiv:2104.09864 , 2021.\\n[128] O. Press, N. A. Smith, and M. Lewis, ‚ÄúTrain short, test long: Attention\\nwith linear biases enables input length extrapolation,‚Äù arXiv preprint\\narXiv:2108.12409 , 2021.\\n[129] G. Ke, D. He, and T.-Y . Liu, ‚ÄúRethinking positional encoding in\\nlanguage pre-training,‚Äù arXiv preprint arXiv:2006.15595 , 2020.\\n[130] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\\nand J. Dean, ‚ÄúOutrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer,‚Äù arXiv preprint arXiv:1701.06538 , 2017.\\n[131] W. Fedus, B. Zoph, and N. Shazeer, ‚ÄúSwitch transformers: Scaling\\nto trillion parameter models with simple and efficient sparsity,‚Äù The\\nJournal of Machine Learning Research , vol. 23, no. 1, pp. 5232‚Äì5270,\\n2022.\\n[132] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson,\\n‚ÄúParameter-efficient multi-task fine-tuning for transformers via shared\\nhypernetworks,‚Äù 2021.\\n[133] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,\\nT. Zhang, F. Wu, and G. Wang, ‚ÄúInstruction tuning for large language\\nmodels: A survey,‚Äù 2023.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 39}, page_content='[134] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, ‚ÄúCross-task\\ngeneralization via natural language crowdsourcing instructions,‚Äù arXiv\\npreprint arXiv:2104.08773 , 2021.\\n[135] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,\\nand H. Hajishirzi, ‚ÄúSelf-instruct: Aligning language model with self\\ngenerated instructions,‚Äù arXiv preprint arXiv:2212.10560 , 2022.\\n[136] K. Ethayarajh, W. Xu, D. Jurafsky, and D. Kiela. Kto. [Online].\\nAvailable: https://github.com/ContextualAI/HALOs/blob/main/assets/\\nreport.pdf\\n[137] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and\\nD. Amodei, ‚ÄúDeep reinforcement learning from human preferences,‚Äù\\nAdvances in neural information processing systems , vol. 30, 2017.\\n[138] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V . Car-\\nbune, and A. Rastogi, ‚ÄúRlaif: Scaling reinforcement learning from\\nhuman feedback with ai feedback,‚Äù arXiv preprint arXiv:2309.00267 ,\\n2023.\\n[139] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and\\nC. Finn, ‚ÄúDirect preference optimization: Your language model is\\nsecretly a reward model,‚Äù arXiv preprint arXiv:2305.18290 , 2023.\\n[140] S. Rajbhandari, J. Rasley, O. Ruwase, and Y . He, ‚ÄúZero: Memory\\noptimizations toward training trillion parameter models,‚Äù in SC20: In-\\nternational Conference for High Performance Computing, Networking,\\nStorage and Analysis . IEEE, 2020, pp. 1‚Äì16.\\n[141] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao,\\nX. Cheng, M. Chung, M. Grella, K. K. GV et al. , ‚ÄúRwkv: Reinventing\\nrnns for the transformer era,‚Äù arXiv preprint arXiv:2305.13048 , 2023.\\n[142] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,\\nand W. Chen, ‚ÄúLora: Low-rank adaptation of large language models,‚Äù\\narXiv preprint arXiv:2106.09685 , 2021.\\n[143] G. Hinton, O. Vinyals, and J. Dean, ‚ÄúDistilling the knowledge in a\\nneural network,‚Äù arXiv preprint arXiv:1503.02531 , 2015.\\n[144] J. Gou, B. Yu, S. J. Maybank, and D. Tao, ‚ÄúKnowledge distillation:\\nA survey,‚Äù International Journal of Computer Vision , vol. 129, pp.\\n1789‚Äì1819, 2021.\\n[145] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y . Xu, E. Ishii, Y . J.\\nBang, A. Madotto, and P. Fung, ‚ÄúSurvey of hallucination in natural\\nlanguage generation,‚Äù ACM Comput. Surv. , vol. 55, no. 12, mar 2023.\\n[Online]. Available: https://doi.org/10.1145/3571730\\n[146] N. McKenna, T. Li, L. Cheng, M. J. Hosseini, M. Johnson, and\\nM. Steedman, ‚ÄúSources of hallucination by large language models on\\ninference tasks,‚Äù 2023.\\n[147] C.-Y . Lin, ‚ÄúROUGE: A package for automatic evaluation of\\nsummaries,‚Äù in Text Summarization Branches Out . Barcelona, Spain:\\nAssociation for Computational Linguistics, Jul. 2004, pp. 74‚Äì81.\\n[Online]. Available: https://aclanthology.org/W04-1013\\n[148] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, ‚ÄúBleu: a method for\\nautomatic evaluation of machine translation,‚Äù in Proceedings of the\\n40th Annual Meeting of the Association for Computational Linguistics ,\\nP. Isabelle, E. Charniak, and D. Lin, Eds. Philadelphia, Pennsylvania,\\nUSA: Association for Computational Linguistics, Jul. 2002, pp. 311‚Äì\\n318. [Online]. Available: https://aclanthology.org/P02-1040\\n[149] B. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, and\\nW. Cohen, ‚ÄúHandling divergent reference texts when evaluating\\ntable-to-text generation,‚Äù in Proceedings of the 57th Annual Meeting\\nof the Association for Computational Linguistics , A. Korhonen,\\nD. Traum, and L. M `arquez, Eds. Florence, Italy: Association\\nfor Computational Linguistics, Jul. 2019, pp. 4884‚Äì4895. [Online].\\nAvailable: https://aclanthology.org/P19-1483\\n[150] Z. Wang, X. Wang, B. An, D. Yu, and C. Chen, ‚ÄúTowards faithful\\nneural table-to-text generation with content-matching constraints,‚Äù\\ninProceedings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics , D. Jurafsky, J. Chai, N. Schluter,\\nand J. Tetreault, Eds. Online: Association for Computational\\nLinguistics, Jul. 2020, pp. 1072‚Äì1086. [Online]. Available: https:\\n//aclanthology.org/2020.acl-main.101\\n[151] H. Song, W.-N. Zhang, J. Hu, and T. Liu, ‚ÄúGenerating persona consis-\\ntent dialogues by exploiting natural language inference,‚Äù Proceedings\\nof the AAAI Conference on Artificial Intelligence , vol. 34, no. 05, pp.\\n8878‚Äì8885, Apr. 2020.\\n[152] O. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor,and O. Abend, ‚Äú q2: Evaluating factual consistency in knowledge-\\ngrounded dialogues via question generation and question answering,‚Äù\\ninProceedings of the 2021 Conference on Empirical Methods in\\nNatural Language Processing , M.-F. Moens, X. Huang, L. Specia,\\nand S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic:\\nAssociation for Computational Linguistics, Nov. 2021, pp. 7856‚Äì7870.\\n[Online]. Available: https://aclanthology.org/2021.emnlp-main.619\\n[153] N. Dziri, H. Rashkin, T. Linzen, and D. Reitter, ‚ÄúEvaluating attribution\\nin dialogue systems: The BEGIN benchmark,‚Äù Transactions of the\\nAssociation for Computational Linguistics , vol. 10, pp. 1066‚Äì1083,\\n2022. [Online]. Available: https://aclanthology.org/2022.tacl-1.62\\n[154] S. Santhanam, B. Hedayatnia, S. Gella, A. Padmakumar, S. Kim,\\nY . Liu, and D. Z. Hakkani-T ¬®ur, ‚ÄúRome was built in 1776: A case study\\non factual correctness in knowledge-grounded response generation,‚Äù\\nArXiv , vol. abs/2110.05456, 2021.\\n[155] S. Min, K. Krishna, X. Lyu, M. Lewis, W. tau Yih, P. W. Koh, M. Iyyer,\\nL. Zettlemoyer, and H. Hajishirzi, ‚ÄúFactscore: Fine-grained atomic\\nevaluation of factual precision in long form text generation,‚Äù 2023.\\n[156] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner,\\nV . Chaudhary, and M. Young, ‚ÄúMachine learning: The high interest\\ncredit card of technical debt,‚Äù in SE4ML: Software Engineering for\\nMachine Learning (NIPS 2014 Workshop) , 2014.\\n[157] Z. Zhang, A. Zhang, M. Li, and A. Smola, ‚ÄúAutomatic chain of thought\\nprompting in large language models,‚Äù 2022.\\n[158] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, and\\nK. Narasimhan, ‚ÄúTree of thoughts: Deliberate problem solving with\\nlarge language models,‚Äù 2023.\\n[159] P. Manakul, A. Liusie, and M. J. F. Gales, ‚ÄúSelfcheckgpt: Zero-\\nresource black-box hallucination detection for generative large lan-\\nguage models,‚Äù 2023.\\n[160] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan,\\nand S. Yao, ‚ÄúReflexion: Language agents with verbal reinforcement\\nlearning,‚Äù 2023.\\n[161] S. J. Zhang, S. Florin, A. N. Lee, E. Niknafs, A. Marginean, A. Wang,\\nK. Tyser, Z. Chin, Y . Hicke, N. Singh, M. Udell, Y . Kim, T. Buonassisi,\\nA. Solar-Lezama, and I. Drori, ‚ÄúExploring the mit mathematics and\\neecs curriculum using large language models,‚Äù 2023.\\n[162] T. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J.\\nCai, ‚ÄúPromptchainer: Chaining large language model prompts through\\nvisual programming,‚Äù 2022.\\n[163] Y . Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and\\nJ. Ba, ‚ÄúLarge language models are human-level prompt engineers,‚Äù\\n2023.\\n[164] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin,\\nN. Goyal, H. K ¬®uttler, M. Lewis, W. Yih, T. Rockt ¬®aschel, S. Riedel, and\\nD. Kiela, ‚ÄúRetrieval-augmented generation for knowledge-intensive\\nNLP tasks,‚Äù CoRR , vol. abs/2005.11401, 2020. [Online]. Available:\\nhttps://arxiv.org/abs/2005.11401\\n[165] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, and\\nH. Wang, ‚ÄúRetrieval-augmented generation for large language models:\\nA survey,‚Äù arXiv preprint arXiv:2312.10997 , 2023.\\n[166] A. W. Services. (Year of publication, e.g., 2023) Question answering\\nusing retrieval augmented generation with foundation models in\\namazon sagemaker jumpstart. Accessed: Date of access, e.g.,\\nDecember 5, 2023. [Online]. Available: https://shorturl.at/dSV47\\n[167] S. Pan, L. Luo, Y . Wang, C. Chen, J. Wang, and X. Wu, ‚ÄúUnifying large\\nlanguage models and knowledge graphs: A roadmap,‚Äù arXiv preprint\\narXiv:2306.08302 , 2023.\\n[168] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\\nJ. Callan, and G. Neubig, ‚ÄúActive retrieval augmented generation,‚Äù\\n2023.\\n[169] T. Schick, J. Dwivedi-Yu, R. Dess `ƒ±, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, ‚ÄúToolformer: Language models\\ncan teach themselves to use tools,‚Äù 2023.\\n[170] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer,\\nand M. T. Ribeiro, ‚ÄúArt: Automatic multi-step reasoning and tool-use\\nfor large language models,‚Äù 2023.\\n[171] Y . Shen, K. Song, X. Tan, D. Li, W. Lu, and Y . Zhuang, ‚ÄúHugginggpt:\\nSolving ai tasks with chatgpt and its friends in huggingface,‚Äù arXiv\\npreprint arXiv:2303.17580 , 2023.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 40}, page_content='[172] Z. Xi, W. Chen, X. Guo, W. He, Y . Ding, B. Hong, M. Zhang, J. Wang,\\nS. Jin, E. Zhou et al. , ‚ÄúThe rise and potential of large language model\\nbased agents: A survey,‚Äù arXiv preprint arXiv:2309.07864 , 2023.\\n[173] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\\nJ. Tang, X. Chen, Y . Lin et al. , ‚ÄúA survey on large language model\\nbased autonomous agents,‚Äù arXiv preprint arXiv:2308.11432 , 2023.\\n[174] Z. Durante, Q. Huang, N. Wake, R. Gong, J. S. Park, B. Sarkar,\\nR. Taori, Y . Noda, D. Terzopoulos, Y . Choi, K. Ikeuchi, H. V o, L. Fei-\\nFei, and J. Gao, ‚ÄúAgent ai: Surveying the horizons of multimodal\\ninteraction,‚Äù arXiv preprint arXiv:2401.03568 , 2024.\\n[175] B. Xu, Z. Peng, B. Lei, S. Mukherjee, Y . Liu, and D. Xu, ‚ÄúRewoo:\\nDecoupling reasoning from observations for efficient augmented lan-\\nguage models,‚Äù 2023.\\n[176] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao,\\n‚ÄúReact: Synergizing reasoning and acting in language models,‚Äù 2023.\\n[177] V . Nair, E. Schumacher, G. Tso, and A. Kannan, ‚ÄúDera: Enhanc-\\ning large language model completions with dialog-enabled resolving\\nagents,‚Äù 2023.\\n[178] Y . Chang, X. Wang, J. Wang, Y . Wu, L. Yang, K. Zhu, H. Chen, X. Yi,\\nC. Wang, Y . Wang, W. Ye, Y . Zhang, Y . Chang, P. S. Yu, Q. Yang,\\nand X. Xie, ‚ÄúA survey on evaluation of large language models,‚Äù 2023.\\n[179] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova,\\nL. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit,\\nQ. Le, and S. Petrov, ‚ÄúNatural questions: A benchmark for\\nquestion answering research,‚Äù Transactions of the Association for\\nComputational Linguistics , vol. 7, pp. 452‚Äì466, 2019. [Online].\\nAvailable: https://aclanthology.org/Q19-1026\\n[180] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, ‚ÄúMeasuring massive multitask language understanding,‚Äù\\n2021.\\n[181] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. Cai, M. Terry, Q. Le et al. , ‚ÄúProgram synthesis with large\\nlanguage models,‚Äù arXiv preprint arXiv:2108.07732 , 2021.\\n[182] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y . Choi, P. Liang,\\nand L. Zettlemoyer, ‚ÄúQuAC: Question answering in context,‚Äù in\\nProceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing , E. Riloff, D. Chiang, J. Hockenmaier, and\\nJ. Tsujii, Eds. Brussels, Belgium: Association for Computational\\nLinguistics, Oct.-Nov. 2018, pp. 2174‚Äì2184. [Online]. Available:\\nhttps://aclanthology.org/D18-1241\\n[183] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\\nC. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, ‚ÄúMeasuring\\ncoding challenge competence with apps,‚Äù NeurIPS , 2021.\\n[184] V . Zhong, C. Xiong, and R. Socher, ‚ÄúSeq2sql: Generating structured\\nqueries from natural language using reinforcement learning,‚Äù arXiv\\npreprint arXiv:1709.00103 , 2017.\\n[185] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, ‚ÄúTriviaQA:\\nA large scale distantly supervised challenge dataset for reading\\ncomprehension,‚Äù in Proceedings of the 55th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\\nR. Barzilay and M.-Y . Kan, Eds. Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601‚Äì1611. [Online].\\nAvailable: https://aclanthology.org/P17-1147\\n[186] G. Lai, Q. Xie, H. Liu, Y . Yang, and E. Hovy, ‚ÄúRACE: Large-scale\\nReAding comprehension dataset from examinations,‚Äù in Proceedings\\nof the 2017 Conference on Empirical Methods in Natural Language\\nProcessing , M. Palmer, R. Hwa, and S. Riedel, Eds. Copenhagen,\\nDenmark: Association for Computational Linguistics, Sep. 2017, pp.\\n785‚Äì794. [Online]. Available: https://aclanthology.org/D17-1082\\n[187] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, ‚ÄúSQuAD: 100,000+\\nquestions for machine comprehension of text,‚Äù in Proceedings of\\nthe 2016 Conference on Empirical Methods in Natural Language\\nProcessing , J. Su, K. Duh, and X. Carreras, Eds. Austin, Texas:\\nAssociation for Computational Linguistics, Nov. 2016, pp. 2383‚Äì2392.\\n[Online]. Available: https://aclanthology.org/D16-1264\\n[188] C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and\\nK. Toutanova, ‚ÄúBoolq: Exploring the surprising difficulty of natural\\nyes/no questions,‚Äù CoRR , vol. abs/1905.10044, 2019. [Online].\\nAvailable: http://arxiv.org/abs/1905.10044[189] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth,\\n‚ÄúLooking beyond the surface:a challenge set for reading compre-\\nhension over multiple sentences,‚Äù in Proceedings of North American\\nChapter of the Association for Computational Linguistics (NAACL) ,\\n2018.\\n[190] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and\\nJ. Schulman, ‚ÄúTraining verifiers to solve math word problems,‚Äù\\nCoRR , vol. abs/2110.14168, 2021. [Online]. Available: https:\\n//arxiv.org/abs/2110.14168\\n[191] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,\\nD. Song, and J. Steinhardt, ‚ÄúMeasuring mathematical problem solving\\nwith the MATH dataset,‚Äù CoRR , vol. abs/2103.03874, 2021. [Online].\\nAvailable: https://arxiv.org/abs/2103.03874\\n[192] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, ‚ÄúHellaswag:\\nCan a machine really finish your sentence?‚Äù 2019.\\n[193] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, ‚ÄúThink you have solved question answering? try\\narc, the AI2 reasoning challenge,‚Äù CoRR , vol. abs/1803.05457, 2018.\\n[Online]. Available: http://arxiv.org/abs/1803.05457\\n[194] Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi, ‚ÄúPIQA:\\nreasoning about physical commonsense in natural language,‚Äù CoRR ,\\nvol. abs/1911.11641, 2019. [Online]. Available: http://arxiv.org/abs/\\n1911.11641\\n[195] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y . Choi, ‚ÄúSocialiqa:\\nCommonsense reasoning about social interactions,‚Äù CoRR , vol.\\nabs/1904.09728, 2019. [Online]. Available: http://arxiv.org/abs/1904.\\n09728\\n[196] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, ‚ÄúCan a suit of\\narmor conduct electricity? A new dataset for open book question\\nanswering,‚Äù CoRR , vol. abs/1809.02789, 2018. [Online]. Available:\\nhttp://arxiv.org/abs/1809.02789\\n[197] S. Lin, J. Hilton, and O. Evans, ‚ÄúTruthfulqa: Measuring how models\\nmimic human falsehoods,‚Äù arXiv preprint arXiv:2109.07958 , 2021.\\n[198] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. W. Cohen, R. Salakhutdinov,\\nand C. D. Manning, ‚ÄúHotpotqa: A dataset for diverse, explainable\\nmulti-hop question answering,‚Äù CoRR , vol. abs/1809.09600, 2018.\\n[Online]. Available: http://arxiv.org/abs/1809.09600\\n[199] Y . Zhuang, Y . Yu, K. Wang, H. Sun, and C. Zhang, ‚ÄúToolqa: A\\ndataset for llm question answering with external tools,‚Äù arXiv preprint\\narXiv:2306.13304 , 2023.\\n[200] D. Chen, J. Bolton, and C. D. Manning, ‚ÄúA thorough examination\\nof the cnn/daily mail reading comprehension task,‚Äù in Association for\\nComputational Linguistics (ACL) , 2016.\\n[201] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang et al. , ‚ÄúAbstractive text\\nsummarization using sequence-to-sequence rnns and beyond,‚Äù arXiv\\npreprint arXiv:1602.06023 , 2016.\\n[202] Y . Bai and D. Z. Wang, ‚ÄúMore than reading comprehension: A survey\\non datasets and metrics of textual question answering,‚Äù arXiv preprint\\narXiv:2109.12264 , 2021.\\n[203] H.-Y . Huang, E. Choi, and W.-t. Yih, ‚ÄúFlowqa: Grasping flow in\\nhistory for conversational machine comprehension,‚Äù arXiv preprint\\narXiv:1810.06683 , 2018.\\n[204] S. Lee, J. Lee, H. Moon, C. Park, J. Seo, S. Eo, S. Koo, and H. Lim, ‚ÄúA\\nsurvey on evaluation metrics for machine translation,‚Äù Mathematics ,\\nvol. 11, no. 4, p. 1006, 2023.\\n[205] J. Li, X. Cheng, W. X. Zhao, J.-Y . Nie, and J.-R. Wen, ‚ÄúHalueval:\\nA large-scale hallucination evaluation benchmark for large language\\nmodels,‚Äù in Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing , 2023, pp. 6449‚Äì6464.\\n[206] Simon Mark Hughes, ‚ÄúHughes hallucination evaluation model\\n(hhem) leaderboard,‚Äù 2024, https://huggingface.co/spaces/vectara/\\nHallucination-evaluation-leaderboard, Last accessed on 2024-01-21.\\n[207] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and\\nR. McHardy, ‚ÄúChallenges and applications of large language models,‚Äù\\narXiv preprint arXiv:2307.10169 , 2023.\\n[208] S. Gunasekar, Y . Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno,\\nS. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi et al. ,\\n‚ÄúTextbooks are all you need,‚Äù arXiv preprint arXiv:2306.11644 , 2023.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 41}, page_content='[209] Y . Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y . T.\\nLee, ‚ÄúTextbooks are all you need ii: phi-1.5 technical report,‚Äù arXiv\\npreprint arXiv:2309.05463 , 2023.\\n[210] M. Poli, S. Massaroli, E. Nguyen, D. Y . Fu, T. Dao, S. Baccus,\\nY . Bengio, S. Ermon, and C. R ¬¥e, ‚ÄúHyena hierarchy: Towards larger\\nconvolutional language models,‚Äù 2023.\\n[211] M. Poli, J. Wang, S. Massaroli, J. Quesnelle, E. Nguyen, and\\nA. Thomas, ‚ÄúStripedHyena: Moving Beyond Transformers with\\nHybrid Signal Processing Models,‚Äù 12 2023. [Online]. Available:\\nhttps://github.com/togethercomputer/stripedhyena\\n[212] D. Y . Fu, S. Arora, J. Grogan, I. Johnson, S. Eyuboglu, A. W. Thomas,\\nB. Spector, M. Poli, A. Rudra, and C. R ¬¥e, ‚ÄúMonarch mixer: A simple\\nsub-quadratic gemm-based architecture,‚Äù 2023.\\n[213] G. J. McLachlan, S. X. Lee, and S. I. Rathnayake, ‚ÄúFinite mixture\\nmodels,‚Äù Annual review of statistics and its application , vol. 6, pp.\\n355‚Äì378, 2019.\\n[214] H. Liu, C. Li, Q. Wu, and Y . J. Lee, ‚ÄúVisual instruction tuning,‚Äù arXiv\\npreprint arXiv:2304.08485 , 2023.\\n[215] S. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou,\\nJ. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, ‚ÄúLlava-plus:\\nLearning to use tools for creating multimodal agents,‚Äù arXiv preprint\\narXiv:2311.05437 , 2023.\\n[216] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, ‚ÄúNext-gpt: Any-to-any\\nmultimodal llm,‚Äù arXiv preprint arXiv:2309.05519 , 2023.\\n[217] N. N. Khasmakhi, M. Asgari-Chenaghlu, N. Asghar, P. Schaer, and\\nD. Z ¬®uhlke, ‚ÄúConvgenvismo: Evaluation of conversational generative\\nvision models,‚Äù 2023.\\n[218] N. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya, M. Harman,\\nI. Harper, A. Marginean, S. Sengupta, and E. Wang, ‚ÄúAutomated unit\\ntest improvement using large language models at meta,‚Äù arXiv preprint\\narXiv:2402.09171 , 2024.\\n[219] L. Sun, Y . Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y . Huang,\\nW. Lyu, Y . Zhang, X. Li et al. , ‚ÄúTrustllm: Trustworthiness in large\\nlanguage models,‚Äù arXiv preprint arXiv:2401.05561 , 2024.\\n[220] Microsoft. Deepspeed. [Online]. Available: https://github.com/\\nmicrosoft/DeepSpeed\\n[221] HuggingFace. Transformers. [Online]. Available: https://github.com/\\nhuggingface/transformers\\n[222] Nvidia. Megatron. [Online]. Available: https://github.com/NVIDIA/\\nMegatron-LM\\n[223] BMTrain. Bmtrain. [Online]. Available: https://github.com/OpenBMB/\\nBMTrain\\n[224] EleutherAI. gpt-neox. [Online]. Available: https://github.com/\\nEleutherAI/gpt-neox\\n[225] microsoft. Lora. [Online]. Available: https://github.com/microsoft/\\nLoRA\\n[226] ColossalAI. Colossalai. [Online]. Available: https://github.com/\\nhpcaitech/ColossalAI\\n[227] FastChat. Fastchat. [Online]. Available: https://github.com/lm-sys/\\nFastChat\\n[228] skypilot. skypilot. [Online]. Available: https://github.com/skypilot-org/\\nskypilot\\n[229] vllm. vllm. [Online]. Available: https://github.com/vllm-project/vllm\\n[230] huggingface. text-generation-inference. [Online]. Available: https:\\n//github.com/huggingface/text-generation-inference\\n[231] langchain. langchain. [Online]. Available: https://github.com/\\nlangchain-ai/langchain\\n[232] bentoml. Openllm. [Online]. Available: https://github.com/bentoml/\\nOpenLLM\\n[233] embedchain. embedchain. [Online]. Available: https://github.com/\\nembedchain/embedchain\\n[234] microsoft. autogen. [Online]. Available: https://github.com/microsoft/\\nautogen\\n[235] babyagi. babyagi. [Online]. Available: https://github.com/\\nyoheinakajima/babyagi\\n[236] guidance. guidance. [Online]. Available: https://github.com/\\nguidance-ai/guidance[237] prompttools. prompttools. [Online]. Available: https://github.com/\\nhegelai/prompttools\\n[238] promptfoo. promptfoo. [Online]. Available: https://github.com/\\npromptfoo/promptfoo\\n[239] facebook. faiss. [Online]. Available: https://github.com/\\nfacebookresearch/faiss\\n[240] milvus. milvus. [Online]. Available: https://github.com/milvus-io/\\nmilvus\\n[241] qdrant. qdrant. [Online]. Available: https://github.com/qdrant/qdrant\\n[242] weaviate. weaviate. [Online]. Available: https://github.com/weaviate/\\nweaviate\\n[243] llama index. llama-index. [Online]. Available: https://github.com/\\nrun-llama/llama index\\nAPPENDIX\\n1. Open Source Toolkits For LLM Development and\\nDeployment\\nThere are various frameworks and libraries developed for\\nLLM training, evaluation, and deployment, and covering every\\nsingle framework is out of this paper‚Äôs scope. But we try to\\nprovide a brief introduction of some of the most popular ones,\\ngrouped into different categories.\\nA. LLM Training/Inference Frameworks\\nSome of the popular frameworks which are useful for LLM\\ntraining includes (note that some of them can be used beyond\\nLLM training too):\\nDeepSpeed [220] is a deep learning optimization library\\nthat makes distributed training and inference easy, efficient,\\nand effective. DeepSpeed enables world‚Äôs most powerful lan-\\nguage models like MT-530B and BLOOM. It is an easy-\\nto-use deep learning optimization software suite that powers\\nunprecedented scale and speed for both training and inference.\\nWith DeepSpeed you can:\\nTransformers [221] is library by HuggingFace which\\nprovides thousands of pretrained models to perform tasks on\\ndifferent modalities such as text, vision, and audio. Using\\npretrained models one can reduce compute costs, carbon\\nfootprint, and save the time and resources required to train\\na model from scratch.\\nMegatron-LM [222] is a large, powerful transformer\\ndeveloped by the Applied Deep Learning Research team\\nat NVIDIA. It contains efficient, model-parallel (tensor, se-\\nquence, and pipeline), and multi-node pre-training of trans-\\nformer based models such as GPT, BERT, and T5 using mixed\\nprecision.\\nBMTrain [223] is an efficient large model training toolkit\\nthat can be used to train large models with tens of billions of\\nparameters. It can train models in a distributed manner while\\nkeeping the code as simple as stand-alone training.\\nGPT-NeoX [224] leverages many of the same features and\\ntechnologies as the popular Megatron-DeepSpeed library but\\nwith substantially increased usability and novel optimizations.\\nLoRA [225] library provides the support for Low-Rank\\nAdaptation of Large Language Models. It reduces the number\\nof trainable parameters by learning pairs of rank-decompostion\\nmatrices while freezing the original weights. This vastly'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/2402.06196v2.pdf', 'page': 42}, page_content='reduces the storage requirement for large language models\\nadapted to specific tasks and enables efficient task-switching\\nduring deployment all without introducing inference latency.\\nLoRA also outperforms several other adaptation methods in-\\ncluding adapter, prefix-tuning, and fine-tuning.\\nColossalAI library [226] provides a collection of parallel\\ncomponents. It aims to support developers to write their\\ndistributed deep learning models just like how they write their\\nmodel on their laptop. They provide user-friendly tools to\\nkickstart distributed training and inference in a few lines. In\\nterms of Parallelism strategies, they support: Data Parallelism,\\nPipeline Parallelism, Sequence Parallelism, Zero Redundancy\\nOptimizer (ZeRO) [140], and Auto-Parallelism.\\nB. Deployment Tools\\nWe provide an overview of some of the most popular LLM\\ndeployment tools here.\\nFastChat [227] is an open platform for training, serv-\\ning, and evaluating large language model based chatbots.\\nFastChat‚Äôs core features include: The training and evaluation\\ncode for state-of-the-art models (e.g., Vicuna, MT-Bench), and\\na distributed multi-model serving system with web UI and\\nOpenAI-compatible RESTful APIs.\\nSkypilot [228] is a framework for running LLMs, AI,\\nand batch jobs on any cloud, offering maximum cost savings,\\nhighest GPU availability, and managed execution.\\nvLLM [229] is a fast and easy-to-use library for LLM in-\\nference and serving. vLLM seamlessly supports many Hugging\\nFace models, including the following architectures: Aquila,\\nBaichuan, BLOOM, ChatGLM, DeciLM, Falcon, GPT Big-\\nCode, LLaMA, LLaMA 2, Mistral, Mixtral, MPT, OPT, Qwen,\\nYi, and many more.\\ntext-generation-inference [230] is a toolkit for deploying\\nand serving Large Language Models (LLMs). TGI enables\\nhigh-performance text generation for the most popular open-\\nsource LLMs, including Llama, Falcon, StarCoder, BLOOM,\\nGPT-NeoX, and more.\\nLangChain [231] is a framework for developing applica-\\ntions powered by language models. It enables applications that:\\n‚Ä¢ Are context-aware: connect a language model to\\nsources of context (prompt instructions, few shot ex-\\namples, content to ground its response in, etc.)\\n‚Ä¢ Reason: rely on a language model to reason (about\\nhow to answer based on provided context, what ac-\\ntions to take, etc.)\\nOpenLLM [232] is an open-source platform designed to\\nfacilitate the deployment and operation of large language mod-\\nels (LLMs) in real-world applications. With OpenLLM, you\\ncan run inference on any open-source LLM, deploy them on\\nthe cloud or on-premises, and build powerful AI applications.\\nEmbedchain [233] is an Open Source RAG Framework\\nthat makes it easy to create and deploy AI apps. Embedchain\\nstreamlines the creation of RAG applications, offering a seam-\\nless process for managing various types of unstructured data.\\nIt efficiently segments data into manageable chunks, generatesrelevant embeddings, and stores them in a vector database for\\noptimized retrieval.\\nAutogen [234] is a framework that enables the devel-\\nopment of LLM applications using multiple agents that can\\nconverse with each other to solve tasks. AutoGen agents\\nare customizable, conversable, and seamlessly allow human\\nparticipation. They can operate in various modes that employ\\ncombinations of LLMs, human inputs, and tools.\\nBabyAGI [235] is an autonomous Artificial Intelligence\\nagent, that is designed to generate and execute tasks based on\\ngiven objectives. It harnesses cutting-edge technologies from\\nOpenAI, Pinecone, LangChain, and Chroma to automate tasks\\nand achieve specific goals. In this blog post, we will dive\\ninto the unique features of BabyAGI and explore how it can\\nstreamline task automation.\\nC. Prompting Libraries\\nGuidance [236] is a programming paradigm that offers\\nsuperior control and efficiency compared to conventional\\nprompting and chaining. It allows users to constrain generation\\n(e.g. with regex and CFGs) as well as to interleave control\\n(conditional, loops) and generation seamlessly.\\nPromptTools [237] offers a set of open-source, self-\\nhostable tools for experimenting with, testing, and evaluating\\nLLMs, vector databases, and prompts. The core idea is to\\nenable developers to evaluate using familiar interfaces like\\ncode, notebooks, and a local playground.\\nPromptBench [?] is a Pytorch-based Python package for\\nEvaluation of Large Language Models (LLMs). It provides\\nuser-friendly APIs for researchers to conduct evaluation on\\nLLMs.\\nPromptfoo [238] is a tool for testing and evaluating LLM\\noutput quality. It systematically test prompts, models, and\\nRAGs with predefined test cases.\\nD. VectorDB\\nFaiss [239] is a library developed by Facebook AI Re-\\nsearch that provides efficient similarity search and clustering\\nof dense vectors. It is designed for use with large-scale,\\nhigh-dimensional data and supports several index types and\\nalgorithms for various use cases.\\nMilvus [240] is an open-source vector database built to\\npower embedding similarity search and AI applications. Mil-\\nvus makes unstructured data search more accessible, and pro-\\nvides a consistent user experience regardless of the deployment\\nenvironment.\\nQdrant [241] is a vector similarity search engine and\\nvector database. It provides a production-ready service with a\\nconvenient API to store, search, and manage points‚Äîvectors\\nwith an additional payload Qdrant is tailored to extended\\nfiltering support. environment.\\nWeaviate [242] is an open-source, GraphQL-based vec-\\ntor search engine that enables similarity search on high-\\ndimensional data. While it is open-source, the commercial ver-\\nsion offers additional features, support, and managed services.\\nSome of the other popular options includes LlamaIndex\\n[243] and Pinecone .'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Mask rcnn and cnns.pdf', 'page': 0}, page_content='Mask R-CNN\\nKaiming He Georgia Gkioxari Piotr Doll ¬¥ar Ross Girshick\\nFacebook AI Research (FAIR)\\nAbstract\\nWe present a conceptually simple, Ô¨Çexible, and general\\nframework for object instance segmentation. Our approach\\nefÔ¨Åciently detects objects in an image while simultaneously\\ngenerating a high-quality segmentation mask for each in-\\nstance. The method, called Mask R-CNN, extends Faster\\nR-CNN by adding a branch for predicting an object mask in\\nparallel with the existing branch for bounding box recogni-\\ntion. Mask R-CNN is simple to train and adds only a small\\noverhead to Faster R-CNN, running at 5 fps. Moreover,\\nMask R-CNN is easy to generalize to other tasks, e.g., al-\\nlowing us to estimate human poses in the same framework.\\nWe show top results in all three tracks of the COCO suite\\nof challenges, including instance segmentation, bounding-\\nbox object detection, and person keypoint detection. With-\\nout bells and whistles, Mask R-CNN outperforms all ex-\\nisting, single-model entries on every task, including the\\nCOCO 2016 challenge winners. We hope our simple and\\neffective approach will serve as a solid baseline and help\\nease future research in instance-level recognition. Code\\nhas been made available at: https://github.com/\\nfacebookresearch/Detectron .\\n1. Introduction\\nThe vision community has rapidly improved object de-\\ntection and semantic segmentation results over a short pe-\\nriod of time. In large part, these advances have been driven\\nby powerful baseline systems, such as the Fast/Faster R-\\nCNN [12, 36] and Fully Convolutional Network (FCN) [30]\\nframeworks for object detection and semantic segmenta-\\ntion, respectively. These methods are conceptually intuitive\\nand offer Ô¨Çexibility and robustness, together with fast train-\\ning and inference time. Our goal in this work is to develop a\\ncomparably enabling framework for instance segmentation .\\nInstance segmentation is challenging because it requires\\nthe correct detection of all objects in an image while also\\nprecisely segmenting each instance. It therefore combines\\nelements from the classical computer vision tasks of ob-\\nject detection , where the goal is to classify individual ob-\\njects and localize each using a bounding box, and semantic\\nRoIAlignRoIAlignclass\\nbox\\nconvconv\\n convconvFigure 1. The Mask R-CNN framework for instance segmentation.\\nsegmentation , where the goal is to classify each pixel into\\na Ô¨Åxed set of categories without differentiating object in-\\nstances.1Given this, one might expect a complex method\\nis required to achieve good results. However, we show that\\na surprisingly simple, Ô¨Çexible, and fast system can surpass\\nprior state-of-the-art instance segmentation results.\\nOur method, called Mask R-CNN , extends Faster R-CNN\\n[36] by adding a branch for predicting segmentation masks\\non each Region of Interest (RoI), in parallel with the ex-\\nisting branch for classiÔ¨Åcation and bounding box regres-\\nsion (Figure 1). The mask branch is a small FCN applied\\nto each RoI, predicting a segmentation mask in a pixel-to-\\npixel manner. Mask R-CNN is simple to implement and\\ntrain given the Faster R-CNN framework, which facilitates\\na wide range of Ô¨Çexible architecture designs. Additionally,\\nthe mask branch only adds a small computational overhead,\\nenabling a fast system and rapid experimentation.\\nIn principle Mask R-CNN is an intuitive extension of\\nFaster R-CNN, yet constructing the mask branch properly\\nis critical for good results. Most importantly, Faster R-\\nCNN was not designed for pixel-to-pixel alignment be-\\ntween network inputs and outputs. This is most evident in\\nhow RoIPool [18, 12], the de facto core operation for at-\\ntending to instances, performs coarse spatial quantization\\nfor feature extraction. To Ô¨Åx the misalignment, we pro-\\npose a simple, quantization-free layer, called RoIAlign , that\\nfaithfully preserves exact spatial locations. Despite being\\n1Following common terminology, we use object detection to denote\\ndetection via bounding boxes , not masks, and semantic segmentation to\\ndenote per-pixel classiÔ¨Åcation without differentiating instances. Yet we\\nnote that instance segmentation is both semantic and a form of detection.\\n1arXiv:1703.06870v3  [cs.CV]  24 Jan 2018'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Mask rcnn and cnns.pdf', 'page': 1}, page_content='dining table.96person1.00person1.00 person1.00 person1.00 person1.00person1.00\\nperson1.00person.94\\nbottle.99\\nbottle.99\\nbottle.99\\nmotorcycle1.00motorcycle1.00person1.00\\nperson1.00\\nperson.96person1.00person.83person.96\\nperson.98 person.90person.92person.99person.91\\nbus.99\\nperson1.00\\nperson1.00person1.00\\nbackpack.93person1.00person.99\\nperson1.00\\nbackpack.99\\nperson.99person.98person.89person.95\\nperson1.00\\nperson1.00\\ncar1.00traffic light .96\\nperson.96truck1.00person.99\\ncar.99person.85\\nmotorcycle.95car.99car.92person.99person1.00traffic light .92 traffic light .84traffic light .95\\ncar.93person.87\\nperson1.00\\nperson1.00umbrella.98\\numbrella.98\\nbackpack1.00\\nhandba g.96\\nelephant1.00person1.00person1.00 person.99\\nsheep1. 00person1.00\\nsheep.99sheep.91 sheep1. 00\\nsheep.99sheep.99sheep.95person.99\\nsheep1. 00sheep.96sheep.99sheep.99\\nsheep.96\\nsheep.96sheep.96sheep.86\\nsheep.82sheep.93\\ndining table.99\\nchair.99chair.90\\nchair.99chair.98\\nchair.96chair.86chair.99\\nbowl.81chair.96tv.99\\nbottle.99\\nwine glass.99wine glass1.00bowl.85\\nknife.83wine glass1.00 wine glass.93wine glass.97\\nfork.95Figure 2. Mask R-CNN results on the COCO test set. These results are based on ResNet-101 [19], achieving a mask AP of 35.7 and\\nrunning at 5 fps. Masks are shown in color, and bounding box, category, and conÔ¨Ådences are also shown.\\na seemingly minor change, RoIAlign has a large impact: it\\nimproves mask accuracy by relative 10% to 50%, showing\\nbigger gains under stricter localization metrics. Second, we\\nfound it essential to decouple mask and class prediction: we\\npredict a binary mask for each class independently, without\\ncompetition among classes, and rely on the network‚Äôs RoI\\nclassiÔ¨Åcation branch to predict the category. In contrast,\\nFCNs usually perform per-pixel multi-class categorization,\\nwhich couples segmentation and classiÔ¨Åcation, and based\\non our experiments works poorly for instance segmentation.\\nWithout bells and whistles, Mask R-CNN surpasses all\\nprevious state-of-the-art single-model results on the COCO\\ninstance segmentation task [28], including the heavily-\\nengineered entries from the 2016 competition winner. As\\na by-product, our method also excels on the COCO object\\ndetection task. In ablation experiments, we evaluate multi-\\nple basic instantiations, which allows us to demonstrate its\\nrobustness and analyze the effects of core factors.\\nOur models can run at about 200ms per frame on a GPU,\\nand training on COCO takes one to two days on a single\\n8-GPU machine. We believe the fast train and test speeds,\\ntogether with the framework‚Äôs Ô¨Çexibility and accuracy, will\\nbeneÔ¨Åt and ease future research on instance segmentation.\\nFinally, we showcase the generality of our framework\\nvia the task of human pose estimation on the COCO key-\\npoint dataset [28]. By viewing each keypoint as a one-hot\\nbinary mask, with minimal modiÔ¨Åcation Mask R-CNN can\\nbe applied to detect instance-speciÔ¨Åc poses. Mask R-CNN\\nsurpasses the winner of the 2016 COCO keypoint compe-\\ntition, and at the same time runs at 5 fps. Mask R-CNN,\\ntherefore, can be seen more broadly as a Ô¨Çexible framework\\nforinstance-level recognition and can be readily extended\\nto more complex tasks.\\nWe have released code to facilitate future research.2. Related Work\\nR-CNN: The Region-based CNN (R-CNN) approach [13]\\nto bounding-box object detection is to attend to a manage-\\nable number of candidate object regions [42, 20] and evalu-\\nate convolutional networks [25, 24] independently on each\\nRoI. R-CNN was extended [18, 12] to allow attending to\\nRoIs on feature maps using RoIPool, leading to fast speed\\nand better accuracy. Faster R-CNN [36] advanced this\\nstream by learning the attention mechanism with a Region\\nProposal Network (RPN). Faster R-CNN is Ô¨Çexible and ro-\\nbust to many follow-up improvements ( e.g., [38, 27, 21]),\\nand is the current leading framework in several benchmarks.\\nInstance Segmentation: Driven by the effectiveness of R-\\nCNN, many approaches to instance segmentation are based\\nonsegment proposals . Earlier methods [13, 15, 16, 9] re-\\nsorted to bottom-up segments [42, 2]. DeepMask [33] and\\nfollowing works [34, 8] learn to propose segment candi-\\ndates, which are then classiÔ¨Åed by Fast R-CNN. In these\\nmethods, segmentation precedes recognition, which is slow\\nand less accurate. Likewise, Dai et al. [10] proposed a com-\\nplex multiple-stage cascade that predicts segment proposals\\nfrom bounding-box proposals, followed by classiÔ¨Åcation.\\nInstead, our method is based on parallel prediction of masks\\nand class labels, which is simpler and more Ô¨Çexible.\\nMost recently, Li et al. [26] combined the segment pro-\\nposal system in [8] and object detection system in [11] for\\n‚Äúfully convolutional instance segmentation‚Äù (FCIS). The\\ncommon idea in [8, 11, 26] is to predict a set of position-\\nsensitive output channels fully convolutionally. These\\nchannels simultaneously address object classes, boxes, and\\nmasks, making the system fast. But FCIS exhibits system-\\natic errors on overlapping instances and creates spurious\\nedges (Figure 6), showing that it is challenged by the fun-\\ndamental difÔ¨Åculties of segmenting instances.\\n2'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Mask rcnn and cnns.pdf', 'page': 2}, page_content='Another family of solutions [23, 4, 3, 29] to instance seg-\\nmentation are driven by the success of semantic segmen-\\ntation. Starting from per-pixel classiÔ¨Åcation results ( e.g.,\\nFCN outputs), these methods attempt to cut the pixels of\\nthe same category into different instances. In contrast to the\\nsegmentation-Ô¨Årst strategy of these methods, Mask R-CNN\\nis based on an instance-Ô¨Årst strategy. We expect a deeper in-\\ncorporation of both strategies will be studied in the future.\\n3. Mask R-CNN\\nMask R-CNN is conceptually simple: Faster R-CNN has\\ntwo outputs for each candidate object, a class label and a\\nbounding-box offset; to this we add a third branch that out-\\nputs the object mask. Mask R-CNN is thus a natural and in-\\ntuitive idea. But the additional mask output is distinct from\\nthe class and box outputs, requiring extraction of much Ô¨Åner\\nspatial layout of an object. Next, we introduce the key ele-\\nments of Mask R-CNN, including pixel-to-pixel alignment,\\nwhich is the main missing piece of Fast/Faster R-CNN.\\nFaster R-CNN: We begin by brieÔ¨Çy reviewing the Faster\\nR-CNN detector [36]. Faster R-CNN consists of two stages.\\nThe Ô¨Årst stage, called a Region Proposal Network (RPN),\\nproposes candidate object bounding boxes. The second\\nstage, which is in essence Fast R-CNN [12], extracts fea-\\ntures using RoIPool from each candidate box and performs\\nclassiÔ¨Åcation and bounding-box regression. The features\\nused by both stages can be shared for faster inference. We\\nrefer readers to [21] for latest, comprehensive comparisons\\nbetween Faster R-CNN and other frameworks.\\nMask R-CNN: Mask R-CNN adopts the same two-stage\\nprocedure, with an identical Ô¨Årst stage (which is RPN). In\\nthe second stage, in parallel to predicting the class and box\\noffset, Mask R-CNN also outputs a binary mask for each\\nRoI. This is in contrast to most recent systems, where clas-\\nsiÔ¨Åcation depends on mask predictions ( e.g. [33, 10, 26]).\\nOur approach follows the spirit of Fast R-CNN [12] that\\napplies bounding-box classiÔ¨Åcation and regression in par-\\nallel (which turned out to largely simplify the multi-stage\\npipeline of original R-CNN [13]).\\nFormally, during training, we deÔ¨Åne a multi-task loss on\\neach sampled RoI as L=Lcls+Lbox+Lmask . The clas-\\nsiÔ¨Åcation loss Lclsand bounding-box loss Lboxare identi-\\ncal as those deÔ¨Åned in [12]. The mask branch has a Km2-\\ndimensional output for each RoI, which encodes Kbinary\\nmasks of resolution m√óm, one for each of the Kclasses.\\nTo this we apply a per-pixel sigmoid, and deÔ¨Åne Lmask as\\nthe average binary cross-entropy loss. For an RoI associated\\nwith ground-truth class k,Lmask is only deÔ¨Åned on the k-th\\nmask (other mask outputs do not contribute to the loss).\\nOur deÔ¨Ånition of Lmask allows the network to generate\\nmasks for every class without competition among classes;\\nwe rely on the dedicated classiÔ¨Åcation branch to predict the\\nFigure 3. RoIAlign: The dashed grid rep-\\nresents a feature map, the solid lines an RoI\\n(with 2√ó2 bins in this example), and the dots\\nthe 4 sampling points in each bin. RoIAlign\\ncomputes the value of each sampling point\\nby bilinear interpolation from the nearby grid\\npoints on the feature map. No quantization is\\nperformed on any coordinates involved in the\\nRoI, its bins, or the sampling points.\\nclass label used to select the output mask. This decouples\\nmask and class prediction. This is different from common\\npractice when applying FCNs [30] to semantic segmenta-\\ntion, which typically uses a per-pixel softmax and a multino-\\nmial cross-entropy loss. In that case, masks across classes\\ncompete; in our case, with a per-pixel sigmoid and a binary\\nloss, they do not. We show by experiments that this formu-\\nlation is key for good instance segmentation results.\\nMask Representation: A mask encodes an input object‚Äôs\\nspatial layout. Thus, unlike class labels or box offsets\\nthat are inevitably collapsed into short output vectors by\\nfully-connected ( fc) layers, extracting the spatial structure\\nof masks can be addressed naturally by the pixel-to-pixel\\ncorrespondence provided by convolutions.\\nSpeciÔ¨Åcally, we predict an m√ómmask from each RoI\\nusing an FCN [30]. This allows each layer in the mask\\nbranch to maintain the explicit m√ómobject spatial lay-\\nout without collapsing it into a vector representation that\\nlacks spatial dimensions. Unlike previous methods that re-\\nsort to fclayers for mask prediction [33, 34, 10], our fully\\nconvolutional representation requires fewer parameters, and\\nis more accurate as demonstrated by experiments.\\nThis pixel-to-pixel behavior requires our RoI features,\\nwhich themselves are small feature maps, to be well aligned\\nto faithfully preserve the explicit per-pixel spatial corre-\\nspondence. This motivated us to develop the following\\nRoIAlign layer that plays a key role in mask prediction.\\nRoIAlign: RoIPool [12] is a standard operation for extract-\\ning a small feature map ( e.g., 7√ó7) from each RoI. RoIPool\\nÔ¨Årstquantizes a Ô¨Çoating-number RoI to the discrete granu-\\nlarity of the feature map, this quantized RoI is then subdi-\\nvided into spatial bins which are themselves quantized, and\\nÔ¨Ånally feature values covered by each bin are aggregated\\n(usually by max pooling). Quantization is performed, e.g.,\\non a continuous coordinate xby computing [x/16], where\\n16 is a feature map stride and [¬∑]is rounding; likewise, quan-\\ntization is performed when dividing into bins ( e.g., 7√ó7).\\nThese quantizations introduce misalignments between the\\nRoI and the extracted features. While this may not impact\\nclassiÔ¨Åcation, which is robust to small translations, it has a\\nlarge negative effect on predicting pixel-accurate masks.\\nTo address this, we propose an RoIAlign layer that re-\\nmoves the harsh quantization of RoIPool, properly aligning\\nthe extracted features with the input. Our proposed change\\nis simple: we avoid any quantization of the RoI boundaries\\n3'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Mask rcnn and cnns.pdf', 'page': 3}, page_content='or bins ( i.e., we use x/16instead of [x/16]). We use bi-\\nlinear interpolation [22] to compute the exact values of the\\ninput features at four regularly sampled locations in each\\nRoI bin, and aggregate the result (using max or average),\\nsee Figure 3 for details. We note that the results are not sen-\\nsitive to the exact sampling locations, or how many points\\nare sampled, as long as no quantization is performed.\\nRoIAlign leads to large improvements as we show in\\n¬ß4.2. We also compare to the RoIWarp operation proposed\\nin [10]. Unlike RoIAlign, RoIWarp overlooked the align-\\nment issue and was implemented in [10] as quantizing RoI\\njust like RoIPool. So even though RoIWarp also adopts\\nbilinear resampling motivated by [22], it performs on par\\nwith RoIPool as shown by experiments (more details in Ta-\\nble 2c), demonstrating the crucial role of alignment.\\nNetwork Architecture: To demonstrate the generality of\\nour approach, we instantiate Mask R-CNN with multiple\\narchitectures. For clarity, we differentiate between: (i) the\\nconvolutional backbone architecture used for feature ex-\\ntraction over an entire image, and (ii) the network head\\nfor bounding-box recognition (classiÔ¨Åcation and regression)\\nand mask prediction that is applied separately to each RoI.\\nWe denote the backbone architecture using the nomen-\\nclature network-depth-features . We evaluate ResNet [19]\\nand ResNeXt [45] networks of depth 50 or 101 layers. The\\noriginal implementation of Faster R-CNN with ResNets\\n[19] extracted features from the Ô¨Ånal convolutional layer\\nof the 4-th stage, which we call C4. This backbone with\\nResNet-50, for example, is denoted by ResNet-50-C4. This\\nis a common choice used in [19, 10, 21, 39].\\nWe also explore another more effective backbone re-\\ncently proposed by Lin et al. [27], called a Feature Pyra-\\nmid Network (FPN). FPN uses a top-down architecture with\\nlateral connections to build an in-network feature pyramid\\nfrom a single-scale input. Faster R-CNN with an FPN back-\\nbone extracts RoI features from different levels of the fea-\\nture pyramid according to their scale, but otherwise the\\nrest of the approach is similar to vanilla ResNet. Using a\\nResNet-FPN backbone for feature extraction with Mask R-\\nCNN gives excellent gains in both accuracy and speed. For\\nfurther details on FPN, we refer readers to [27].\\nFor the network head we closely follow architectures\\npresented in previous work to which we add a fully con-\\nvolutional mask prediction branch. SpeciÔ¨Åcally, we ex-\\ntend the Faster R-CNN box heads from the ResNet [19]\\nand FPN [27] papers. Details are shown in Figure 4. The\\nhead on the ResNet-C4 backbone includes the 5-th stage of\\nResNet (namely, the 9-layer ‚Äòres5‚Äô [19]), which is compute-\\nintensive. For FPN, the backbone already includes res5 and\\nthus allows for a more efÔ¨Åcient head that uses fewer Ô¨Ålters.\\nWe note that our mask branches have a straightforward\\nstructure. More complex designs have the potential to im-\\nprove performance but are not the focus of this work.\\nave\\nRoI\\nRoI14√ó14\\n√ó2567√ó7\\n√ó256\\n14√ó14\\n√ó2561024\\n28√ó28\\n√ó2561024\\nmask14√ó14\\n√ó256class\\nbox2048RoI res57√ó7\\n√ó10247√ó7\\n√ó2048\\n√ó4class\\nbox\\n14√ó14\\n√ó80\\nmask28√ó28\\n√ó80Faster R-CNN\\nw/ ResNet [19]Faster R-CNN\\nw/ FPN [27]\\nFigure 4. Head Architecture : We extend two existing Faster R-\\nCNN heads [19, 27]. Left/Right panels show the heads for the\\nResNet C4 and FPN backbones, from [19] and [27], respectively,\\nto which a mask branch is added. Numbers denote spatial resolu-\\ntion and channels. Arrows denote either conv, deconv, or fclayers\\nas can be inferred from context (conv preserves spatial dimension\\nwhile deconv increases it). All convs are 3 √ó3, except the output\\nconv which is 1 √ó1, deconvs are 2 √ó2 with stride 2, and we use\\nReLU [31] in hidden layers. Left: ‚Äòres5‚Äô denotes ResNet‚Äôs Ô¨Åfth\\nstage, which for simplicity we altered so that the Ô¨Årst conv oper-\\nates on a 7 √ó7 RoI with stride 1 (instead of 14 √ó14 / stride 2 as in\\n[19]). Right : ‚Äò√ó4‚Äô denotes a stack of four consecutive convs.\\n3.1. Implementation Details\\nWe set hyper-parameters following existing Fast/Faster\\nR-CNN work [12, 36, 27]. Although these decisions were\\nmade for object detection in original papers [12, 36, 27], we\\nfound our instance segmentation system is robust to them.\\nTraining: As in Fast R-CNN, an RoI is considered positive\\nif it has IoU with a ground-truth box of at least 0.5 and\\nnegative otherwise. The mask loss Lmask is deÔ¨Åned only on\\npositive RoIs. The mask target is the intersection between\\nan RoI and its associated ground-truth mask.\\nWe adopt image-centric training [12]. Images are resized\\nsuch that their scale (shorter edge) is 800 pixels [27]. Each\\nmini-batch has 2 images per GPU and each image has N\\nsampled RoIs, with a ratio of 1:3 of positive to negatives\\n[12]. Nis 64 for the C4 backbone (as in [12, 36]) and 512\\nfor FPN (as in [27]). We train on 8 GPUs (so effective mini-\\nbatch size is 16) for 160k iterations, with a learning rate of\\n0.02 which is decreased by 10 at the 120k iteration. We\\nuse a weight decay of 0.0001 and momentum of 0.9. With\\nResNeXt [45], we train with 1 image per GPU and the same\\nnumber of iterations, with a starting learning rate of 0.01.\\nThe RPN anchors span 5 scales and 3 aspect ratios, fol-\\nlowing [27]. For convenient ablation, RPN is trained sep-\\narately and does not share features with Mask R-CNN, un-\\nless speciÔ¨Åed. For every entry in this paper, RPN and Mask\\nR-CNN have the same backbones and so they are shareable.\\nInference: At test time, the proposal number is 300 for the\\nC4 backbone (as in [36]) and 1000 for FPN (as in [27]). We\\nrun the box prediction branch on these proposals, followed\\nby non-maximum suppression [14]. The mask branch is\\nthen applied to the highest scoring 100 detection boxes. Al-\\nthough this differs from the parallel computation used in\\ntraining, it speeds up inference and improves accuracy (due\\nto the use of fewer, more accurate RoIs). The mask branch\\n4'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Mask rcnn and cnns.pdf', 'page': 4}, page_content='horse1.00horse1.00horse1.00\\nbus1.00bus1.00\\ncar.98truck.88\\ncar.93\\ncar.78car.98\\ncar.91 car.96car.99\\ncar.94car.99car.98 truck.86\\ncar.99car.95car1.00car.93 car.98car.95\\ncar.97car.87\\ncar.99\\ncar.82car.78car.93\\ncar.95\\ncar.97\\nperson.99traffic light .73\\nperson1.00\\nperson.99person.95person.93\\nperson.93\\nperson1.00person.98\\nskateboard.82\\nsuitcase1.00\\nsuitcase.99suitcase.96suitcase1.00\\nsuitcase.93suitcase.98\\nsuitcase.88suitcase.72stop sign.88\\nperson1.00 person1.00\\nperson1.00person1.00\\nperson.99person.99\\nbench.76skateboard.91\\nskateboard.83handba g.81\\nsurfboard1.00person1.00person1.00 surfboard1.00person1.00person.98\\nsurfboard1.00person1.00\\nsurfboard.98surfboard1.00person.91\\nperson.74\\nperson1.00person1.00\\nperson1.00person1.00person1.00 person1.00 person.98person.99\\nperson1.00person.99 umbrella1.00\\nperson.95umbrella.99umbrella.97umbrella.97\\numbrella.96\\numbrella1.00\\nbackpack.96umbrella.98\\nbackpack.95person.80\\nbackpack.98\\nbicycle.93umbrella.89person.89handba g.97\\nhandba g.85\\nperson1.00person1.00 person1.00person1.00person1.00person1.00\\nmotorcycle.72kite.89\\nperson.99kite.95\\nperson.99\\nperson1.00person.81person.72kite.93\\nperson.89kite1.00\\nperson.98\\nperson1.00kite.84kite.97\\nperson.80\\nhandba g.80person.99kite.82\\nperson.98 person.96kite.98\\nperson.99person.82kite.81\\nperson.95 person.84kite.98kite.72\\nkite.99kite.84kite.99\\nperson.94person.72person.98kite.95\\nperson.98 person.77kite.73\\nperson.78 person.71 person.87kite.88kite.88\\nperson.94kite.86kite.89\\nzebra.99\\nzebra1.00zebra1.00\\nzebra.99zebra1.00zebra.96\\nzebra.74\\nzebra.96zebra.99zebra.90\\nzebra.88zebra.76\\ndining table.91dining table.78\\nchair.97person.99\\nperson.86\\nchair.94chair.98person.95\\nchair.95person.97\\nchair.92chair.99person.97\\nperson.99person.94 person.99person.87\\nperson.99\\nchair.83person.94person.99person.98\\nchair.87chair.95person.97\\nperson.96\\nchair.99person.86person.89\\nchair.89\\nwine glass.93person.98person.88person.97\\nperson.88person.88\\nperson.91 chair.96person.95\\nperson.77person.92\\nwine glass.94cup.83\\nwine glass.94\\nwine glass.83\\ncup.91chair.85 dining table.96\\nwine glass.91person.96\\ncup.98person.83\\ndining table.75\\ncup.96person.72\\nwine glass.80chair.98person.81person.82\\ndining table.81\\nchair.85chair.78\\ncup.75person.77\\ncup.71wine glass.80cup.79cup.93\\ncup.71\\nperson.99person.99\\nperson1.00person1.00\\nfrisbee1.00\\nperson.80person.82\\nelephant1.00elephant1.00elephant1.00\\nelephant.97\\nelephant.99\\nperson1.00person1.00\\ndining table.95person1.00person.88\\nwine glass1.00bottle.97\\nwine glass1.00wine glass.99tv.98 tv.84\\nperson1.00\\nbench.97person.98person1.00person1.00\\nhandba g.73person.86 potted plant.92\\nbird.93person.76person.98person.78 person.78 backpack.88handba g.91\\ncell phone.77 clock.73\\nperson.99person1.00person.98\\nperson1.00person1.00 person1.00person.99\\nperson.99 person.99 person1.00 person1.00person.98 person.99\\nhandba g.88person1.00 person.98person.92\\nhandba g.99person.97\\nperson.95\\nhandba g.88traffic light .99\\nperson.95\\nperson.87person.95traffic light .87\\ntraffic light .71\\nperson.80person.95 person.95 person.73person.74\\ntie.85\\ncar.99\\ncar.86car.97car1.00 car.95car.97traffic light 1.00traffic light .99\\ncar.99person.99car.95\\ncar.97 car.98car.98\\ncar.91\\ncar1.00car.96car.96\\nbicycle.86car.97car.97\\ncar.97car.94car.95\\ncar.94car.81\\nperson.87\\nparking meter.98car.89\\ndonut1.00donut.90\\ndonut.88donut.81\\ndonut.95\\ndonut.96donut1.00 donut.98\\ndonut.99donut.94donut.97donut.99\\ndonut.98donut1.00\\ndonut.95donut1.00\\ndonut.98donut.98donut.99\\ndonut.96\\ndonut.89donut.96donut.95donut.98donut.89\\ndonut.93donut.95\\ndonut.90donut.89\\ndonut.89donut.89\\ndonut.86donut.86\\nperson1.00person1.00 person1.00\\nperson1.00person1.00person1.00\\nperson1.00\\ndog1.00baseball bat.99\\nbaseball bat.85\\nbaseball bat.98\\ntruck.92\\ntruck.99truck.96truck.99truck.97\\nbus.99truck.93 bus.90\\nperson1.00person1.00horse.77horse.99\\ncow.93person.96person1.00\\nperson.99horse.97\\nperson.98 person.97person.98\\nperson.96\\nperson1.00\\ntennis racket1.00chair.73person.90person.77\\nperson.97\\nperson.81person.87\\nperson.71 person.96 person.99 person.98 person.94chair.97\\nchair.80\\nchair.71chair.94chair.92chair.99chair.93\\nchair.99\\nchair.91 chair.81 chair.98 chair.83chair.81chair.81\\nchair.93\\nsports ball.99\\nperson1.00\\ncouch.82person1.00\\nperson.99person1.00person1.00person1.00 person.99skateboard.99\\nperson.90person.98person.99person.91\\nperson.99person1.00\\nperson.80\\nskateboard.98Figure 5. More results of Mask R-CNN on COCO test images, using ResNet-101-FPN and running at 5 fps, with 35.7 mask AP (Table 1).\\nbackbone AP AP 50 AP75 APS APM APL\\nMNC [10] ResNet-101-C4 24.6 44.3 24.8 4.7 25.9 43.6\\nFCIS [26] +OHEM ResNet-101-C5-dilated 29.2 49.5 - 7.1 31.3 50.0\\nFCIS+++ [26] +OHEM ResNet-101-C5-dilated 33.6 54.5 - - - -\\nMask R-CNN ResNet-101-C4 33.1 54.9 34.8 12.1 35.6 51.1\\nMask R-CNN ResNet-101-FPN 35.7 58.0 37.8 15.5 38.1 52.4\\nMask R-CNN ResNeXt-101-FPN 37.1 60.0 39.4 16.9 39.9 53.5\\nTable 1. Instance segmentation mask AP on COCO test-dev . MNC [10] and FCIS [26] are the winners of the COCO 2015 and 2016\\nsegmentation challenges, respectively. Without bells and whistles, Mask R-CNN outperforms the more complex FCIS+++, which includes\\nmulti-scale train/test, horizontal Ô¨Çip test, and OHEM [38]. All entries are single-model results.\\ncan predict Kmasks per RoI, but we only use the k-th mask,\\nwhere kis the predicted class by the classiÔ¨Åcation branch.\\nThem√ómÔ¨Çoating-number mask output is then resized to\\nthe RoI size, and binarized at a threshold of 0.5.\\nNote that since we only compute masks on the top 100\\ndetection boxes, Mask R-CNN adds a small overhead to its\\nFaster R-CNN counterpart ( e.g.,‚àº20% on typical models).\\n4. Experiments: Instance Segmentation\\nWe perform a thorough comparison of Mask R-CNN to\\nthe state of the art along with comprehensive ablations on\\nthe COCO dataset [28]. We report the standard COCO met-\\nrics including AP (averaged over IoU thresholds), AP 50,\\nAP75, and AP S, AP M, AP L(AP at different scales). Un-\\nless noted, AP is evaluating using mask IoU. As in previous\\nwork [5, 27], we train using the union of 80k train images\\nand a 35k subset of val images ( trainval35k ), and re-\\nport ablations on the remaining 5k val images ( minival ).\\nWe also report results on test-dev [28].4.1. Main Results\\nWe compare Mask R-CNN to the state-of-the-art meth-\\nods in instance segmentation in Table 1. All instantia-\\ntions of our model outperform baseline variants of pre-\\nvious state-of-the-art models. This includes MNC [10]\\nand FCIS [26], the winners of the COCO 2015 and 2016\\nsegmentation challenges, respectively. Without bells and\\nwhistles, Mask R-CNN with ResNet-101-FPN backbone\\noutperforms FCIS+++ [26], which includes multi-scale\\ntrain/test, horizontal Ô¨Çip test, and online hard example min-\\ning (OHEM) [38]. While outside the scope of this work, we\\nexpect many such improvements to be applicable to ours.\\nMask R-CNN outputs are visualized in Figures 2 and 5.\\nMask R-CNN achieves good results even under challeng-\\ning conditions. In Figure 6 we compare our Mask R-CNN\\nbaseline and FCIS+++ [26]. FCIS+++ exhibits systematic\\nartifacts on overlapping instances, suggesting that it is chal-\\nlenged by the fundamental difÔ¨Åculty of instance segmenta-\\ntion. Mask R-CNN shows no such artifacts.\\n5'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Mask rcnn and cnns.pdf', 'page': 5}, page_content='person1.00\\nperson1.00\\nperson1.00person1.00umbrella1.00 umbrella.99\\ncar.99car.93\\ngiraffe1.00 giraffe1.00\\nperson1.00person1.00person1.00person1.00\\nperson.95\\nsports ball1.00sports ball.98person1.00\\nperson1.00\\nperson1.00\\ntie.95\\ntie1.00\\nFCIS Mask R-CNN\\ntrain1.00\\ntrain.99\\ntrain.80\\nperson1.00 person1.00person1.00\\nperson1.00person1.00 person1.00\\nskateboard.98person.99 person.99\\nskateboard.99handba g.93\\nFigure 6. FCIS+++ [26] (top) vs. Mask R-CNN (bottom, ResNet-101-FPN). FCIS exhibits systematic artifacts on overlapping objects.\\nnet-depth-features AP AP 50 AP75\\nResNet-50-C4 30.3 51.2 31.5\\nResNet-101-C4 32.7 54.2 34.3\\nResNet-50-FPN 33.6 55.2 35.3\\nResNet-101-FPN 35.4 57.3 37.5\\nResNeXt-101-FPN 36.7 59.5 38.9\\n(a)Backbone Architecture : Better back-\\nbones bring expected gains: deeper networks\\ndo better, FPN outperforms C4 features, and\\nResNeXt improves on ResNet.AP AP 50 AP75\\nsoftmax 24.8 44.1 25.1\\nsigmoid 30.3 51.2 31.5\\n+5.5 +7.1 +6.4\\n(b)Multinomial vs. Independent Masks\\n(ResNet-50-C4): Decoupling via per-\\nclass binary masks (sigmoid) gives large\\ngains over multinomial masks (softmax).align? bilinear? agg. AP AP 50 AP75\\nRoIPool [12] max 26.9 48.8 26.4\\nRoIWarp [10]‚úì max 27.2 49.2 27.1\\n‚úì ave 27.1 48.9 27.1\\nRoIAlign‚úì ‚úì max 30.2 51.0 31.8\\n‚úì ‚úì ave 30.3 51.2 31.5\\n(c)RoIAlign (ResNet-50-C4): Mask results with various RoI\\nlayers. Our RoIAlign layer improves AP by ‚àº3 points and\\nAP75by‚àº5 points. Using proper alignment is the only fac-\\ntor that contributes to the large gap between RoI layers.\\nAP AP 50 AP75 APbbAPbb\\n50APbb\\n75\\nRoIPool 23.6 46.5 21.6 28.2 52.7 26.9\\nRoIAlign 30.9 51.8 32.1 34.0 55.3 36.4\\n+7.3 + 5.3 +10.5 +5.8 +2.6 +9.5\\n(d)RoIAlign (ResNet-50- C5,stride 32 ): Mask-level and box-level\\nAP using large-stride features. Misalignments are more severe than\\nwith stride-16 features (Table 2c), resulting in big accuracy gaps.mask branch AP AP 50 AP75\\nMLP fc: 1024‚Üí1024‚Üí80¬∑28231.5 53.7 32.8\\nMLP fc: 1024‚Üí1024‚Üí1024‚Üí80¬∑28231.5 54.0 32.6\\nFCN conv: 256‚Üí256‚Üí256‚Üí256‚Üí256‚Üí80 33.6 55.2 35.3\\n(e)Mask Branch (ResNet-50-FPN): Fully convolutional networks (FCN) vs.\\nmulti-layer perceptrons (MLP, fully-connected) for mask prediction. FCNs im-\\nprove results as they take advantage of explicitly encoding spatial layout.\\nTable 2. Ablations . We train on trainval35k , test on minival , and report mask AP unless otherwise noted.\\n4.2. Ablation Experiments\\nWe run a number of ablations to analyze Mask R-CNN.\\nResults are shown in Table 2 and discussed in detail next.\\nArchitecture: Table 2a shows Mask R-CNN with various\\nbackbones. It beneÔ¨Åts from deeper networks (50 vs. 101)\\nand advanced designs including FPN and ResNeXt. We\\nnote that notall frameworks automatically beneÔ¨Åt from\\ndeeper or advanced networks (see benchmarking in [21]).\\nMultinomial vs. Independent Masks: Mask R-CNN de-\\ncouples mask and class prediction: as the existing box\\nbranch predicts the class label, we generate a mask for each\\nclass without competition among classes (by a per-pixel sig-\\nmoid and a binary loss). In Table 2b, we compare this to\\nusing a per-pixel softmax and a multinomial loss (as com-\\nmonly used in FCN [30]). This alternative couples the tasks\\nof mask and class prediction, and results in a severe loss\\nin mask AP (5.5 points). This suggests that once the in-\\nstance has been classiÔ¨Åed as a whole (by the box branch),\\nit is sufÔ¨Åcient to predict a binary mask without concern for\\nthe categories, which makes the model easier to train.\\nClass-SpeciÔ¨Åc vs. Class-Agnostic Masks: Our default in-\\nstantiation predicts class-speciÔ¨Åc masks, i.e., one m√ómmask per class. Interestingly, Mask R-CNN with class-\\nagnostic masks ( i.e., predicting a single m√ómoutput re-\\ngardless of class) is nearly as effective: it has 29.7 mask AP\\nvs. 30.3 for the class-speciÔ¨Åc counterpart on ResNet-50-C4.\\nThis further highlights the division of labor in our approach\\nwhich largely decouples classiÔ¨Åcation and segmentation.\\nRoIAlign: An evaluation of our proposed RoIAlign layer is\\nshown in Table 2c. For this experiment we use the ResNet-\\n50-C4 backbone, which has stride 16. RoIAlign improves\\nAP by about 3 points over RoIPool, with much of the gain\\ncoming at high IoU (AP 75). RoIAlign is insensitive to\\nmax/average pool; we use average in the rest of the paper.\\nAdditionally, we compare with RoIWarp proposed in\\nMNC [10] that also adopt bilinear sampling. As discussed\\nin¬ß3, RoIWarp still quantizes the RoI, losing alignment\\nwith the input. As can be seen in Table 2c, RoIWarp per-\\nforms on par with RoIPool and much worse than RoIAlign.\\nThis highlights that proper alignment is key.\\nWe also evaluate RoIAlign with a ResNet-50-C5 back-\\nbone, which has an even larger stride of 32 pixels. We use\\nthe same head as in Figure 4 (right), as the res5 head is not\\napplicable. Table 2d shows that RoIAlign improves mask\\nAP by a massive 7.3 points, and mask AP 75by 10.5 points\\n6'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Mask rcnn and cnns.pdf', 'page': 6}, page_content='backbone APbbAPbb\\n50APbb\\n75APbb\\nSAPbb\\nMAPbb\\nL\\nFaster R-CNN+++ [19] ResNet-101-C4 34.9 55.7 37.4 15.6 38.7 50.9\\nFaster R-CNN w FPN [27] ResNet-101-FPN 36.2 59.1 39.0 18.2 39.0 48.2\\nFaster R-CNN by G-RMI [21] Inception-ResNet-v2 [41] 34.7 55.5 36.7 13.5 38.1 52.0\\nFaster R-CNN w TDM [39] Inception-ResNet-v2-TDM 36.8 57.7 39.2 16.2 39.8 52.1\\nFaster R-CNN, RoIAlign ResNet-101-FPN 37.3 59.6 40.3 19.8 40.2 48.8\\nMask R-CNN ResNet-101-FPN 38.2 60.3 41.7 20.1 41.1 50.2\\nMask R-CNN ResNeXt-101-FPN 39.8 62.3 43.4 22.1 43.2 51.2\\nTable 3. Object detection single-model results (bounding box AP), vs. state-of-the-art on test-dev . Mask R-CNN using ResNet-101-\\nFPN outperforms the base variants of all previous state-of-the-art models (the mask output is ignored in these experiments). The gains of\\nMask R-CNN over [27] come from using RoIAlign (+1.1 APbb), multitask training (+0.9 APbb), and ResNeXt-101 (+1.6 APbb).\\n(50% relative improvement ). Moreover, we note that with\\nRoIAlign, using stride-32 C5 features (30.9 AP) is more ac-\\ncurate than using stride-16 C4 features (30.3 AP, Table 2c).\\nRoIAlign largely resolves the long-standing challenge of\\nusing large-stride features for detection and segmentation.\\nFinally, RoIAlign shows a gain of 1.5 mask AP and 0.5\\nbox AP when used with FPN, which has Ô¨Åner multi-level\\nstrides. For keypoint detection that requires Ô¨Åner alignment,\\nRoIAlign shows large gains even with FPN (Table 6).\\nMask Branch: Segmentation is a pixel-to-pixel task and\\nwe exploit the spatial layout of masks by using an FCN.\\nIn Table 2e, we compare multi-layer perceptrons (MLP)\\nand FCNs, using a ResNet-50-FPN backbone. Using FCNs\\ngives a 2.1 mask AP gain over MLPs. We note that we\\nchoose this backbone so that the conv layers of the FCN\\nhead are not pre-trained, for a fair comparison with MLP.\\n4.3. Bounding Box Detection Results\\nWe compare Mask R-CNN to the state-of-the-art COCO\\nbounding-box object detection in Table 3. For this result,\\neven though the full Mask R-CNN model is trained, only\\nthe classiÔ¨Åcation and box outputs are used at inference (the\\nmask output is ignored). Mask R-CNN using ResNet-101-\\nFPN outperforms the base variants of all previous state-of-\\nthe-art models, including the single-model variant of G-\\nRMI [21], the winner of the COCO 2016 Detection Chal-\\nlenge. Using ResNeXt-101-FPN, Mask R-CNN further im-\\nproves results, with a margin of 3.0 points box AP over\\nthe best previous single model entry from [39] (which used\\nInception-ResNet-v2-TDM).\\nAs a further comparison, we trained a version of Mask\\nR-CNN but without the mask branch, denoted by ‚ÄúFaster\\nR-CNN, RoIAlign‚Äù in Table 3. This model performs better\\nthan the model presented in [27] due to RoIAlign. On the\\nother hand, it is 0.9 points box AP lower than Mask R-CNN.\\nThis gap of Mask R-CNN on box detection is therefore due\\nsolely to the beneÔ¨Åts of multi-task training.\\nLastly, we note that Mask R-CNN attains a small gap\\nbetween its mask and box AP: e.g., 2.7 points between 37.1\\n(mask, Table 1) and 39.8 (box, Table 3). This indicates that\\nour approach largely closes the gap between object detec-\\ntion and the more challenging instance segmentation task.4.4. Timing\\nInference: We train a ResNet-101-FPN model that shares\\nfeatures between the RPN and Mask R-CNN stages, follow-\\ning the 4-step training of Faster R-CNN [36]. This model\\nruns at 195ms per image on an Nvidia Tesla M40 GPU (plus\\n15ms CPU time resizing the outputs to the original resolu-\\ntion), and achieves statistically the same mask AP as the\\nunshared one. We also report that the ResNet-101-C4 vari-\\nant takes ‚àº400ms as it has a heavier box head (Figure 4), so\\nwe do not recommend using the C4 variant in practice.\\nAlthough Mask R-CNN is fast, we note that our design\\nis not optimized for speed, and better speed/accuracy trade-\\noffs could be achieved [21], e.g., by varying image sizes and\\nproposal numbers, which is beyond the scope of this paper.\\nTraining: Mask R-CNN is also fast to train. Training with\\nResNet-50-FPN on COCO trainval35k takes 32 hours\\nin our synchronized 8-GPU implementation (0.72s per 16-\\nimage mini-batch), and 44 hours with ResNet-101-FPN. In\\nfact, fast prototyping can be completed in less than one day\\nwhen training on the train set. We hope such rapid train-\\ning will remove a major hurdle in this area and encourage\\nmore people to perform research on this challenging topic.\\n5. Mask R-CNN for Human Pose Estimation\\nOur framework can easily be extended to human pose\\nestimation. We model a keypoint‚Äôs location as a one-hot\\nmask, and adopt Mask R-CNN to predict Kmasks, one for\\neach of Kkeypoint types ( e.g., left shoulder, right elbow).\\nThis task helps demonstrate the Ô¨Çexibility of Mask R-CNN.\\nWe note that minimal domain knowledge for human pose\\nis exploited by our system, as the experiments are mainly to\\ndemonstrate the generality of the Mask R-CNN framework.\\nWe expect that domain knowledge ( e.g., modeling struc-\\ntures [6]) will be complementary to our simple approach.\\nImplementation Details: We make minor modiÔ¨Åcations to\\nthe segmentation system when adapting it for keypoints.\\nFor each of the Kkeypoints of an instance, the training\\ntarget is a one-hot m√ómbinary mask where only a single\\npixel is labeled as foreground. During training, for each vis-\\nible ground-truth keypoint, we minimize the cross-entropy\\nloss over an m2-way softmax output (which encourages a\\n7'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Mask rcnn and cnns.pdf', 'page': 7}, page_content='Figure 7. Keypoint detection results on COCO test using Mask R-CNN (ResNet-50-FPN), with person segmentation masks predicted\\nfrom the same model. This model has a keypoint AP of 63.1 and runs at 5 fps.\\nAPkpAPkp\\n50APkp\\n75APkp\\nMAPkp\\nL\\nCMU-Pose+++ [6] 61.8 84.9 67.5 57.1 68.2\\nG-RMI [32]‚Ä†62.4 84.0 68.5 59.1 68.1\\nMask R-CNN , keypoint-only 62.7 87.0 68.4 57.4 71.1\\nMask R-CNN , keypoint & mask 63.1 87.3 68.7 57.8 71.4\\nTable 4. Keypoint detection AP on COCO test-dev . Ours is a\\nsingle model (ResNet-50-FPN) that runs at 5 fps. CMU-Pose+++\\n[6] is the 2016 competition winner that uses multi-scale testing,\\npost-processing with CPM [44], and Ô¨Åltering with an object detec-\\ntor, adding a cumulative ‚àº5 points (clariÔ¨Åed in personal commu-\\nnication).‚Ä†: G-RMI was trained on COCO plus MPII [1] (25k im-\\nages), using two models (Inception-ResNet-v2 for bounding box\\ndetection and ResNet-101 for keypoints).\\nsingle point to be detected). We note that as in instance seg-\\nmentation, the Kkeypoints are still treated independently.\\nWe adopt the ResNet-FPN variant, and the keypoint head\\narchitecture is similar to that in Figure 4 (right). The key-\\npoint head consists of a stack of eight 3 √ó3 512-d conv lay-\\ners, followed by a deconv layer and 2 √óbilinear upscaling,\\nproducing an output resolution of 56 √ó56. We found that\\na relatively high resolution output (compared to masks) is\\nrequired for keypoint-level localization accuracy.\\nModels are trained on all COCO trainval35k im-\\nages that contain annotated keypoints. To reduce overÔ¨Åt-\\nting, as this training set is smaller, we train using image\\nscales randomly sampled from [640, 800] pixels; inference\\nis on a single scale of 800 pixels. We train for 90k iterations,\\nstarting from a learning rate of 0.02 and reducing it by 10 at\\n60k and 80k iterations. We use bounding-box NMS with a\\nthreshold of 0.5. Other details are identical as in ¬ß3.1.\\nMain Results and Ablations: We evaluate the person key-\\npoint AP (APkp) and experiment with a ResNet-50-FPN\\nbackbone; more backbones will be studied in the appendix.\\nTable 4 shows that our result (62.7 APkp) is 0.9 points higher\\nthan the COCO 2016 keypoint detection winner [6] that\\nuses a multi-stage processing pipeline (see caption of Ta-\\nble 4). Our method is considerably simpler and faster.\\nMore importantly, we have a uniÔ¨Åed model that can si-APbb\\nperson APmask\\nperson APkp\\nFaster R-CNN 52.5 - -\\nMask R-CNN, mask-only 53.6 45.8 -\\nMask R-CNN, keypoint-only 50.7 - 64.2\\nMask R-CNN, keypoint & mask 52.0 45.1 64.7\\nTable 5. Multi-task learning of box, mask, and keypoint about the\\nperson category, evaluated on minival . All entries are trained\\non the same data for fair comparisons. The backbone is ResNet-\\n50-FPN. The entries with 64.2 and 64.7 AP on minival have\\ntest-dev AP of 62.7 and 63.1, respectively (see Table 4).\\nAPkpAPkp\\n50APkp\\n75APkp\\nMAPkp\\nL\\nRoIPool 59.8 86.2 66.7 55.1 67.4\\nRoIAlign 64.2 86.6 69.7 58.7 73.0\\nTable 6. RoIAlign vs. RoIPool for keypoint detection on\\nminival . The backbone is ResNet-50-FPN.\\nmultaneously predict boxes, segments, and keypoints while\\nrunning at 5 fps. Adding a segment branch (for the per-\\nson category) improves the APkpto 63.1 (Table 4) on\\ntest-dev . More ablations of multi-task learning on\\nminival are in Table 5. Adding the mask branch to the\\nbox-only ( i.e., Faster R-CNN) or keypoint-only versions\\nconsistently improves these tasks. However, adding the\\nkeypoint branch reduces the box/mask AP slightly, suggest-\\ning that while keypoint detection beneÔ¨Åts from multitask\\ntraining, it does not in turn help the other tasks. Neverthe-\\nless, learning all three tasks jointly enables a uniÔ¨Åed system\\nto efÔ¨Åciently predict all outputs simultaneously (Figure 7).\\nWe also investigate the effect of RoIAlign on keypoint\\ndetection (Table 6). Though this ResNet-50-FPN backbone\\nhas Ô¨Åner strides ( e.g., 4 pixels on the Ô¨Ånest level), RoIAlign\\nstill shows signiÔ¨Åcant improvement over RoIPool and in-\\ncreases APkpby 4.4 points. This is because keypoint detec-\\ntions are more sensitive to localization accuracy. This again\\nindicates that alignment is essential for pixel-level localiza-\\ntion, including masks and keypoints.\\nGiven the effectiveness of Mask R-CNN for extracting\\nobject bounding boxes, masks, and keypoints, we expect it\\nbe an effective framework for other instance-level tasks.\\n8'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Mask rcnn and cnns.pdf', 'page': 8}, page_content='training data AP [val] AP AP 50 person rider car truck bus train mcycle bicycle\\nInstanceCut [23] fine +coarse 15.8 13.0 27.9 10.0 8.0 23.7 14.0 19.5 15.2 9.3 4.7\\nDWT [4] fine 19.8 15.6 30.0 15.1 11.7 32.9 17.1 20.4 15.0 7.9 4.9\\nSAIS [17] fine - 17.4 36.7 14.6 12.9 35.7 16.0 23.2 19.0 10.3 7.8\\nDIN [3] fine +coarse - 20.0 38.8 16.5 16.7 25.7 20.6 30.0 23.4 17.1 10.1\\nSGN [29] fine +coarse 29.2 25.0 44.9 21.8 20.1 39.4 24.8 33.2 30.8 17.7 12.4\\nMask R-CNN fine 31.5 26.2 49.9 30.5 23.7 46.9 22.8 32.2 18.6 19.1 16.0\\nMask R-CNN fine + COCO 36.4 32.0 58.1 34.8 27.0 49.1 30.1 40.9 30.9 24.1 18.7\\nTable 7. Results on Cityscapes val (‚ÄòAP [ val]‚Äô column) and test (remaining columns) sets. Our method uses ResNet-50-FPN.\\nAppendix A: Experiments on Cityscapes\\nWe further report instance segmentation results on the\\nCityscapes [7] dataset. This dataset has fine annota-\\ntions for 2975 train, 500 val, and 1525 test images. It has\\n20kcoarse training images without instance annotations,\\nwhich we do notuse. All images are 2048 √ó1024 pixels.\\nThe instance segmentation task involves 8 object categories,\\nwhose numbers of instances on the fine training set are:\\nperson rider car truck bus train mcycle bicycle\\n17.9k 1.8k 26.9k 0.5k 0.4k 0.2k 0.7k 3.7k\\nInstance segmentation performance on this task is measured\\nby the COCO-style mask AP (averaged over IoU thresh-\\nolds); AP 50(i.e., mask AP at an IoU of 0.5) is also reported.\\nImplementation: We apply our Mask R-CNN models with\\nthe ResNet-FPN-50 backbone; we found the 101-layer\\ncounterpart performs similarly due to the small dataset size.\\nWe train with image scale (shorter side) randomly sampled\\nfrom [800, 1024], which reduces overÔ¨Åtting; inference is on\\na single scale of 1024 pixels. We use a mini-batch size of\\n1 image per GPU (so 8 on 8 GPUs) and train the model\\nfor 24k iterations, starting from a learning rate of 0.01 and\\nreducing it to 0.001 at 18k iterations. It takes ‚àº4 hours of\\ntraining on a single 8-GPU machine under this setting.\\nResults: Table 7 compares our results to the state of the\\nart on the val andtest sets. Without using the coarse\\ntraining set, our method achieves 26.2 AP on test , which\\nis over 30% relative improvement over the previous best en-\\ntry (DIN [3]), and is also better than the concurrent work of\\nSGN‚Äôs 25.0 [29]. Both DIN and SGN use fine +coarse\\ndata. Compared to the best entry using fine data only\\n(17.4 AP), we achieve a ‚àº50% improvement.\\nFor the person andcarcategories, the Cityscapes dataset\\nexhibits a large number of within -category overlapping in-\\nstances (on average 6 people and 9 cars per image). We\\nargue that within-category overlap is a core difÔ¨Åculty of in-\\nstance segmentation. Our method shows massive improve-\\nment on these two categories over the other best entries (rel-\\native‚àº40% improvement on person from 21.8 to 30.5 and\\n‚àº20% improvement on carfrom 39.4 to 46.9), even though\\nour method does not exploit the coarse data.\\nA main challenge of the Cityscapes dataset is training\\nmodels in a low-data regime, particularly for the categories\\noftruck ,bus, and train , which have about 200-500 train-\\ncar:1.00car:0.98\\ncar:0.98 car:0.95 car:0.81car:0.52person:1.00person:1.00person:1.00\\nperson:1.00 person:1.00person:1.00person:1.00\\nperson:1.00 person:1.00person:1.00\\nperson:1.00person:1.00\\nperson:1.00\\nperson:1.00person:0.99\\nperson:0.99person:0.99 person:0.99person:0.98\\nperson:0.98person:0.98person:0.98person:0.94 person:0.94person:0.82\\nperson:0.82person:0.79\\nperson:0.73person:0.67person:0.66person:0.59\\ntruck:0.66bus:1.00\\nbus:0.95rider:0.59\\nbicycle:0.83\\nbicycle:0.56car:1.00car:1.00\\ncar:1.00 car:1.00car:1.00car:1.00\\ncar:1.00car:1.00 car:1.00car:0.99car:0.95\\ncar:0.95car:0.95 car:0.69car:0.68 car:0.68car:0.64\\ncar:0.57 car:0.52person:1.00\\nperson:0.99person:0.99person:0.99 person:0.99person:0.98person:0.98 person:0.98 person:0.97 person:0.93person:0.92\\nperson:0.91person:0.86 person:0.84person:0.82 person:0.73person:0.72 person:0.72person:0.72 person:0.63 rider:0.68\\ncar:1.00\\ncar:1.00car:1.00car:1.00\\ncar:1.00car:1.00car:1.00car:1.00\\ncar:1.00 car:1.00\\ncar:1.00car:1.00\\ncar:1.00car:1.00car:1.00car:1.00car:1.00\\ncar:0.98car:0.97\\ncar:0.88car:0.76car:0.72car:0.72 car:0.65car:0.50person:1.00\\nperson:1.00 person:0.98person:0.93person:0.85person:0.78person:0.73 person:0.58\\nperson:1.00\\nperson:1.00person:1.00\\nperson:1.00\\nperson:1.00person:1.00\\nperson:1.00person:1.00person:1.00\\nperson:1.00person:1.00person:1.00\\nperson:1.00person:1.00\\nperson:1.00person:1.00person:1.00\\nperson:1.00person:1.00\\nperson:1.00person:1.00\\nperson:1.00person:0.99\\nperson:0.99 person:0.98person:0.97\\nperson:0.96person:0.92\\nperson:0.91person:0.70 person:0.59\\nbicycle:0.99bicycle:0.97car:1.00\\ncar:1.00 car:0.99\\ncar:0.89person:1.00\\nperson:1.00person:1.00\\nperson:1.00\\nperson:1.00person:0.96person:0.93\\nperson:0.89person:0.88person:0.75\\nrider:0.94\\ncar:1.00\\ncar:1.00car:1.00\\ncar:1.00car:1.00\\ncar:1.00car:1.00 car:0.99car:0.89 car:0.67person:1.00\\nperson:1.00\\nperson:1.00 person:1.00 person:0.82\\nbus:0.75Figure 8. Mask R-CNN results on Cityscapes test (32.0 AP).\\nThe bottom-right image shows a failure prediction.\\ning samples each. To partially remedy this issue, we further\\nreport a result using COCO pre-training. To do this, we ini-\\ntialize the corresponding 7 categories in Cityscapes from a\\npre-trained COCO Mask R-CNN model ( rider being ran-\\ndomly initialized). We Ô¨Åne-tune this model for 4k iterations\\nin which the learning rate is reduced at 3k iterations, which\\ntakes‚àº1 hour for training given the COCO model.\\nThe COCO pre-trained Mask R-CNN model achieves\\n32.0 AP on test , almost a 6 point improvement over the\\nfine -only counterpart. This indicates the important role\\nthe amount of training data plays. It also suggests that\\nmethods on Cityscapes might be inÔ¨Çuenced by their low-\\nshot learning performance. We show that using COCO pre-\\ntraining is an effective strategy on this dataset.\\nFinally, we observed a bias between the val andtest\\nAP, as is also observed from the results of [23, 4, 29]. We\\nfound that this bias is mainly caused by the truck ,bus,\\nand train categories, with the fine -only model having\\nval/test AP of 28.8/22.8, 53.5/32.2, and 33.0/18.6, re-\\nspectively. This suggests that there is a domain shift on\\nthese categories, which also have little training data. COCO\\npre-training helps to improve results the most on these cat-\\negories; however, the domain shift persists with 38.0/30.1,\\n57.5/40.9, and 41.2/30.9 val/test AP, respectively. Note\\nthat for the person andcarcategories we do not see any\\nsuch bias ( val/test AP are within¬±1point).\\nExample results on Cityscapes are shown in Figure 8.\\n9'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Mask rcnn and cnns.pdf', 'page': 9}, page_content='description backbone AP AP 50 AP75 APbbAPbb\\n50APbb\\n75\\noriginal baseline X-101-FPN 36.7 59.5 38.9 39.6 61.5 43.2\\n+updated baseline X-101-FPN 37.0 59.7 39.0 40.5 63.0 43.7\\n+e2e training X-101-FPN 37.6 60.4 39.9 41.7 64.1 45.2\\n+ImageNet-5k X-101-FPN 38.6 61.7 40.9 42.7 65.1 46.6\\n+train-time augm. X-101-FPN 39.2 62.5 41.6 43.5 65.9 47.2\\n+deeper X-152-FPN 39.7 63.2 42.2 44.1 66.4 48.4\\n+Non-local [43] X-152-FPN-NL 40.3 64.4 42.8 45.0 67.8 48.9\\n+test-time augm. X-152-FPN-NL 41.8 66.0 44.8 47.3 69.3 51.5\\nTable 8. Enhanced detection results of Mask R-CNN on COCO\\nminival . Each row adds an extra component to the above row.\\nWe denote ResNeXt model by ‚ÄòX‚Äô for notational brevity.\\nAppendix B: Enhanced Results on COCO\\nAs a general framework, Mask R-CNN is compat-\\nible with complementary techniques developed for de-\\ntection/segmentation, including improvements made to\\nFast/Faster R-CNN and FCNs. In this appendix we de-\\nscribe some techniques that improve over our original re-\\nsults. Thanks to its generality and Ô¨Çexibility, Mask R-CNN\\nwas used as the framework by the three winning teams in\\nthe COCO 2017 instance segmentation competition, which\\nall signiÔ¨Åcantly outperformed the previous state of the art.\\nInstance Segmentation and Object Detection\\nWe report some enhanced results of Mask R-CNN in Ta-\\nble 8. Overall, the improvements increase mask AP 5.1\\npoints (from 36.7 to 41.8) and box AP 7.7 points (from 39.6\\nto 47.3). Each model improvement increases both mask AP\\nand box AP consistently, showing good generalization of\\nthe Mask R-CNN framework. We detail the improvements\\nnext. These results, along with future updates, can be repro-\\nduced by our released code at https://github.com/\\nfacebookresearch/Detectron , and can serve as\\nhigher baselines for future research.\\nUpdated baseline: We start with an updated baseline\\nwith a different set of hyper-parameters. We lengthen the\\ntraining to 180k iterations, in which the learning rate is re-\\nduced by 10 at 120k and 160k iterations. We also change\\nthe NMS threshold to 0.5 (from a default value of 0.3). The\\nupdated baseline has 37.0 mask AP and 40.5 box AP.\\nEnd-to-end training: All previous results used stage-\\nwise training, i.e., training RPN as the Ô¨Årst stage and Mask\\nR-CNN as the second. Following [37], we evaluate end-\\nto-end (‚Äòe2e‚Äô) training that jointly trains RPN and Mask R-\\nCNN. We adopt the ‚Äòapproximate‚Äô version in [37] that only\\ncomputes partial gradients in the RoIAlign layer by ignor-\\ning the gradient w.r.t. RoI coordinates. Table 8 shows that\\ne2e training improves mask AP by 0.6 and box AP by 1.2.\\nImageNet-5k pre-training: Following [45], we experi-\\nment with models pre-trained on a 5k-class subset of Ima-\\ngeNet (in contrast to the standard 1k-class subset). This 5 √ó\\nincrease in pre-training data improves both mask and box 1\\nAP. As a reference, [40] used ‚àº250√ómore images (300M)\\nand reported a 2-3 box AP improvement on their baselines.description backbone APkpAPkp\\n50APkp\\n75APkp\\nMAPkp\\nL\\noriginal baseline R-50-FPN 64.2 86.6 69.7 58.7 73.0\\n+updated baseline R-50-FPN 65.1 86.6 70.9 59.9 73.6\\n+deeper R-101-FPN 66.1 87.7 71.7 60.5 75.0\\n+ResNeXt X-101-FPN 67.3 88.0 73.3 62.2 75.6\\n+data distillation [35] X-101-FPN 69.1 88.9 75.3 64.1 77.1\\n+test-time augm. X-101-FPN 70.4 89.3 76.8 65.8 78.1\\nTable 9. Enhanced keypoint results of Mask R-CNN on COCO\\nminival . Each row adds an extra component to the above row.\\nHere we use only keypoint annotations but no mask annotations.\\nWe denote ResNet by ‚ÄòR‚Äô and ResNeXt by ‚ÄòX‚Äô for brevity.\\nTrain-time augmentation: Scale augmentation at train\\ntime further improves results. During training, we randomly\\nsample a scale from [640, 800] pixels and we increase the\\nnumber of iterations to 260k (with the learning rate reduced\\nby 10 at 200k and 240k iterations). Train-time augmenta-\\ntion improves mask AP by 0.6 and box AP by 0.8.\\nModel architecture: By upgrading the 101-layer\\nResNeXt to its 152-layer counterpart [19], we observe an\\nincrease of 0.5 mask AP and 0.6 box AP. This shows a\\ndeeper model can still improve results on COCO.\\nUsing the recently proposed non-local (NL) model [43],\\nwe achieve 40.3 mask AP and 45.0 box AP. This result is\\nwithout test-time augmentation, and the method runs at 3fps\\non an Nvidia Tesla P100 GPU at test time.\\nTest-time augmentation: We combine the model results\\nevaluated using scales of [400, 1200] pixels with a step of\\n100 and on their horizontal Ô¨Çips. This gives us a single-\\nmodel result of 41.8 mask AP and 47.3 box AP.\\nThe above result is the foundation of our submission to\\nthe COCO 2017 competition (which also used an ensemble,\\nnot discussed here). The Ô¨Årst three winning teams for the\\ninstance segmentation task were all reportedly based on an\\nextension of the Mask R-CNN framework.\\nKeypoint Detection\\nWe report enhanced results of keypoint detection in Ta-\\nble 9. As an updated baseline, we extend the training sched-\\nule to 130k iterations in which the learning rate is reduced\\nby 10 at 100k and 120k iterations. This improves APkpby\\nabout 1 point. Replacing ResNet-50 with ResNet-101 and\\nResNeXt-101 increases APkpto 66.1 and 67.3, respectively.\\nWith a recent method called data distillation [35], we are\\nable to exploit the additional 120k unlabeled images pro-\\nvided by COCO. In brief, data distillation is a self-training\\nstrategy that uses a model trained on labeled data to pre-\\ndict annotations on unlabeled images, and in turn updates\\nthe model with these new annotations. Mask R-CNN pro-\\nvides an effective framework for such a self-training strat-\\negy. With data distillation, Mask R-CNN APkpimprove by\\n1.8 points to 69.1. We observe that Mask R-CNN can ben-\\neÔ¨Åt from extra data, even if that data is unlabeled .\\nBy using the same test-time augmentation as used for\\ninstance segmentation, we further boost APkpto 70.4.\\n10'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Mask rcnn and cnns.pdf', 'page': 10}, page_content='Acknowledgements: We would like to acknowledge Ilija\\nRadosavovic for contributions to code release and enhanced\\nresults, and the Caffe2 team for engineering support.\\nReferences\\n[1] M. Andriluka, L. Pishchulin, P. Gehler, and B. Schiele. 2D\\nhuman pose estimation: New benchmark and state of the art\\nanalysis. In CVPR , 2014. 8\\n[2] P. Arbel ¬¥aez, J. Pont-Tuset, J. T. Barron, F. Marques, and\\nJ. Malik. Multiscale combinatorial grouping. In CVPR ,\\n2014. 2\\n[3] A. Arnab and P. H. Torr. Pixelwise instance segmentation\\nwith a dynamically instantiated network. In CVPR , 2017. 3,\\n9\\n[4] M. Bai and R. Urtasun. Deep watershed transform for in-\\nstance segmentation. In CVPR , 2017. 3, 9\\n[5] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-\\noutside net: Detecting objects in context with skip pooling\\nand recurrent neural networks. In CVPR , 2016. 5\\n[6] Z. Cao, T. Simon, S.-E. Wei, and Y . Sheikh. Realtime multi-\\nperson 2d pose estimation using part afÔ¨Ånity Ô¨Åelds. In CVPR ,\\n2017. 7, 8\\n[7] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,\\nR. Benenson, U. Franke, S. Roth, and B. Schiele. The\\nCityscapes dataset for semantic urban scene understanding.\\nInCVPR , 2016. 9\\n[8] J. Dai, K. He, Y . Li, S. Ren, and J. Sun. Instance-sensitive\\nfully convolutional networks. In ECCV , 2016. 2\\n[9] J. Dai, K. He, and J. Sun. Convolutional feature masking for\\njoint object and stuff segmentation. In CVPR , 2015. 2\\n[10] J. Dai, K. He, and J. Sun. Instance-aware semantic segmen-\\ntation via multi-task network cascades. In CVPR , 2016. 2, 3,\\n4, 5, 6\\n[11] J. Dai, Y . Li, K. He, and J. Sun. R-FCN: Object detection via\\nregion-based fully convolutional networks. In NIPS , 2016. 2\\n[12] R. Girshick. Fast R-CNN. In ICCV , 2015. 1, 2, 3, 4, 6\\n[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\\nture hierarchies for accurate object detection and semantic\\nsegmentation. In CVPR , 2014. 2, 3\\n[14] R. Girshick, F. Iandola, T. Darrell, and J. Malik. Deformable\\npart models are convolutional neural networks. In CVPR ,\\n2015. 4\\n[15] B. Hariharan, P. Arbel ¬¥aez, R. Girshick, and J. Malik. Simul-\\ntaneous detection and segmentation. In ECCV . 2014. 2\\n[16] B. Hariharan, P. Arbel ¬¥aez, R. Girshick, and J. Malik. Hyper-\\ncolumns for object segmentation and Ô¨Åne-grained localiza-\\ntion. In CVPR , 2015. 2\\n[17] Z. Hayder, X. He, and M. Salzmann. Shape-aware instance\\nsegmentation. In CVPR , 2017. 9\\n[18] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\\nin deep convolutional networks for visual recognition. In\\nECCV . 2014. 1, 2\\n[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning\\nfor image recognition. In CVPR , 2016. 2, 4, 7, 10\\n[20] J. Hosang, R. Benenson, P. Doll ¬¥ar, and B. Schiele. What\\nmakes for effective detection proposals? PAMI , 2015. 2[21] J. Huang, V . Rathod, C. Sun, M. Zhu, A. Korattikara,\\nA. Fathi, I. Fischer, Z. Wojna, Y . Song, S. Guadarrama, et al.\\nSpeed/accuracy trade-offs for modern convolutional object\\ndetectors. In CVPR , 2017. 2, 3, 4, 6, 7\\n[22] M. Jaderberg, K. Simonyan, A. Zisserman, and\\nK. Kavukcuoglu. Spatial transformer networks. In\\nNIPS , 2015. 4\\n[23] A. Kirillov, E. Levinkov, B. Andres, B. Savchynskyy, and\\nC. Rother. Instancecut: from edges to instances with multi-\\ncut. In CVPR , 2017. 3, 9\\n[24] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-\\nsiÔ¨Åcation with deep convolutional neural networks. In NIPS ,\\n2012. 2\\n[25] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\\nHoward, W. Hubbard, and L. D. Jackel. Backpropagation\\napplied to handwritten zip code recognition. Neural compu-\\ntation , 1989. 2\\n[26] Y . Li, H. Qi, J. Dai, X. Ji, and Y . Wei. Fully convolutional\\ninstance-aware semantic segmentation. In CVPR , 2017. 2,\\n3, 5, 6\\n[27] T.-Y . Lin, P. Doll ¬¥ar, R. Girshick, K. He, B. Hariharan, and\\nS. Belongie. Feature pyramid networks for object detection.\\nInCVPR , 2017. 2, 4, 5, 7\\n[28] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\\nmanan, P. Doll ¬¥ar, and C. L. Zitnick. Microsoft COCO: Com-\\nmon objects in context. In ECCV , 2014. 2, 5\\n[29] S. Liu, J. Jia, S. Fidler, and R. Urtasun. SGN: Sequen-\\ntial grouping networks for instance segmentation. In ICCV ,\\n2017. 3, 9\\n[30] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\\nnetworks for semantic segmentation. In CVPR , 2015. 1, 3, 6\\n[31] V . Nair and G. E. Hinton. RectiÔ¨Åed linear units improve re-\\nstricted boltzmann machines. In ICML , 2010. 4\\n[32] G. Papandreou, T. Zhu, N. Kanazawa, A. Toshev, J. Tomp-\\nson, C. Bregler, and K. Murphy. Towards accurate multi-\\nperson pose estimation in the wild. In CVPR , 2017. 8\\n[33] P. O. Pinheiro, R. Collobert, and P. Dollar. Learning to seg-\\nment object candidates. In NIPS , 2015. 2, 3\\n[34] P. O. Pinheiro, T.-Y . Lin, R. Collobert, and P. Doll ¬¥ar. Learn-\\ning to reÔ¨Åne object segments. In ECCV , 2016. 2, 3\\n[35] I. Radosavovic, P. Doll ¬¥ar, R. Girshick, G. Gkioxari, and\\nK. He. Data distillation: Towards omni-supervised learning.\\narXiv:1712.04440 , 2017. 10\\n[36] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-\\nwards real-time object detection with region proposal net-\\nworks. In NIPS , 2015. 1, 2, 3, 4, 7\\n[37] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: To-\\nwards real-time object detection with region proposal net-\\nworks. In TPAMI , 2017. 10\\n[38] A. Shrivastava, A. Gupta, and R. Girshick. Training region-\\nbased object detectors with online hard example mining. In\\nCVPR , 2016. 2, 5\\n[39] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be-\\nyond skip connections: Top-down modulation for object de-\\ntection. arXiv:1612.06851 , 2016. 4, 7\\n[40] C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting\\nunreasonable effectiveness of data in deep learning era. In\\nICCV , 2017. 10\\n11'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Mask rcnn and cnns.pdf', 'page': 11}, page_content='[41] C. Szegedy, S. Ioffe, and V . Vanhoucke. Inception-v4,\\ninception-resnet and the impact of residual connections on\\nlearning. In ICLR Workshop , 2016. 7\\n[42] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.\\nSmeulders. Selective search for object recognition. IJCV ,\\n2013. 2\\n[43] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural\\nnetworks. arXiv:1711.07971 , 2017. 10\\n[44] S.-E. Wei, V . Ramakrishna, T. Kanade, and Y . Sheikh. Con-\\nvolutional pose machines. In CVPR , 2016. 8\\n[45] S. Xie, R. Girshick, P. Doll ¬¥ar, Z. Tu, and K. He. Aggregated\\nresidual transformations for deep neural networks. In CVPR ,\\n2017. 4, 10\\n12'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PE_Transfer_Learning.pdf', 'page': 0}, page_content='Parameter-EfÔ¨Åcient Transfer Learning for NLP\\nNeil Houlsby1Andrei Giurgiu1 *Stanis≈Çaw Jastrze ¬∏bski2 *Bruna Morrone1Quentin de Laroussilhe1\\nAndrea Gesmundo1Mona Attariyan1Sylvain Gelly1\\nAbstract\\nFine-tuning large pre-trained models is an effec-\\ntive transfer mechanism in NLP. However, in the\\npresence of many downstream tasks, Ô¨Åne-tuning\\nis parameter inefÔ¨Åcient: an entire new model is\\nrequired for every task. As an alternative, we\\npropose transfer with adapter modules. Adapter\\nmodules yield a compact and extensible model;\\nthey add only a few trainable parameters per task,\\nand new tasks can be added without revisiting\\nprevious ones. The parameters of the original\\nnetwork remain Ô¨Åxed, yielding a high degree of\\nparameter sharing. To demonstrate adapter‚Äôs ef-\\nfectiveness, we transfer the recently proposed\\nBERT Transformer model to 26diverse text clas-\\nsiÔ¨Åcation tasks, including the GLUE benchmark.\\nAdapters attain near state-of-the-art performance,\\nwhilst adding only a few parameters per task. On\\nGLUE, we attain within 0.4%of the performance\\nof full Ô¨Åne-tuning, adding only 3.6%parameters\\nper task. By contrast, Ô¨Åne-tuning trains 100% of\\nthe parameters per task.1\\n1. Introduction\\nTransfer from pre-trained models yields strong performance\\non many NLP tasks (Dai & Le, 2015; Howard & Ruder,\\n2018; Radford et al., 2018). BERT, a Transformer network\\ntrained on large text corpora with an unsupervised loss,\\nattained state-of-the-art performance on text classiÔ¨Åcation\\nand extractive question answering (Devlin et al., 2018).\\nIn this paper we address the online setting, where tasks\\narrive in a stream. The goal is to build a system that per-\\nforms well on all of them, but without training an entire new\\nmodel for every new task. A high degree of sharing between\\n*Equal contribution1Google Research2Jagiellonian University.\\nCorrespondence to: Neil Houlsby <neilhoulsby@google.com>.\\nProceedings of the 36thInternational Conference on Machine\\nLearning , Long Beach, California, PMLR 97, 2019. Copyright\\n2019 by the author(s).\\n1Code at https://github.com/google-research/\\nadapter-bert\\n105106107108109\\nNum trainable parameters / task‚àí25‚àí20‚àí15‚àí10‚àí505Accuracy delta (%)\\nAdapters (ours)\\nFine-tune top layersFigure 1. Trade-off between accuracy and number of trained task-\\nspeciÔ¨Åc parameters, for adapter tuning and Ô¨Åne-tuning. The y-axis\\nis normalized by the performance of full Ô¨Åne-tuning, details in\\nSection 3. The curves show the 20th,50th, and 80th performance\\npercentiles across nine tasks from the GLUE benchmark. Adapter-\\nbased tuning attains a similar performance to full Ô¨Åne-tuning with\\ntwo orders of magnitude fewer trained parameters.\\ntasks is particularly useful for applications such as cloud\\nservices, where models need to be trained to solve many\\ntasks that arrive from customers in sequence. For this, we\\npropose a transfer learning strategy that yields compact and\\nextensible downstream models. Compact models are those\\nthat solve many tasks using a small number of additional\\nparameters per task. Extensible models can be trained in-\\ncrementally to solve new tasks, without forgetting previous\\nones. Our method yields a such models without sacriÔ¨Åcing\\nperformance.\\nThe two most common transfer learning techniques in NLP\\nare feature-based transfer and Ô¨Åne-tuning. Instead, we\\npresent an alternative transfer method based on adapter\\nmodules (RebufÔ¨Å et al., 2017). Features-based transfer in-\\nvolves pre-training real-valued embeddings vectors. These\\nembeddings may be at the word (Mikolov et al., 2013), sen-\\ntence (Cer et al., 2019), or paragraph level (Le & Mikolov,\\n2014). The embeddings are then fed to custom downstream\\nmodels. Fine-tuning involves copying the weights from a\\npre-trained network and tuning them on the downstream\\ntask. Recent work shows that Ô¨Åne-tuning often enjoys betterarXiv:1902.00751v2  [cs.LG]  13 Jun 2019'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PE_Transfer_Learning.pdf', 'page': 1}, page_content='Parameter-EfÔ¨Åcient Transfer Learning for NLP\\nperformance than feature-based transfer (Howard & Ruder,\\n2018).\\nBoth feature-based transfer and Ô¨Åne-tuning require a new\\nset of weights for each task. Fine-tuning is more parameter\\nefÔ¨Åcient if the lower layers of a network are shared between\\ntasks. However, our proposed adapter tuning method is even\\nmore parameter efÔ¨Åcient. Figure 1 demonstrates this trade-\\noff. The x-axis shows the number of parameters trained per\\ntask; this corresponds to the marginal increase in the model\\nsize required to solve each additional task. Adapter-based\\ntuning requires training two orders of magnitude fewer pa-\\nrameters to Ô¨Åne-tuning, while attaining similar performance.\\nAdapters are new modules added between layers of a\\npre-trained network. Adapter-based tuning differs from\\nfeature-based transfer and Ô¨Åne-tuning in the following way.\\nConsider a function (neural network) with parameters w:\\nœÜw(x). Feature-based transfer composes œÜwwith a new\\nfunction,œáv, to yieldœáv(œÜw(x)). Only the new, task-\\nspeciÔ¨Åc, parameters, v, are then trained. Fine-tuning in-\\nvolves adjusting the original parameters, w, for each new\\ntask, limiting compactness. For adapter tuning, a new\\nfunction,œàw,v(x), is deÔ¨Åned, where parameters ware\\ncopied over from pre-training. The initial parameters v0\\nare set such that the new function resembles the original:\\nœàw,v0(x)‚âàœÜw(x). During training, only vare tuned.\\nFor deep networks, deÔ¨Åning œàw,vtypically involves adding\\nnew layers to the original network, œÜw. If one chooses\\n|v|‚â™|w|, the resulting model requires ‚àº|w|parameters\\nfor many tasks. Since wis Ô¨Åxed, the model can be extended\\nto new tasks without affecting previous ones.\\nAdapter-based tuning relates to multi-task andcontinual\\nlearning. Multi-task learning also results in compact models.\\nHowever, multi-task learning requires simultaneous access\\nto all tasks, which adapter-based tuning does not require.\\nContinual learning systems aim to learn from an endless\\nstream of tasks. This paradigm is challenging because net-\\nworks forget previous tasks after re-training (McCloskey\\n& Cohen, 1989; French, 1999). Adapters differ in that the\\ntasks do not interact and the shared parameters are frozen.\\nThis means that the model has perfect memory of previous\\ntasks using a small number of task-speciÔ¨Åc parameters.\\nWe demonstrate on a large and diverse set of text classiÔ¨Åca-\\ntion tasks that adapters yield parameter-efÔ¨Åcient tuning for\\nNLP. The key innovation is to design an effective adapter\\nmodule and its integration with the base model. We propose\\na simple yet effective, bottleneck architecture. On the GLUE\\nbenchmark, our strategy almost matches the performance of\\nthe fully Ô¨Åne-tuned BERT, but uses only 3% task-speciÔ¨Åc\\nparameters, while Ô¨Åne-tuning uses 100% task-speciÔ¨Åc pa-\\nrameters. We observe similar results on a further 17public\\ntext datasets, and SQuAD extractive question answering. In\\nsummary, adapter-based tuning yields a single, extensible,model that attains near state-of-the-art performance in text\\nclassiÔ¨Åcation.\\n2. Adapter tuning for NLP\\nWe present a strategy for tuning a large text model on several\\ndownstream tasks. Our strategy has three key properties:\\n(i) it attains good performance, (ii) it permits training on\\ntasks sequentially, that is, it does not require simultaneous\\naccess to all datasets, and (iii) it adds only a small number\\nof additional parameters per task. These properties are\\nespecially useful in the context of cloud services, where\\nmany models need to be trained on a series of downstream\\ntasks, so a high degree of sharing is desirable.\\nTo achieve these properties, we propose a new bottleneck\\nadapter module. Tuning with adapter modules involves\\nadding a small number of new parameters to a model, which\\nare trained on the downstream task (RebufÔ¨Å et al., 2017).\\nWhen performing vanilla Ô¨Åne-tuning of deep networks, a\\nmodiÔ¨Åcation is made to the top layer of the network. This is\\nrequired because the label spaces and losses for the upstream\\nand downstream tasks differ. Adapter modules perform\\nmore general architectural modiÔ¨Åcations to re-purpose a pre-\\ntrained network for a downstream task. In particular, the\\nadapter tuning strategy involves injecting new layers into\\nthe original network. The weights of the original network\\nare untouched, whilst the new adapter layers are initialized\\nat random. In standard Ô¨Åne-tuning, the new top-layer and\\nthe original weights are co-trained. In contrast, in adapter-\\ntuning, the parameters of the original network are frozen\\nand therefore may be shared by many tasks.\\nAdapter modules have two main features: a small number\\nof parameters, and a near-identity initialization. The adapter\\nmodules need to be small compared to the layers of the orig-\\ninal network. This means that the total model size grows\\nrelatively slowly when more tasks are added. A near-identity\\ninitialization is required for stable training of the adapted\\nmodel; we investigate this empirically in Section 3.6. By\\ninitializing the adapters to a near-identity function, original\\nnetwork is unaffected when training starts. During training,\\nthe adapters may then be activated to change the distribution\\nof activations throughout the network. The adapter mod-\\nules may also be ignored if not required; in Section 3.6 we\\nobserve that some adapters have more inÔ¨Çuence on the net-\\nwork than others. We also observe that if the initialization\\ndeviates too far from the identity function, the model may\\nfail to train.\\n2.1. Instantiation for Transformer Networks\\nWe instantiate adapter-based tuning for text Transformers.\\nThese models attain state-of-the-art performance in many\\nNLP tasks, including translation, extractive QA, and text'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PE_Transfer_Learning.pdf', 'page': 2}, page_content='Parameter-EfÔ¨Åcient Transfer Learning for NLP\\nMulti-headed \\nattention Layer Norm \\n+\\nAdapter 2x Feed-forward \\nlayer Layer Norm \\n+\\nAdapter \\nFeed-forward layer Transformer \\nLayer \\nNonlinearity Feedforward \\nup-project \\nFeedforward \\ndown-project Adapter \\nLayer +\\nFigure 2. Architecture of the adapter module and its integration\\nwith the Transformer. Left: We add the adapter module twice\\nto each Transformer layer: after the projection following multi-\\nheaded attention and after the two feed-forward layers. Right: The\\nadapter consists of a bottleneck which contains few parameters rel-\\native to the attention and feedforward layers in the original model.\\nThe adapter also contains a skip-connection. During adapter tun-\\ning, the green layers are trained on the downstream data, this\\nincludes the adapter, the layer normalization parameters, and the\\nÔ¨Ånal classiÔ¨Åcation layer (not shown in the Ô¨Ågure).\\nclassiÔ¨Åcation problems (Vaswani et al., 2017; Radford et al.,\\n2018; Devlin et al., 2018). We consider the standard Trans-\\nformer architecture, as proposed in Vaswani et al. (2017).\\nAdapter modules present many architectural choices. We\\nprovide a simple design that attains good performance. We\\nexperimented with a number of more complex designs, see\\nSection 3.6, but we found the following strategy performed\\nas well as any other that we tested, across many datasets.\\nFigure 2 shows our adapter architecture, and its application\\nit to the Transformer. Each layer of the Transformer contains\\ntwo primary sub-layers: an attention layer and a feedforward\\nlayer. Both layers are followed immediately by a projection\\nthat maps the features size back to the size of layer‚Äôs input.\\nA skip-connection is applied across each of the sub-layers.\\nThe output of each sub-layer is fed into layer normalization.\\nWe insert two serial adapters after each of these sub-layers.\\nThe adapter is always applied directly to the output of the\\nsub-layer, after the projection back to the input size, but\\nbefore adding the skip connection back. The output of\\nthe adapter is then passed directly into the following layer\\nnormalization.\\nTo limit the number of parameters, we propose a bottle-\\nneck architecture. The adapters Ô¨Årst project the original\\nd-dimensional features into a smaller dimension, m, apply\\na nonlinearity, then project back to ddimensions. The total\\nnumber of parameters added per layer, including biases, is\\n2md+d+m. By setting m‚â™d, we limit the number\\nof parameters added per task; in practice, we use around\\n0.5‚àí8%of the parameters of the original model. The\\nbottleneck dimension, m, provides a simple means to trade-\\noff performance with parameter efÔ¨Åciency. The adapter\\nmodule itself has a skip-connection internally. With the\\nskip-connection, if the parameters of the projection layers\\nare initialized to near-zero, the module is initialized to an\\napproximate identity function.\\nAlongside the layers in the adapter module, we also train\\nnew layer normalization parameters per task. This tech-nique, similar to conditional batch normalization (De Vries\\net al., 2017), FiLM (Perez et al., 2018), and self-\\nmodulation (Chen et al., 2019), also yields parameter-\\nefÔ¨Åcient adaptation of a network; with only 2dparameters\\nper layer. However, training the layer normalization pa-\\nrameters alone is insufÔ¨Åcient for good performance, see\\nSection 3.4.\\n3. Experiments\\nWe show that adapters achieve parameter efÔ¨Åcient transfer\\nfor text tasks. On the GLUE benchmark (Wang et al., 2018),\\nadapter tuning is within 0.4%of full Ô¨Åne-tuning of BERT,\\nbut it adds only 3%of the number of parameters trained by\\nÔ¨Åne-tuning. We conÔ¨Årm this result on a further 17public\\nclassiÔ¨Åcation tasks and SQuAD question answering. Analy-\\nsis shows that adapter-based tuning automatically focuses\\non the higher layers of the network.\\n3.1. Experimental Settings\\nWe use the public, pre-trained BERT Transformer network\\nas our base model. To perform classiÔ¨Åcation with BERT,\\nwe follow the approach in Devlin et al. (2018). The Ô¨Årst\\ntoken in each sequence is a special ‚ÄúclassiÔ¨Åcation token‚Äù.\\nWe attach a linear layer to the embedding of this token to\\npredict the class label.\\nOur training procedure also follows Devlin et al. (2018).\\nWe optimize using Adam (Kingma & Ba, 2014), whose\\nlearning rate is increased linearly over the Ô¨Årst 10% of the\\nsteps, and then decayed linearly to zero. All runs are trained\\non4Google Cloud TPUs with a batch size of 32. For each\\ndataset and algorithm, we run a hyperparameter sweep and\\nselect the best model according to accuracy on the validation\\nset. For the GLUE tasks, we report the test metrics provided\\nby the submission website2. For the other classiÔ¨Åcation\\ntasks we report test-set accuracy.\\n2https://gluebenchmark.com/'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PE_Transfer_Learning.pdf', 'page': 3}, page_content='Parameter-EfÔ¨Åcient Transfer Learning for NLP\\nWe compare to Ô¨Åne-tuning, the current standard for transfer\\nof large pre-trained models, and the strategy successfully\\nused by BERT. For Ntasks, full Ô¨Åne-tuning requires N√ó\\nthe number of parameters of the pre-trained model. Our\\ngoal is to attain performance equal to Ô¨Åne-tuning, but with\\nfewer total parameters, ideally near to 1√ó.\\n3.2. GLUE benchmark\\nWe Ô¨Årst evaluate on GLUE.3For these datasets, we trans-\\nfer from the pre-trained BERT LARGE model, which con-\\ntains24layers, and a total of 330M parameters, see Devlin\\net al. (2018) for details. We perform a small hyperparam-\\neter sweep for adapter tuning: We sweep learning rates\\nin{3¬∑10‚àí5,3¬∑10‚àí4,3¬∑10‚àí3}, and number of epochs\\nin{3,20}. We test both using a Ô¨Åxed adapter size (num-\\nber of units in the bottleneck), and selecting the best size\\nper task from{8,64,256}. The adapter size is the only\\nadapter-speciÔ¨Åc hyperparameter that we tune. Finally, due\\nto training instability, we re-run 5times with different ran-\\ndom seeds and select the best model on the validation set.\\nTable 1 summarizes the results. Adapters achieve a mean\\nGLUE score of 80.0, compared to 80.4achieved by full\\nÔ¨Åne-tuning. The optimal adapter size varies per dataset. For\\nexample, 256is chosen for MNLI, whereas for the smallest\\ndataset, RTE, 8is chosen. Restricting always to size 64,\\nleads to a small decrease in average accuracy to 79.6. To\\nsolve all of the datasets in Table 1, Ô¨Åne-tuning requires 9√ó\\nthe total number of BERT parameters.4In contrast, adapters\\nrequire only 1.3√óparameters.\\n3.3. Additional ClassiÔ¨Åcation Tasks\\nTo further validate that adapters yields compact, performant,\\nmodels, we test on additional, publicly available, text clas-\\nsiÔ¨Åcation tasks. This suite contains a diverse set of tasks:\\nThe number of training examples ranges from 900to330k,\\nthe number of classes ranges from 2to157, and the average\\ntext length ranging from 57to1.9k characters. Statistics\\nand references for all of the datasets are in the appendix.\\nFor these datasets, we use a batch size of 32. The datasets\\nare diverse, so we sweep a wide range of learning rates:\\n{1¬∑10‚àí5,3¬∑10‚àí5,1¬∑10‚àí4,3¬∑10‚àí3}. Due to the large\\nnumber of datasets, we select the number of training epochs\\nfrom the set{20,50,100}manually, from inspection of the\\nvalidation set learning curves. We select the optimal values\\nfor both Ô¨Åne-tuning and adapters; the exact values are in the\\nappendix.\\n3We omit WNLI as in Devlin et al. (2018) because the no\\ncurrent algorithm beats the baseline of predicting the majority\\nclass.\\n4We treat MNLI mand MNLI mmas separate tasks with individ-\\nually tuned hyperparameters. However, they could be combined\\ninto one model, leaving 8√óoverall.We test adapters sizes in {2,4,8,16,32,64}. Since some\\nof the datasets are small, Ô¨Åne-tuning the entire network\\nmay be sub-optimal. Therefore, we run an additional base-\\nline: variable Ô¨Åne-tuning. For this, we Ô¨Åne-tune only\\nthe topnlayers, and freeze the remainder. We sweep\\nn‚àà{1,2,3,5,7,9,11,12}. In these experiments, we use\\nthe BERT BASE model with 12layers, therefore, variable\\nÔ¨Åne-tuning subsumes full Ô¨Åne-tuning when n= 12 .\\nUnlike the GLUE tasks, there is no comprehensive set of\\nstate-of-the-art numbers for this suite of tasks. Therefore, to\\nconÔ¨Årm that our BERT-based models are competitive, we\\ncollect our own benchmark performances. For this, we run\\na large-scale hyperparameter search over standard network\\ntopologies. SpeciÔ¨Åcally, we run the single-task Neural Au-\\ntoML algorithm, similar to Zoph & Le (2017); Wong et al.\\n(2018). This algorithm searches over a space of feedfor-\\nward and convolutional networks, stacked on pre-trained\\ntext embeddings modules publicly available via TensorFlow\\nHub5. The embeddings coming from the TensorFlow Hub\\nmodules may be frozen or Ô¨Åne-tuned. The full search space\\nis described in the appendix. For each task, we run AutoML\\nfor one week on CPUs, using 30machines. In this time\\nthe algorithm explores over 10k models on average per task.\\nWe select the best Ô¨Ånal model for each task according to\\nvalidation set accuracy.\\nThe results for the AutoML benchmark (‚Äúno BERT base-\\nline‚Äù), Ô¨Åne-tuning, variable Ô¨Åne-tuning, and adapter-tuning\\nare reported in Table 2. The AutoML baseline demon-\\nstrates that the BERT models are competitive. This baseline\\nexplores thousands of models, yet the BERT models per-\\nform better on average. We see similar pattern of results to\\nGLUE. The performance of adapter-tuning is close to full\\nÔ¨Åne-tuning ( 0.4%behind). Fine-tuning requires 17√óthe\\nnumber of parameters to BERT BASE to solve all tasks. Vari-\\nable Ô¨Åne-tuning performs slightly better than Ô¨Åne-tuning,\\nwhilst training fewer layers. The optimal setting of variable\\nÔ¨Åne-tuning results in training 52% of the network on average\\nper task, reducing the total to 9.9√óparameters. Adapters,\\nhowever, offer a much more compact model. They intro-\\nduce 1.14% new parameters per task, resulting in 1.19√ó\\nparameters for all 17tasks.\\n3.4. Parameter/Performance trade-off\\nThe adapter size controls the parameter efÔ¨Åciency, smaller\\nadapters introduce fewer parameters, at a possible cost to\\nperformance. To explore this trade-off, we consider different\\nadapter sizes, and compare to two baselines: (i) Fine-tuning\\nof only the top klayers of BERT BASE. (ii) Tuning only the\\nlayer normalization parameters. The learning rate is tuned\\nusing the range presented in Section 3.2.\\n5https://www.tensorflow.org/hub'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PE_Transfer_Learning.pdf', 'page': 4}, page_content='Parameter-EfÔ¨Åcient Transfer Learning for NLP\\nTotal num\\nparamsTrained\\nparams / taskCoLA SST MRPC STS-B QQP MNLI mMNLI mm QNLI RTE Total\\nBERT LARGE 9.0√ó 100% 60.5 94.9 89.3 87.6 72.1 86.7 85.9 91.1 70.180.4\\nAdapters ( 8-256)1.3√ó 3.6% 59.5 94.0 89.5 86.9 71.8 84.9 85.1 90.7 71.580.0\\nAdapters ( 64) 1.2√ó 2.1% 56.9 94.2 89.6 87.3 71.8 85.3 84.6 91.4 68.879.6\\nTable 1. Results on GLUE test sets scored using the GLUE evaluation server. MRPC and QQP are evaluated using F1 score. STS-B is\\nevaluated using Spearman‚Äôs correlation coefÔ¨Åcient. CoLA is evaluated using Matthew‚Äôs Correlation. The other tasks are evaluated using\\naccuracy. Adapter tuning achieves comparable overall score ( 80.0) to full Ô¨Åne-tuning ( 80.4) using 1.3√óparameters in total, compared to\\n9√ó. Fixing the adapter size to 64leads to a slightly decreased overall score of 79.6and slightly smaller model.\\nDatasetNo BERT\\nbaselineBERT BASE\\nFine-tuneBERT BASE\\nVariable FTBERT BASE\\nAdapters\\n20 newsgroups 91.1 92.8¬±0.1 92.8¬±0.1 91.7¬±0.2\\nCrowdÔ¨Çower airline 84.5 83.6¬±0.3 84.0¬±0.1 84.5¬±0.2\\nCrowdÔ¨Çower corporate messaging 91.9 92.5¬±0.5 92.4¬±0.6 92.9¬±0.3\\nCrowdÔ¨Çower disasters 84.9 85.3¬±0.4 85.3¬±0.4 84.1¬±0.2\\nCrowdÔ¨Çower economic news relevance 81.1 82.1¬±0.0 78.9¬±2.8 82.5¬±0.3\\nCrowdÔ¨Çower emotion 36.3 38.4¬±0.1 37.6¬±0.2 38.7¬±0.1\\nCrowdÔ¨Çower global warming 82.7 84.2¬±0.4 81.9¬±0.2 82.7¬±0.3\\nCrowdÔ¨Çower political audience 81.0 80.9¬±0.3 80.7¬±0.8 79.0¬±0.5\\nCrowdÔ¨Çower political bias 76.8 75.2¬±0.9 76.5¬±0.4 75.9¬±0.3\\nCrowdÔ¨Çower political message 43.8 38.9¬±0.6 44.9¬±0.6 44.1¬±0.2\\nCrowdÔ¨Çower primary emotions 33.5 36.9¬±1.6 38.2¬±1.0 33.9¬±1.4\\nCrowdÔ¨Çower progressive opinion 70.6 71.6¬±0.5 75.9¬±1.3 71.7¬±1.1\\nCrowdÔ¨Çower progressive stance 54.3 63.8¬±1.0 61.5¬±1.3 60.6¬±1.4\\nCrowdÔ¨Çower US economic performance 75.6 75.3¬±0.1 76.5¬±0.4 77.3¬±0.1\\nCustomer complaint database 54.5 55.9¬±0.1 56.4¬±0.1 55.4¬±0.1\\nNews aggregator dataset 95.2 96.3¬±0.0 96.5¬±0.0 96.2¬±0.0\\nSMS spam collection 98.5 99.3¬±0.2 99.3¬±0.2 95.1¬±2.2\\nAverage 72.7 73 .7 74 .0 73 .3\\nTotal number of params ‚Äî 17√ó 9.9√ó 1.19√ó\\nTrained params/task ‚Äî 100% 52 .9% 1.14%\\nTable 2. Test accuracy for additional classiÔ¨Åcation tasks. In these experiments we transfer from the BERT BASE model. For each task\\nand algorithm, the model with the best validation set accuracy is chosen. We report the mean test accuracy and s.e.m. across runs with\\ndifferent random seeds.\\nFigure 3 shows the parameter/performance trade-off ag-\\ngregated over all classiÔ¨Åcation tasks in each suite (GLUE\\nand ‚Äúadditional‚Äù). On GLUE, performance decreases dra-\\nmatically when fewer layers are Ô¨Åne-tuned. Some of the\\nadditional tasks beneÔ¨Åt from training fewer layers, so per-\\nformance of Ô¨Åne-tuning decays much less. In both cases,\\nadapters yield good performance across a range of sizes two\\norders of magnitude fewer than Ô¨Åne-tuning.\\nFigure 4 shows more details for two GLUE tasks: MNLI m\\nand CoLA. Tuning the top layers trains more task-speciÔ¨Åc\\nparameters for all k>2. When Ô¨Åne-tuning using a compa-\\nrable number of task-speciÔ¨Åc parameters, the performance\\ndecreases substantially compared to adapters. For instance,\\nÔ¨Åne-tuning just the top layer yields approximately 9M train-\\nable parameters and 77.8%¬±0.1%validation accuracy on\\nMNLI m. In contrast, adapter tuning with size 64yields ap-\\nproximately 2M trainable parameters and 83.7%¬±0.1%validation accuracy. For comparison, full Ô¨Åne-tuning attains\\n84.4%¬±0.02% on MNLI m. We observe a similar trend on\\nCoLA.\\nAs a further comparison, we tune the parameters of layer\\nnormalization alone. These layers only contain point-wise\\nadditions and multiplications, so introduce very few train-\\nable parameters: 40k for BERT BASE. However this strategy\\nperforms poorly: performance decreases by approximately\\n3.5%on CoLA and 4%on MNLI.\\nTo summarize, adapter tuning is highly parameter-efÔ¨Åcient,\\nand produces a compact model with a strong performance,\\ncomparable to full Ô¨Åne-tuning. Training adapters with sizes\\n0.5‚àí5%of the original model, performance is within 1%\\nof the competitive published results on BERT LARGE .'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PE_Transfer_Learning.pdf', 'page': 5}, page_content='Parameter-EfÔ¨Åcient Transfer Learning for NLP\\nGLUE (BERT LARGE ) Additional Tasks (BERT BASE)\\n105106107108109\\nNum trainable parameters / task‚àí25‚àí20‚àí15‚àí10‚àí505Accuracy delta (%)\\nAdapters\\nFine-tune top layers\\n105106107108\\nNum trainable parameters / task‚àí4‚àí3‚àí2‚àí10123Accuracy delta (%)\\nAdapters\\nFine-tune top layers\\nFigure 3. Accuracy versus the number of trained parameters, aggregated across tasks. We compare adapters of different sizes (orange)\\nwith Ô¨Åne-tuning the top nlayers, for varying n(blue). The lines and shaded areas indicate the 20th,50th, and 80th percentiles across\\ntasks. For each task and algorithm, the best model is selected for each point along the curve. For GLUE, the validation set accuracy is\\nreported. For the additional tasks, we report the test-set accuracies. To remove the intra-task variance in scores, we normalize the scores\\nfor each model and task by subtracting the performance of full Ô¨Åne-tuning on the corresponding task.\\nMNLI m(BERT BASE) CoLA (BERT BASE)\\n104105106107108\\nNum trainable parameters / task767880828486Validation accuracy (%)\\nLayer Norm.\\nAdapters\\nFine-tune top layers\\n104105106107108\\nNum trainable parameters / task74767880828486Validation accuracy (%)\\nLayer Norm.\\nAdapters\\nFine-tune top layers\\nFigure 4. Validation set accuracy versus number of trained parameters for three methods: (i) Adapter tuning with an adapter sizes 2n\\nforn= 0. . .9(orange). (ii) Fine-tuning the top klayers for k= 1. . .12(blue). (iii) Tuning the layer normalization parameters only\\n(green). Error bars indicate ¬±1s.e.m. across three random seeds.\\n3.5. SQuAD Extractive Question Answering\\nFinally, we conÔ¨Årm that adapters work on tasks other than\\nclassiÔ¨Åcation by running on SQuAD v1.1 (Rajpurkar et al.,\\n2018). Given a question and Wikipedia paragraph, this task\\nrequires selecting the answer span to the question from the\\nparagraph. Figure 5 displays the parameter/performance\\ntrade-off of Ô¨Åne-tuning and adapters on the SQuAD valida-\\ntion set. For Ô¨Åne-tuning, we sweep the number of trained lay-\\ners, learning rate in {3¬∑10‚àí5,5¬∑10‚àí5,1¬∑10‚àí4}, and number\\nof epochs in{2,3,5}. For adapters, we sweep the adapter\\nsize, learning rate in {3¬∑10‚àí5,1¬∑10‚àí4,3¬∑10‚àí4,1¬∑10‚àí3},\\nand number of epochs in {3,10,20}. As for classiÔ¨Åcation,\\nadapters attain performance comparable to full Ô¨Åne-tuning,\\nwhile training many fewer parameters. Adapters of size 64\\n(2%parameters) attain a best F1 of 90.4%, while Ô¨Åne-tuningattains 90.7. SQuAD performs well even with very small\\nadapters, those of size 2(0.1%parameters) attain an F1 of\\n89.9.\\n3.6. Analysis and Discussion\\nWe perform an ablation to determine which adapters are\\ninÔ¨Çuential. For this, we remove some trained adapters and\\nre-evaluate the model (without re-training) on the valida-\\ntion set. Figure 6 shows the change in performance when\\nremoving adapters from all continuous layer spans. The\\nexperiment is performed on BERT BASE with adapter size 64\\non MNLI and CoLA.\\nFirst, we observe that removing any single layer‚Äôs adapters\\nhas only a small impact on performance. The elements on'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PE_Transfer_Learning.pdf', 'page': 6}, page_content='Parameter-EfÔ¨Åcient Transfer Learning for NLP\\n105106107108109\\nNum trainable parameters707580859095f1 score\\nAdapters\\nFine-tune top layers\\nFigure 5. Validation accuracy versus the number of trained param-\\neters for SQuAD v1.1. Error bars indicate the s.e.m. across three\\nseeds, using the best hyperparameters.\\nthe heatmaps‚Äô diagonals show the performances of removing\\nadapters from single layers, where largest performance drop\\nis2%. In contrast, when all of the adapters are removed\\nfrom the network, the performance drops substantially: to\\n37% on MNLI and 69% on CoLA ‚Äì scores attained by\\npredicting the majority class. This indicates that although\\neach adapter has a small inÔ¨Çuence on the overall network,\\nthe overall effect is large.\\nSecond, Figure 6 suggests that adapters on the lower layers\\nhave a smaller impact than the higher-layers. Removing\\nthe adapters from the layers 0‚àí4on MNLI barely affects\\nperformance. This indicates that adapters perform well\\nbecause they automatically prioritize higher layers. Indeed,\\nfocusing on the upper layers is a popular strategy in Ô¨Åne-\\ntuning (Howard & Ruder, 2018). One intuition is that the\\nlower layers extract lower-level features that are shared\\namong tasks, while the higher layers build features that are\\nunique to different tasks. This relates to our observation that\\nfor some tasks, Ô¨Åne-tuning only the top layers outperforms\\nfull Ô¨Åne-tuning, see Table 2.\\nNext, we investigate the robustness of the adapter modules\\nto the number of neurons and initialization scale. In our\\nmain experiments the weights in the adapter module were\\ndrawn from a zero-mean Gaussian with standard deviation\\n10‚àí2, truncated to two standard deviations. To analyze the\\nimpact of the initialization scale on the performance, we\\ntest standard deviations in the interval [10‚àí7,1]. Figure 6\\nsummarizes the results. We observe that on both datasets,\\nthe performance of adapters is robust for standard deviations\\nbelow 10‚àí2. However, when the initialization is too large,\\nperformance degrades, more substantially on CoLA.\\nTo investigate robustness of adapters to the number of neu-\\nrons, we re-examine the experimental data from Section 3.2.\\nWe Ô¨Ånd that the quality of the model across adapter sizes is\\nstable, and a Ô¨Åxed adapter size across all the tasks could be\\nused with small detriment to performance. For each adaptersize we calculate the mean validation accuracy across the\\neight classiÔ¨Åcation tasks by selecting the optimal learning\\nrate and number of epochs6. For adapter sizes 8,64, and\\n256, the mean validation accuracies are 86.2%,85.8%and\\n85.7%, respectively. This message is further corroborated\\nby Figures 4 and 5, which show a stable performance across\\na few orders of magnitude.\\nFinally, we tried a number of extensions to the adapter‚Äôs\\narchitecture that did not yield a signiÔ¨Åcant boost in perfor-\\nmance. We document them here for completeness. We\\nexperimented with (i) adding a batch/layer normalization to\\nthe adapter, (ii) increasing the number of layers per adapter,\\n(iii) different activation functions, such as tanh, (iv) inserting\\nadapters only inside the attention layer, (v) adding adapters\\nin parallel to the main layers, and possibly with a multi-\\nplicative interaction. In all cases we observed the resulting\\nperformance to be similar to the bottleneck proposed in\\nSection 2.1. Therefore, due to its simplicity and strong per-\\nformance, we recommend the original adapter architecture.\\n4. Related Work\\nPre-trained text representations Pre-trained textual rep-\\nresentations are widely used to improve performance on\\nNLP tasks. These representations are trained on large cor-\\npora (usually unsupervised), and fed as features to down-\\nstream models. In deep networks, these features may also be\\nÔ¨Åne-tuned on the downstream task. Brown clusters, trained\\non distributional information, are a classic example of pre-\\ntrained representations (Brown et al., 1992). Turian et al.\\n(2010) show that pre-trained embeddings of words outper-\\nform those trained from scratch. Since deep-learning be-\\ncame popular, word embeddings have been widely used, and\\nmany training strategies have arisen (Mikolov et al., 2013;\\nPennington et al., 2014; Bojanowski et al., 2017). Embed-\\ndings of longer texts, sentences and paragraphs, have also\\nbeen developed (Le & Mikolov, 2014; Kiros et al., 2015;\\nConneau et al., 2017; Cer et al., 2019).\\nTo encode context in these representations, features are\\nextracted from internal representations of sequence models,\\nsuch as MT systems (McCann et al., 2017), and BiLSTM\\nlanguage models, as used in ELMo (Peters et al., 2018). As\\nwith adapters, ELMo exploits the layers other than the top\\nlayer of a pre-trained network. However, this strategy only\\nreads from the inner layers. In contrast, adapters write to\\nthe inner layers, re-conÔ¨Åguring the processing of features\\nthrough the entire network.\\nFine-tuning Fine-tuning an entire pre-trained model has\\nbecome a popular alternative to features (Dai & Le, 2015;\\n6We treat here MNLI mand MNLI mmas separate tasks. For\\nconsistency, for all datasets we use accuracy metric and exclude\\nthe regression STS-B task.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PE_Transfer_Learning.pdf', 'page': 7}, page_content='Parameter-EfÔ¨Åcient Transfer Learning for NLP\\nMNLI m CoLA\\n01234567891011\\nLast ablated layer01234567891011First ablated layer\\n‚àí40‚àí32‚àí24‚àí16‚àí80\\n01234567891011\\nLast ablated layer01234567891011First ablated layer\\n‚àí12‚àí9‚àí6‚àí30\\n10-710-610-510-410-310-210-1\\n¬æ657075808590Validation accuracy (%)\\nMNLIm\\nCoLA\\nFigure 6. Left, Center: Ablation of trained adapters from continuous layer spans. The heatmap shows the relative decrease in validation\\naccuracy to the fully trained adapted model. The yandxaxes indicate the Ô¨Årst and last layers ablated (inclusive), respectively. The\\ndiagonal cells, highlighted in green, indicate ablation of a single layer‚Äôs adapters. The cell in the top-right indicates ablation of all adapters.\\nCells in the lower triangle are meaningless, and are set to 0%, the best possible relative performance. Right: Performance of BERT BASE\\nusing adapters with different initial weight magnitudes. The x-axis is the standard deviation of the initialization distribution.\\nHoward & Ruder, 2018; Radford et al., 2018) In NLP, the\\nupstream model is usually a neural language model (Ben-\\ngio et al., 2003). Recent state-of-the-art results on ques-\\ntion answering (Rajpurkar et al., 2016) and text classi-\\nÔ¨Åcation (Wang et al., 2018) have been attained by Ô¨Åne-\\ntuning a Transformer network (Vaswani et al., 2017) with a\\nMasked Language Model loss (Devlin et al., 2018). Perfor-\\nmance aside, an advantage of Ô¨Åne-tuning is that it does not\\nrequire task-speciÔ¨Åc model design, unlike representation-\\nbased transfer. However, vanilla Ô¨Åne-tuning does require a\\nnew set of network weights for every new task.\\nMulti-task Learning Multi-task learning (MTL) involves\\ntraining on tasks simultaneously. Early work shows that\\nsharing network parameters across tasks exploits task reg-\\nularities, yielding improved performance (Caruana, 1997).\\nThe authors share weights in lower layers of a network,\\nand use specialized higher layers. Many NLP systems have\\nexploited MTL. Some examples include: text processing\\nsystems (part of speech, chunking, named entity recogni-\\ntion, etc.) (Collobert & Weston, 2008), multilingual mod-\\nels (Huang et al., 2013), semantic parsing (Peng et al., 2017),\\nmachine translation (Johnson et al., 2017), and question an-\\nswering (Choi et al., 2017). MTL yields a single model\\nto solve all problems. However, unlike our adapters, MTL\\nrequires simultaneous access to the tasks during training.\\nContinual Learning As an alternative to simultaneous\\ntraining, continual, or lifelong, learning aims to learn from a\\nsequence of tasks (Thrun, 1998). However, when re-trained,\\ndeep networks tend to forget how to perform previous tasks;\\na challenge termed catastrophic forgetting (McCloskey &\\nCohen, 1989; French, 1999). Techniques have been pro-\\nposed to mitigate forgetting (Kirkpatrick et al., 2017; Zenke\\net al., 2017), however, unlike for adapters, the memory is\\nimperfect. Progressive Networks avoid forgetting by instan-\\ntiating a new network ‚Äúcolumn‚Äù for each task (Rusu et al.,\\n2016). However, the number of parameters grows linearlywith the number of tasks, since adapters are very small, our\\nmodels scale much more favorably.\\nTransfer Learning in Vision Fine-tuning models pre-\\ntrained on ImageNet (Deng et al., 2009) is ubiquitous when\\nbuilding image recognition models (Yosinski et al., 2014;\\nHuh et al., 2016). This technique attains state-of-the-art per-\\nformance on many vision tasks, including classiÔ¨Åcation (Ko-\\nrnblith et al., 2018), Ô¨Åne-grained classifcation (Hermans\\net al., 2017), segmentation (Long et al., 2015), and de-\\ntection (Girshick et al., 2014). In vision, convolutional\\nadapter modules have been studied (RebufÔ¨Å et al., 2017;\\n2018; Rosenfeld & Tsotsos, 2018). These works perform\\nincremental learning in multiple domains by adding small\\nconvolutional layers to a ResNet (He et al., 2016) or VGG\\nnet (Simonyan & Zisserman, 2014). Adapter size is lim-\\nited using 1√ó1convolutions, whilst the original networks\\ntypically use 3√ó3. This yields 11% increase in overall\\nmodel size per task. Since the kernel size cannot be further\\nreduced other weight compression techniques must be used\\nto attain further savings. Our bottleneck adapters can be\\nmuch smaller, and still perform well.\\nConcurrent work explores similar ideas for BERT (Stickland\\n& Murray, 2019). The authors introduce Projected Atten-\\ntion Layers (PALs), small layers with a similar role to our\\nadapters. The main differences are i) Stickland & Murray\\n(2019) use a different architecture, and ii) they perform mul-\\ntitask training, jointly Ô¨Åne-tuning BERT on all GLUE tasks.\\nSina Semnani (2019) perform an emprical comparison of\\nour bottleneck Adpaters and PALs on SQuAD v2.0 (Ra-\\njpurkar et al., 2018).\\nACKNOWLEDGMENTS\\nWe would like to thank Andrey Khorlin, Lucas Beyer, No√©\\nLutz, and Jeremiah Harmsen for useful comments and dis-\\ncussions.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PE_Transfer_Learning.pdf', 'page': 8}, page_content='Parameter-EfÔ¨Åcient Transfer Learning for NLP\\nReferences\\nAlmeida, T. A., Hidalgo, J. M. G., and Yamakami, A. Con-\\ntributions to the Study of SMS Spam Filtering: New\\nCollection and Results. In Proceedings of the 11th ACM\\nSymposium on Document Engineering . ACM, 2011.\\nBengio, Y ., Ducharme, R., Vincent, P., and Janvin, C. A\\nneural probabilistic language model. Journal of Machine\\nLearning Research , 2003.\\nBojanowski, P., Grave, E., Joulin, A., and Mikolov, T. En-\\nriching word vectors with subword information. ACL,\\n2017.\\nBrown, P. F., deSouza, P. V ., Mercer, R. L., Pietra, V . J. D.,\\nand Lai, J. C. Class-based n-gram models of natural\\nlanguage. Computational Linguistics , 1992.\\nCaruana, R. Multitask learning. Machine Learning , 1997.\\nCer, D., Yang, Y ., Kong, S.-y., Hua, N., Limtiaco, N., John,\\nR. S., Constant, N., Guajardo-Cespedes, M., Yuan, S.,\\nTar, C., et al. Universal sentence encoder. arXiv preprint\\narXiv:1803.11175 , 2018.\\nCer, D., Yang, Y ., Kong, S.-y., Hua, N., Limtiaco, N.,\\nSt. John, R., Constant, N., Guajardo-Cespedes, M., Yuan,\\nS., Tar, C., Strope, B., and Kurzweil, R. Universal sen-\\ntence encoder for english. In EMNLP , 2019.\\nChen, T., Lucic, M., Houlsby, N., and Gelly, S. On self\\nmodulation for generative adversarial networks. ICLR ,\\n2019.\\nChoi, E., Hewlett, D., Uszkoreit, J., Polosukhin, I., Lacoste,\\nA., and Berant, J. Coarse-to-Ô¨Åne question answering for\\nlong documents. In ACL, 2017.\\nCollobert, R. and Weston, J. A uniÔ¨Åed architecture for\\nnatural language processing: Deep neural networks with\\nmultitask learning. In ICML , 2008.\\nConneau, A., Kiela, D., Schwenk, H., Barrault, L., and\\nBordes, A. Supervised learning of universal sentence\\nrepresentations from natural language inference data. In\\nEMNLP , 2017.\\nDai, A. M. and Le, Q. V . Semi-supervised sequence learning.\\nInNIPS . 2015.\\nDe Vries, H., Strub, F., Mary, J., Larochelle, H., Pietquin, O.,\\nand Courville, A. C. Modulating early visual processing\\nby language. In NIPS , 2017.\\nDeng, J., Dong, W., Socher, R., jia Li, L., Li, K., and Fei-fei,\\nL. Imagenet: A large-scale hierarchical image database.\\nInIn CVPR , 2009.Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. arXiv preprint arXiv:1810.04805 ,\\n2018.\\nFrench, R. M. Catastrophic forgetting in connectionist net-\\nworks. Trends in cognitive sciences , 1999.\\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. Rich\\nfeature hierarchies for accurate object detection and se-\\nmantic segmentation. In CVPR , 2014.\\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\\nlearning for image recognition. In CVPR , 2016.\\nHermans, A., Beyer, L., and Leibe, B. In Defense of the\\nTriplet Loss for Person Re-IdentiÔ¨Åcation. arXiv preprint\\narXiv:1703.07737 , 2017.\\nHoward, J. and Ruder, S. Universal language model Ô¨Åne-\\ntuning for text classiÔ¨Åcation. In ACL, 2018.\\nHuang, J.-T., Li, J., Yu, D., Deng, L., and Gong, Y . Cross-\\nlanguage knowledge transfer using multilingual deep neu-\\nral network with shared hidden layers. In ICASSP , 2013.\\nHuh, M., Agrawal, P., and Efros, A. A. What makes\\nimagenet good for transfer learning? arXiv preprint\\narXiv:1608.08614 , 2016.\\nJohnson, M., Schuster, M., Le, Q. V ., Krikun, M., Wu, Y .,\\nChen, Z., Thorat, N., Vi√©gas, F., Wattenberg, M., Corrado,\\nG., Hughes, M., and Dean, J. Google‚Äôs multilingual\\nneural machine translation system: Enabling zero-shot\\ntranslation. ACL, 2017.\\nKingma, D. and Ba, J. Adam: A method for stochastic\\noptimization. ICLR , 2014.\\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Des-\\njardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T.,\\nGrabska-Barwinska, A., et al. Overcoming catastrophic\\nforgetting in neural networks. PNAS , 2017.\\nKiros, R., Zhu, Y ., Salakhutdinov, R. R., Zemel, R., Urtasun,\\nR., Torralba, A., and Fidler, S. Skip-thought vectors. In\\nNIPS . 2015.\\nKornblith, S., Shlens, J., and Le, Q. V . Do better imagenet\\nmodels transfer better? arXiv preprint arXiv:1805.08974 ,\\n2018.\\nLang, K. Newsweeder: Learning to Ô¨Ålter netnews. In ICML ,\\n1995.\\nLe, Q. and Mikolov, T. Distributed representations of sen-\\ntences and documents. In ICML , 2014.\\nLichman, M. UCI machine learning repository, 2013.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PE_Transfer_Learning.pdf', 'page': 9}, page_content='Parameter-EfÔ¨Åcient Transfer Learning for NLP\\nLong, J., Shelhamer, E., and Darrell, T. Fully convolutional\\nnetworks for semantic segmentation. In CVPR , 2015.\\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R.\\nLearned in translation: Contextualized word vectors. In\\nGuyon, I., Luxburg, U. V ., Bengio, S., Wallach, H., Fer-\\ngus, R., Vishwanathan, S., and Garnett, R. (eds.), NIPS .\\n2017.\\nMcCloskey, M. and Cohen, N. J. Catastrophic interference\\nin connectionist networks: The sequential learning prob-\\nlem. In Psychology of learning and motivation . 1989.\\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\\nDean, J. Distributed representations of words and phrases\\nand their compositionality. In NIPS . 2013.\\nPeng, H., Thomson, S., and Smith, N. A. Deep multitask\\nlearning for semantic dependency parsing. In ACL, 2017.\\nPennington, J., Socher, R., and Manning, C. Glove: Global\\nvectors for word representation. In EMNLP , 2014.\\nPerez, E., Strub, F., de Vries, H., Dumoulin, V ., and\\nCourville, A. C. Film: Visual reasoning with a general\\nconditioning layer. AAAI , 2018.\\nPeters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,\\nLee, K., and Zettlemoyer, L. Deep contextualized word\\nrepresentations. In NAACL , 2018.\\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\\nI. Improving language understanding by genera-\\ntive pre-training. URL https://s3-us-west-2. ama-\\nzonaws. com/openai-assets/research-covers/language-\\nunsupervised/language_ understanding_paper. pdf , 2018.\\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad:\\n100,000+ questions for machine comprehension of text.\\nInEMNLP , 2016.\\nRajpurkar, P., Jia, R., and Liang, P. Know what you don‚Äôt\\nknow: Unanswerable questions for squad. In ACL, 2018.\\nRebufÔ¨Å, S., Vedaldi, A., and Bilen, H. EfÔ¨Åcient parametriza-\\ntion of multi-domain deep neural networks. In CVPR ,\\n2018.\\nRebufÔ¨Å, S.-A., Bilen, H., and Vedaldi, A. Learning multiple\\nvisual domains with residual adapters. In NIPS . 2017.\\nRosenfeld, A. and Tsotsos, J. K. Incremental learning\\nthrough deep adaptation. IEEE transactions on pattern\\nanalysis and machine intelligence , 2018.\\nRusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H.,\\nKirkpatrick, J., Kavukcuoglu, K., Pascanu, R., and Had-\\nsell, R. Progressive neural networks. arXiv preprint\\narXiv:1606.04671 , 2016.Simonyan, K. and Zisserman, A. Very deep convolutional\\nnetworks for large-scale image recognition. ICLR , 2014.\\nSina Semnani, Kaushik Sadagopan, F. T. BERT-A: Fine-\\ntuning BERT with Adapters and Data Augmentation.\\nhttp://web.stanford.edu/class/cs224n/reports/default/15848417.pdf ,\\n2019.\\nStickland, A. C. and Murray, I. BERT and PALs: Projected\\nAttention Layers for EfÔ¨Åcient Adaptation in Multi-Task\\nLearning. arXiv preprint arXiv:1902.02671 , 2019.\\nThrun, S. Learning to learn. chapter Lifelong Learning\\nAlgorithms. 1998.\\nTurian, J., Ratinov, L., and Bengio, Y . Word representa-\\ntions: A simple and general method for semi-supervised\\nlearning. In ACL, 2010.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Atten-\\ntion is all you need. In NIPS . 2017.\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\\nBowman, S. R. Glue: A multi-task benchmark and analy-\\nsis platform for natural language understanding. ICLR ,\\n2018.\\nWong, C., Houlsby, N., Lu, Y ., and Gesmundo, A. Transfer\\nlearning with neural automl. In NeurIPS . 2018.\\nYosinski, J., Clune, J., Bengio, Y ., and Lipson, H. How trans-\\nferable are features in deep neural networks? In Ghahra-\\nmani, Z., Welling, M., Cortes, C., Lawrence, N. D., and\\nWeinberger, K. Q. (eds.), NIPS . 2014.\\nZenke, F., Poole, B., and Ganguli, S. Continual learning\\nthrough synaptic intelligence. ICML , 2017.\\nZoph, B. and Le, Q. V . Neural architecture search with\\nreinforcement learning. In ICLR , 2017.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PE_Transfer_Learning.pdf', 'page': 10}, page_content='Supplementary Material for\\nParameter-EfÔ¨Åcient Transfer Learning for NLP\\nA. Additional Text ClassiÔ¨Åcation Tasks\\nDataset Train examples Validation examples Test examples Classes Avg text length Reference\\n20 newsgroups 15076 1885 1885 20 1903 (Lang, 1995)\\nCrowdÔ¨Çower airline 11712 1464 1464 3 104 crowdÔ¨Çower.com\\nCrowdÔ¨Çower corporate messaging 2494 312 312 4 121 crowdÔ¨Çower.com\\nCrowdÔ¨Çower disasters 8688 1086 1086 2 101 crowdÔ¨Çower.com\\nCrowdÔ¨Çower economic news relevance 6392 799 800 2 1400 crowdÔ¨Çower.com\\nCrowdÔ¨Çower emotion 32000 4000 4000 13 73 crowdÔ¨Çower.com\\nCrowdÔ¨Çower global warming 3380 422 423 2 112 crowdÔ¨Çower.com\\nCrowdÔ¨Çower political audience 4000 500 500 2 205 crowdÔ¨Çower.com\\nCrowdÔ¨Çower political bias 4000 500 500 2 205 crowdÔ¨Çower.com\\nCrowdÔ¨Çower political message 4000 500 500 9 205 crowdÔ¨Çower.com\\nCrowdÔ¨Çower primary emotions 2019 252 253 18 87 crowdÔ¨Çower.com\\nCrowdÔ¨Çower progressive opinion 927 116 116 3 102 crowdÔ¨Çower.com\\nCrowdÔ¨Çower progressive stance 927 116 116 4 102 crowdÔ¨Çower.com\\nCrowdÔ¨Çower US economic performance 3961 495 496 2 305 crowdÔ¨Çower.com\\nCustomer complaint database 146667 18333 18334 157 1046 catalog.data.gov\\nNews aggregator dataset 338349 42294 42294 4 57 (Lichman, 2013)\\nSMS spam collection 4459 557 558 2 81 (Almeida et al., 2011)\\nTable 3. Statistics and references for the additional text classiÔ¨Åcation tasks.\\nDataset Epochs (Fine-tune) Epochs (Adapters)\\n20 newsgroups 50 50\\nCrowdÔ¨Çower airline 50 20\\nCrowdÔ¨Çower corporate messaging 100 50\\nCrowdÔ¨Çower disasters 50 50\\nCrowdÔ¨Çower economic news relevance 20 20\\nCrowdÔ¨Çower emotion 20 20\\nCrowdÔ¨Çower global warming 100 50\\nCrowdÔ¨Çower political audience 50 20\\nCrowdÔ¨Çower political bias 50 50\\nCrowdÔ¨Çower political message 50 50\\nCrowdÔ¨Çower primary emotions 100 100\\nCrowdÔ¨Çower progressive opinion 100 100\\nCrowdÔ¨Çower progressive stance 100 100\\nCrowdÔ¨Çower US economic performance 100 20\\nCustomer complaint database 20 20\\nNews aggregator dataset 20 20\\nSMS spam collection 50 20\\nTable 4. Number of training epochs selected for the additional classiÔ¨Åcation tasks.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PE_Transfer_Learning.pdf', 'page': 11}, page_content='Parameter-EfÔ¨Åcient Transfer Learning for NLP\\nParameter Search Space\\n1) Input embedding modules Refer to Table 6\\n2) Fine-tune input embedding module {True, False}\\n3) Lowercase text {True, False}\\n4) Remove non alphanumeric text {True, False}\\n5) Use convolution {True, False}\\n6) Convolution activation {relu, relu6, leaky relu, swish, sigmoid, tanh}\\n7) Convolution batch norm {True, False}\\n8) Convolution max ngram length {2, 3}\\n9) Convolution dropout rate [0.0, 0.4]\\n10) Convolution number of Ô¨Ålters [50, 200]\\n11) Convolution embedding dropout rate [0.0, 0.4]\\n12) Number of hidden layers {0, 1, 2, 3, 5}\\n13) Hidden layers size {64, 128, 256}\\n14) Hidden layers activation {relu, relu6, leaky relu, swish, sigmoid, tanh}\\n15) Hidden layers normalization {none, batch norm, layer norm}\\n16) Hidden layers dropout rate {0.0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5}\\n17) Deep tower learning rate {0.001, 0.005, 0.01, 0.05, 0.1, 0.5}\\n18) Deep tower regularization weight {0.0, 0.0001, 0.001, 0.01}\\n19) Wide tower learning rate {0.001, 0.005, 0.01, 0.05, 0.1, 0.5}\\n20) Wide tower regularization weight {0.0, 0.0001, 0.001, 0.01}\\n21) Number of training samples {1e5, 2e5, 5e5, 1e6, 2e6}\\nTable 5. The search space of baseline models for the additional text classiÔ¨Åcation tasks.\\nID Dataset Embed Vocab. Training TensorFlow Hub Handles\\nsize dim. size algorithm PreÔ¨Åx: https://tfhub.dev/google/\\n(tokens)\\nEnglish-small 7B 50 982k Lang. model nnlm-en-dim50-with-normalization/1\\nEnglish-big 200B 128 999k Lang. model nnlm-en-dim128-with-normalization/1\\nEnglish-wiki-small 4B 250 1M Skipgram Wiki-words-250-with-normalization/1\\nEnglish-wiki-big 4B 500 1M Skipgram Wiki-words-500-with-normalization/1\\nUniversal-sentence-encoder - 512 - (Cer et al., 2018) universal-sentence-encoder/2\\nTable 6. Options for text input embedding modules. These are pre-trained text embedding tables. We provide the handle for the modules\\nthat are publicly distributed via the TensorFlow Hub service ( https://www.tensorflow.org/hub ).\\nB. Learning Rate Robustness\\nWe test the robustness of adapters and Ô¨Åne-tuning to the learning rate. We ran experiments with learning rates in the range\\n[2¬∑10‚àí5,10‚àí3], and selected the best hyperparameters for each method at each learning rate. Figure 7 shows the results.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/PE_Transfer_Learning.pdf', 'page': 12}, page_content='Parameter-EfÔ¨Åcient Transfer Learning for NLP\\nDataset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21\\n20 newsgroups Universal-sentence-encoder False True True False relu6 False 2 0.37 94 0.38 1 128 leaky relu batch norm 0.5 0.5 0 0.05 0.0001 1000000\\nCrowdÔ¨Çower airline English-big False False False True leaky relu False 3 0.36 200 0.07 0 128 tanh layer norm 0.4 0.1 0.001 0.05 0.001 200000\\nCrowdÔ¨Çower corporate messaging English-big False False True True tanh True 3 0.40 56 0.40 1 64 tanh batch norm 0.5 0.5 0.001 0.01 0 200000\\nCrowdÔ¨Çower disasters Universal-sentence-encoder True True False True swish True 3 0.27 52 0.22 0 64 relu none 0.2 0.005 0.0001 0.005 0.01 500000\\nCrowdÔ¨Çower economic news relevance Universal-sentence-encoder True True False False leaky relu False 2 0.27 63 0.04 3 128 swish layer norm 0.2 0.01 0.01 0.001 0 100000\\nCrowdÔ¨Çower emotion Universal-sentence-encoder False True False False relu6 False 3 0.35 132 0.34 1 64 tanh none 0.05 0.05 0 0.05 0 200000\\nCrowdÔ¨Çower global warming Universal-sentence-encoder False True True False swish False 3 0.39 200 0.36 1 128 leaky relu batch norm 0.4 0.05 0 0.001 0.001 1000000\\nCrowdÔ¨Çower political audience English-small True False True True relu False 3 0.11 98 0.07 0 64 relu none 0.5 0.05 0.001 0.001 0 100000\\nCrowdÔ¨Çower political bias English-big False True True False swish False 3 0.12 81 0.30 0 64 relu6 none 0 0.01 0 0.005 0.01 200000\\nCrowdÔ¨Çower political message Universal-sentence-encoder False False True False swish True 2 0.36 57 0.35 0 64 tanh none 0.5 0.01 0.001 0.005 0 200000\\nCrowdÔ¨Çower primary emotions English-big False True True True swish False 3 0.40 191 0.03 0 256 relu6 none 0.5 0.1 0.001 0.05 0 200000\\nCrowdÔ¨Çower progressive opinion English-big True False True True relu6 False 3 0.40 199 0.28 0 128 relu batch norm 0.3 0.1 0.01 0.005 0.001 200000\\nCrowdÔ¨Çower progressive stance Universal-sentence-encoder True False True False relu True 3 0.01 195 0.00 2 256 tanh layer norm 0.4 0.005 0 0.005 0.0001 500000\\nCrowdÔ¨Çower us economic performance English-big True False True True tanh True 2 0.31 53 0.24 1 256 leaky relu batch norm 0.3 0.05 0.0001 0.001 0.0001 100000\\nCustomer complaint database English-big True False False False tanh False 2 0.03 69 0.10 1 256 leaky relu layer norm 0.1 0.05 0.0001 0.05 0.001 1000000\\nNews aggregator dataset Universal-sentence-encoder False True True False sigmoid True 2 0.00 156 0.29 3 256 relu batch norm 0.05 0.05 0 0.5 0.0001 1000000\\nSms spam collection English-wiki-small True True True True leaky relu False 3 0.20 54 0.00 1 128 leaky relu batch norm 0 0.1 0 0.05 0.01 1000000\\nTable 7. Search space parameters (see Table 5) for the AutoML baseline models that were selected.\\n10-410-3\\nLearning rate707580859095f1 score\\nAdapters\\nFine-tune top layers\\nFigure 7. Best performing models at different learning rates. Error vars indicate the s.e.m. across three random seeds.'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 0}, page_content='What do we learn from region based\\nobject detectors (Faster R-CNN, R-\\nFCN, FPN)?\\nJonathan Hui¬∑Follow\\n11 min read¬∑Mar 28, 2018\\n3.6K 25\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 1}, page_content='In this series, we will take a comprehensive journey on object detection. In\\nPart 1 here, we cover the region based object detectors including Fast R-\\nCNN, Faster R-CNN, R-FCN and FPN. In part 2, we will study the single shoot\\ndetectors. In part 3, we cover the performance and some implementation\\nissues. By studying them in one context, we study what is working, what\\nmatters and where can be improved. Hopefully, by studying how we get\\nhere, it will give us more insights on where we are heading.\\nPart 1: What do we learn from region based object detectors (Faster R-CNN,\\nR-FCN, FPN)?\\nPart 2: What do we learn from single shot object detectors (SSD, YOLO), FPN\\n& Focal loss?\\nPart 3: Design choices, lessons learned and trends for object detections?\\nSliding-window detectors\\nSince AlexNet won the 2012 ILSVRC challenge, the use of the CNN for\\nclassification has dominated the field. One brute force approach for object\\ndetection is to slide windows from left and right, and from up to down to\\nidentify objects using classification. To detect different object types at\\nvarious viewing distances, we use windows of varied sizes and aspect ratios.\\nSliding-windows (From right to left, up and down)\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 2}, page_content='We cut out patches from the picture according to the sliding windows. The\\npatches are warped since many classifiers take fixed size images only.\\nHowever, this should not impact the classification accuracy since the\\nclassifier are trained to handle warped images.\\nWarp an image to a fixed size image.\\nThe warped image patch is fed into a CNN classifier to extract 4096 features.\\nThen we apply a SVM classifier to identify the class and another linear\\nregressor for the boundary box.\\nSystem flow for the sliding-window detector.\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 3}, page_content='Below is the pseudo code. We create many windows to detect different object\\nshapes at different locations. To improve performance, one obvious solution\\nis to reduce the number of windows.\\nfor window in windows\\n    patch = get_patch(image, window)\\n    results = detector(patch)\\nSelective Search\\nInstead of a brute force approach, we use a region proposal method to create\\nregions of interest (ROIs) for object detection. In selective search (SS), we\\nstart with each individual pixel as its own group. Next, we calculate the\\ntexture for each group and combine two that are the closest. But to avoid a\\nsingle region in gobbling others, we prefer grouping smaller group first. We\\ncontinue merging regions until everything is combined together. In the first\\nrow below, we show how we grow the regions, and the blue rectangles in the\\nsecond rows show all possible ROIs we made during the merging.\\n(Image source: van de Sande et al. ICCV‚Äô11)\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 4}, page_content='R-CNN\\nR-CNN makes use of a region proposal method to create about 2000 ROIs\\n(regions of interest). The regions are warped into fixed size images and feed\\ninto a CNN network individually. It is then followed by fully connected layers\\nto classify the object and to refine the boundary box.\\nUse region proposals, CNN, affine layers to locate objects.\\nHere is the system flow.\\nSystem flow for R-CNN\\nWith far fewer but higher quality ROIs, R-CNN run faster and more accurate\\nthan the sliding windows.\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 5}, page_content='ROIs = region_proposal(image)\\nfor ROI in ROIs\\n    patch = get_patch(image, ROI)\\n    results = detector(patch)\\nBoundary box regressor\\nRegion proposal methods are computation intense. To speed up the process,\\nwe often pick a less expensive region proposal method to create ROIs\\nfollowed by a linear regressor (using fully connected layers) to refine the\\nboundary box further.\\nUse regression to refine the original ROI in blue to the red one.\\nFast R-CNN\\nR-CNN needs many proposals to be accurate and many regions overlap with\\neach other. R-CNN is slow in training & inference. If we have 2,000\\nproposals, each of them is processed by a CNN separately, i.e. we repeat\\nfeature extractions 2,000 times for different ROIs.\\nInstead of extracting features for each image patch from scratch, we use a\\nfeature extractor (a CNN) to extract features for the whole image first. We\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 6}, page_content='also use an external region proposal method, like the selective search, to\\ncreate ROIs which later combine with the corresponding feature maps to\\nform patches for object detection. We warp the patches to a fixed size using\\nROI pooling and feed them to fully connected layers for classification and\\nlocalization (detecting the location of the object). By not repeating the\\nfeature extractions, Fast R-CNN cuts down the process time significantly.\\nApply region proposal on feature maps and form fixed size patches using ROI pooling.\\nHere is the network flow:\\nIn the pseudo-code below, the expensive feature extraction is moving out of\\nthe for-loop, a significant speed improvement since it was executed for all\\n2000 ROIs. Fast R-CNN is 10x faster than R-CNN in training and 150x faster in\\ninferencing.\\nfeature_maps = process(image)\\nROIs = region_proposal(image)\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 7}, page_content='for ROI in ROIs\\n    patch = roi_pooling(feature_maps, ROI)\\n    results = detector2(patch)\\nOne major takeaway for Fast R-CNN is that the whole network (the feature\\nextractor, the classifier, and the boundary box regressor) are trained end-to-\\nend with multi-task losses (classification loss and localization loss). This\\nimproves accuracy.\\nROI Pooling\\nBecause Fast R-CNN uses fully connected layers, we apply ROI pooling to\\nwarp the variable size ROIs into in a predefined size shape.\\nLet‚Äôs simplify the discussion by transforming 8 √ó  8 feature maps into a\\npredefined 2 √ó  2 shape.\\nTop left below: our feature maps.\\nTop right: we overlap the ROI (blue) with the feature maps.\\nBottom left: we split ROIs into the target dimension. For example, with\\nour 2 √ó 2 target, we split the ROIs into 4 sections with similar or equal\\nsizes.\\nBottom right: find the maximum for each section and the result is our\\nwarped feature maps.\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 8}, page_content='Input feature map (top left), output feature map (bottom right), blue box is the ROI (top right).\\nSo we get a 2 √ó  2 feature patch that we can feed into the classifier and box\\nregressor.\\nFaster R-CNN\\nFast R-CNN depends on an external region proposal method like selective\\nsearch. However, those algorithms run on CPU and they are slow. In testing,\\nFast R-CNN takes 2.3 seconds to make a prediction in which 2 seconds are\\nfor generating 2000 ROIs.\\nfeature_maps = process(image)\\nROIs = region_proposal(image)         # Expensive!\\nfor ROI in ROIs\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 9}, page_content='    patch = roi_pooling(feature_maps, ROI)\\n    results = detector2(patch)\\nFaster R-CNN adopts similar design as the Fast R-CNN except it replaces the\\nregion proposal method by an internal deep network and the ROIs are\\nderived from the feature maps instead. The new region proposal network\\n(RPN) is more efficient and run at 10 ms per image in generating ROIs.\\nNetwork flow is the same as the Fast R-CNN.\\nThe network flow is similar but the region proposal is now replaced by a\\nconvolutional network (RPN).\\nThe external region proposal is replaced by an internal deep network.\\nRegion proposal network\\nThe region proposal network (RPN) takes the output feature maps from the\\nfirst convolutional network as input. It slides 3 √ó  3 filters over the feature\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 10}, page_content='maps to make class-agnostic region proposals using a convolutional network\\nlike ZF network (below). Other deep network likes VGG or ResNet can be\\nused for more comprehensive feature extraction at the cost of speed. The ZF\\nnetwork outputs 256 values, which is feed into 2 separate fully connected\\nlayers to predict a boundary box and 2 objectness scores. The objectness\\nmeasures whether the box contains an object. We can use a regressor to\\ncompute a single objectness score but for simplicity, Faster R-CNN uses a\\nclassifier with 2 possible classes: one for the ‚Äúhave an object‚Äù category and\\none without (i.e. the background class).\\nFor each location in the feature maps, RPN makes k guesses. Therefore RPN\\noutputs 4 √ó k coordinates and 2 √ó k scores per location. The diagram below\\nshows the 8 √ó  8 feature maps with a 3 √ó  3 filter, and it outputs a total of 8 √ó  8 √ó\\n3 ROIs (for k = 3). The right side diagram demonstrates the 3 proposals made\\nby a single location.\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 11}, page_content='Here, we get 3 guesses and we will refine our guesses later. Since we just\\nneed one to be correct, we will be better off if our initial guesses have\\ndifferent shapes and size. Therefore, Faster R-CNN does not make random\\nboundary box proposals. Instead, it predicts offsets like ùõø x, ùõø y that are\\nrelative to the top left corner of some reference boxes called anchors. We\\nconstraints the value of those offsets so our guesses still resemble the\\nanchors.\\nTo make k predictions per location, we need k anchors centered at each\\nlocation. Each prediction is associated with a specific anchor but different\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 12}, page_content='locations share the same anchor shapes.\\nThose anchors are carefully pre-selected so they are diverse and cover real-\\nlife objects at different scales and aspect ratios reasonable well. This guides\\nthe initial training with better guesses and allows each prediction to\\nspecialize in a certain shape. This strategy makes early training more stable\\nand easier.\\nFaster R-CNN uses far more anchors. It deploys 9 anchor boxes: 3 different\\nscales at 3 different aspect ratio. Using 9 anchors per location, it generates 2\\n√ó 9 objectness scores and 4 √ó  9 coordinates per location.\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 13}, page_content='Source\\nAnchors are also called priors or default boundary boxes in different papers.\\nPerformance for R-CNN methods\\nAs shown below, Faster R-CNN is even much faster.\\nRegion-based Fully Convolutional Networks (R-FCN)\\nLet‚Äôs assume we only have a feature map detecting the right eye of a face.\\nCan we use it to locate a face? It should. Since the right eye should be on the\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 14}, page_content='top-left corner of a facial picture, we can use that to locate the face.\\nIf we have other feature maps specialized in detecting the left eye, the nose\\nor the mouth, we can combine the results together to locate the face better.\\nSo why we go through all the trouble. In Faster R-CNN, the detector applies\\nmultiple fully connected layers to make predictions. With 2,000 ROIs, it can\\nbe expensive.\\nfeature_maps = process(image)\\nROIs = region_proposal(feature_maps)\\nfor ROI in ROIs\\n    patch = roi_pooling(feature_maps, ROI)\\n    class_scores, box = detector(patch)         # Expensive!\\n    class_probabilities = softmax(class_scores)\\nR-FCN improves speed by reducing the amount of work needed for each ROI.\\nThe region-based feature maps above are independent of ROIs and can be\\ncomputed outside each ROI. The remaining work is much simpler and\\ntherefore R-FCN is faster than Faster R-CNN.\\nfeature_maps = process(image)\\nROIs = region_proposal(feature_maps)         \\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 15}, page_content='score_maps = compute_score_map(feature_maps)\\nfor ROI in ROIs\\n    V = region_roi_pool(score_maps, ROI)     \\n    class_scores, box = average(V)                   # Much simpler!\\n    class_probabilities = softmax(class_scores)\\nLet‚Äôs consider a 5 √ó  5 feature map M with a blue square object inside. We\\ndivide the square object equally into 3 √ó  3 regions. Now, we create a new\\nfeature map from M to detect the top left (TL) corner of the square only. The\\nnew feature map looks like the one on the right below. Only the yellow grid\\ncell [2, 2] is activated.\\nCreate a new feature map from the left to detect the top left corner of an object.\\nSince we divide the square into 9 parts, we can create 9 feature maps each\\ndetecting the corresponding region of the object. These feature maps are\\ncalled position-sensitive score maps because each map detects (scores) a\\nsub-region of the object.\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 16}, page_content='Generate 9 score maps\\nLet‚Äôs say the dotted red rectangle below is the ROI proposed. We divide it into\\n3 √ó 3 regions and ask how likely each region contains the corresponding part\\nof the object. For example, how likely the top-left ROI region contains the\\nleft eye. We store the results into a 3 √ó  3 vote array in the right diagram\\nbelow. For example, vote_array[0][0] contains the score on whether we find\\nthe top-left region of the square object.\\nApply ROI onto the feature maps to output a 3 \\x00 3 array.\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 17}, page_content='This process to map score maps and ROIs to the vote array is called position-\\nsensitive ROI-pool. The process is extremely close to the ROI pool we\\ndiscussed before. We will not cover it further but you can refer to the future\\nreading section for more information.\\nOverlay a portion of the ROI onto the corresponding score map to calculate V[i][j]\\nAfter calculating all the values for the position-sensitive ROI pool, the class\\nscore is the average of all its elements.\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 18}, page_content='ROI pool\\nLet‚Äôs say we have C classes to detect. We expand it to C + 1 classes so we\\ninclude a new class for the background (non-object). Each class will have its\\nown 3 √ó  3 score maps and therefore a total of (C+1) √ó  3 √ó  3 score maps. Using\\nits own set of score maps, we predict a class score for each class. Then we\\napply a softmax on those scores to compute the probability for each class.\\nThe following is the data flow. For our example, we have k=3 below.\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 19}, page_content='Our journey so far\\nWe start with the basic sliding window algorithm.\\nfor window in windows\\n    patch = get_patch(image, window)\\n    results = detector(patch)\\nThen we try to reduce the number of windows and move as much work as\\npossible outside the for-loop.\\nROIs = region_proposal(image)\\nfor ROI in ROIs\\n    patch = get_patch(image, ROI)\\n    results = detector(patch)\\nIn Part 2, we go even further to completely remove the for-loop. Single shot\\ndetectors make object detections in single shot without a separate step of\\nregion proposal.\\nFurther reading on FPN, R-FCN and Mask R-CNN\\nBoth FPN and R-FCN are more complex than we described here. For further\\nstudy, please refer to:\\nFeature Pyramid Networks (FPN) for object detection.\\nRegion-based Fully Convolutional Networks (R-FCN).\\nImage segmentation with Mask R-CNN\\nIn a previous article, we discuss the use of region based object\\ndetector like Faster R-CNN to detect objects. Instead‚Ä¶\\nmedium.com\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 20}, page_content='Written by Jonathan Hui\\n40K Followers\\nDeep LearningFollowResources\\nDetectron: Facebook Research‚Äôs implementation of the Faster R-CNN and\\nMask R-CNN using Caffe2.\\nThe official implementation for the Faster R-CNN in MATLAB.\\nFaster R-CNN implementation in TensorFlow.\\nR-FCN implementation in MXNet.\\nR-FCN implementation in Caffe and MATLAB.\\nR-FCN implementation in TensorFlow.\\nMachine Learning Deep Learning Computer Vision Object Detection\\nArtificial Intelligence\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 21}, page_content='More from Jonathan Hui\\nSee all from Jonathan Hui\\nThe Study of Mathematical Spaces\\n(Machine Learning)\\nThe terminology of mathematical spaces in AI\\nresearch papers can be intimidating.‚Ä¶\\nMay 15\\nmAP (mean Average Precision) for\\nObject Detection\\nAP (Average precision) is a popular metric in\\nmeasuring the accuracy of object detectors‚Ä¶\\nMar 6, 2018\\nKernels for Machine Learning\\nIn many machine learning problems, input\\ndata is transformed into a higher-dimension‚Ä¶\\nMay 27\\nVector Space, Normed Space &\\nHilbert Space (Machine Learning)\\nEuclidean space, the familiar geometry of our\\neveryday world, provides a useful framework‚Ä¶\\nMay 20\\nJonathan Hui\\n606 4\\nJonathan Hui\\n7.4K 54\\nJonathan Hui\\n218 3\\nJonathan Hui\\n376 4\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 22}, page_content='Recommended from Medium\\nLists\\nPredictive Modeling w/\\nPython\\n20 stories¬∑1337 savesNatural Language Processing\\n1556 stories¬∑1093 saves\\nAI Regulation\\n6 stories¬∑498 savesPractical Guides to Machine\\nLearning\\n10 stories¬∑1618 saves\\nCVPR 2024: Top Highlights You\\nMust Know‚Äî Embodied AI, GenAI‚Ä¶\\nWe bring you the highlights and the key\\ntakeaways directly from CVPR 2024!\\n6d ago\\nin\\nExploring Object Detection with R-\\nCNN Models ‚Äî A Comprehensive‚Ä¶\\nObject Detection Models\\nFeb 16\\nThe Tenyks Blogger\\n15Raghav Bali Towards Data Science\\n151\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/CNN everything.pdf', 'page': 23}, page_content='See more recomme ndationsObject Detection part-2: Two Stage\\nDetectors: R-CNN,Fast R-‚Ä¶\\nObject detection, the task of precisely\\nlocating and classifying objects within an‚Ä¶\\nFeb 29SimCLR: A Simple Framework for\\nContrastive Learning of Visual‚Ä¶\\nUnsupervised contrastive learning\\nMay 21\\nIntuitive Understanding of\\nAutoencoders and Variational‚Ä¶\\nLearn about Autoencoders and Variational\\nAutoencoders, their structures, latent space‚Ä¶\\nMay 16Review ‚Äî YOLOv10: Real-Time End-\\nto-End Object Detection\\nNMS-Free Training, and Holistic Efficiency-\\nAccuracy Driven Model Design are Proposed\\nJun 4Rajesh Katta\\n111Kavishka Abeywardana\\n4\\nHugman Sangkeun Jung\\n53Sik-Ho Tsang\\n16\\n55 s remaining\\n'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani‚àó\\nGoogle Brain\\navaswani@google.comNoam Shazeer‚àó\\nGoogle Brain\\nnoam@google.comNiki Parmar‚àó\\nGoogle Research\\nnikip@google.comJakob Uszkoreit‚àó\\nGoogle Research\\nusz@google.com\\nLlion Jones‚àó\\nGoogle Research\\nllion@google.comAidan N. Gomez‚àó ‚Ä†\\nUniversity of Toronto\\naidan@cs.toronto.edu≈Åukasz Kaiser‚àó\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin‚àó ‚Ä°\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n‚Ä†Work performed while at Google Brain.\\n‚Ä°Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 1}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht‚àí1and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 3}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by‚àödk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\\nthe matrix of outputs as:\\nAttention( Q, K, V ) = softmax(QKT\\n‚àödk)V (1)\\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof1‚àödk. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients4. To counteract this effect, we scale the dot products by1‚àödk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q¬∑k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\\nwhere head i= Attention( QWQ\\ni, KWK\\ni, V WV\\ni)\\nWhere the projections are parameter matrices WQ\\ni‚ààRdmodel√ódk,WK\\ni‚ààRdmodel√ódk,WV\\ni‚ààRdmodel√ódv\\nandWO‚ààRhdv√ódmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n‚Ä¢In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n‚Ä¢The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n‚Ä¢Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to ‚àí‚àû) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\\ndff= 2048 .\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by‚àödmodel.\\n5'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2¬∑d) O(1) O(1)\\nRecurrent O(n¬∑d2) O(n) O(n)\\nConvolutional O(k¬∑n¬∑d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r¬∑n¬∑d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2œÄto10000 ¬∑2œÄ. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi‚ààRd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 6}, page_content='length nis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k¬∑n¬∑d+n¬∑d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [ 20] with Œ≤1= 0.9,Œ≤2= 0.98andœµ= 10‚àí9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate =d‚àí0.5\\nmodel¬∑min(step_num‚àí0.5, step _num¬∑warmup _steps‚àí1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup _steps = 4000 .\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModelBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0¬∑1020\\nGNMT + RL [38] 24.6 39.92 2.3¬∑10191.4¬∑1020\\nConvS2S [9] 25.16 40.46 9.6¬∑10181.5¬∑1020\\nMoE [32] 26.03 40.56 2.0¬∑10191.2¬∑1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0¬∑1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8¬∑10201.1¬∑1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7¬∑10191.2¬∑1021\\nTransformer (base model) 27.3 38.1 3.3¬∑1018\\nTransformer (big) 28.4 41.8 2.3¬∑1019\\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop= 0.1.\\nLabel Smoothing During training, we employed label smoothing of value œµls= 0.1[36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop= 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4and length penalty Œ±= 0.6[38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dvPdrop œµlstrain PPL BLEU params\\nsteps (dev) (dev) √ó106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21andŒ±= 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n10'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 10}, page_content='[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, √áaglar G√ºl√ßehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL , 2016.\\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770‚Äì778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735‚Äì1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing , pages 832‚Äì841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[16] ≈Åukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n[17] ≈Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n11'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313‚Äì330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\\npages 152‚Äì159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433‚Äì440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research , 15(1):1929‚Äì1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440‚Äì2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104‚Äì3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems , 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google‚Äôs neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers) , pages 434‚Äì443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 12}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‚Äòmaking‚Äô, completing the phrase ‚Äòmaking...more difficult‚Äô. Attentions here shown only for\\nthe word ‚Äòmaking‚Äô. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 13}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‚Äòits‚Äô for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'source': '/Users/satish/Desktop/Llamaindex/Good_papers_CNN_NLP/Transformer_Attention is all we need.pdf', 'page': 14}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size =1000,chunk_overlap = 200)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector EMbedding and vector store using OPENAI\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(documents[:20],OpenAIEmbeddings())\n",
    "\n",
    "## USe that free LLM model for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector EMbedding and vector store using OLLAMA \n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "#from langchain_community.vectorstores import Chroma\n",
    "#db = Chroma.from_documents(documents[:20],OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USig  FAISS as a vector store\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents[:20],OllamaEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'critical deployment challenge for GPT-3 (Brown et al., 2020) with\\n175 billion trainable parameters.1\\nMany sought to mitigate this by adapting only some parameters or\\nlearning external modules for new tasks. This way, we only need\\nto store and load a small number of task-speciÔ¨Åc parameters in ad-\\ndition to the pre-trained model for each task, greatly boosting the\\noperational efÔ¨Åciency when deployed. However, existing techniques\\n‚àóEqual contribution.\\n0Compared to V1, this draft includes better baselines, experiments on GLUE, and more on adapter latency.\\n1While GPT-3 175B achieves non-trivial performance with few-shot learning, Ô¨Åne-tuning boosts its perfor-\\nmance signiÔ¨Åcantly as shown in Appendix A.\\n1arXiv:2106.09685v2  [cs.CL]  16 Oct 2021'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Give me the paragraph headings\"\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\")\n",
    "#from langchain_community.llms import Ollama\n",
    "#llm = Ollama(model = \"llama2\")\n",
    "#llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Designing ChatPrompt Template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        First go through all the research papers and integrate with the training data of chatgpt to give the\n",
    "        precise answer. \n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chain Implementation\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retriever\n",
    "retriever= db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieval Chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Suppose I have an output data of Sentiment Analysis and I want to integrate the sentiment inference with trend data. How can I do that\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Integrating sentiment analysis output with trend data can be achieved through a multi-step approach. Here‚Äôs a structured way to approach this integration:',\n",
       " '',\n",
       " '1. **Data Preparation**:',\n",
       " '   - **Sentiment Analysis Output**: Ensure that your sentiment analysis data is structured, typically in a format that includes the text (e.g., reviews, comments), sentiment score (positive, negative, neutral), and any relevant metadata (timestamp, user ID, etc.).',\n",
       " '   - **Trend Data**: Gather trend data relevant to your analysis. This could include time-series data related to social media trends, sales figures, stock prices, or any other relevant metrics that fluctuate over time.',\n",
       " '',\n",
       " '2. **Time Alignment**:',\n",
       " '   - **Timestamps**: If your sentiment data has timestamps, align it with your trend data. This could involve aggregating sentiment scores over specific time intervals (e.g., daily, weekly) to match the frequency of your trend data.',\n",
       " '   - **Resampling**: If the sentiment data is more granular (e.g., at the minute level), consider resampling it to the same frequency as your trend data to facilitate comparison.',\n",
       " '',\n",
       " '3. **Feature Engineering**:',\n",
       " '   - **Aggregate Sentiment Scores**: Compute aggregate sentiment scores (e.g., average, sum) over the selected time intervals. This creates a new feature that represents the overall sentiment for those time periods.',\n",
       " '   - **Create Sentiment Indicators**: Develop additional features based on sentiment, such as the proportion of positive vs. negative sentiments, sentiment volatility, or sentiment trends (e.g., rolling averages).',\n",
       " '',\n",
       " '4. **Integration**:',\n",
       " '   - **Merge Datasets**: Combine the sentiment analysis output and trend data into a single dataset. This can be done using a common key, typically the timestamp.',\n",
       " '   - **Data Frame Manipulation**: If you are using programming languages like Python with libraries such as Pandas, you can use functions like `merge` or `join` to integrate the datasets based on the timestamp.',\n",
       " '',\n",
       " '5. **Analysis**:',\n",
       " '   - **Correlation Analysis**: Investigate correlations between sentiment scores and trend metrics. This can help identify if changes in sentiment are associated with trends in your other data.',\n",
       " '   - **Visualizations**: Create visualizations (e.g., line charts) to depict the relationship between sentiment and trends over time. This can provide insights into how sentiment impacts or reflects trends.',\n",
       " '',\n",
       " '6. **Modeling**:',\n",
       " '   - **Predictive Modeling**: If your goal is to predict trends based on sentiment, consider building a predictive model. Use sentiment scores as features in your regression or classification models to see if they can predict changes in trend data.',\n",
       " '   - **Time Series Analysis**: If both datasets are time-series, consider using time-series analysis techniques (e.g., ARIMA, LSTM) to model the relationship between sentiment and trends over time.',\n",
       " '',\n",
       " '7. **Interpretation & Insights**:',\n",
       " '   - After analyzing the integrated data, interpret the results. Look for patterns, anomalies, or significant relationships that can inform business decisions or further research.',\n",
       " '',\n",
       " 'By following these steps, you should be able to effectively integrate sentiment analysis output with trend data, allowing for deeper insights and more informed decision-making.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer'].split('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
